{
  "nbformat": 4,
  "nbformat_minor": 5,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.0"
    },
    "colab": {
      "name": "Copy of NLP_Bert.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "141cac0152f84a0c81b219f9d0acd2fd": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_99ba54ff201746b4ac486ccf66586e4b",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_550d0bee5ded4be7a6af558ca08df83c",
              "IPY_MODEL_ac9406b4e2cd419ca246a62d85d7108d",
              "IPY_MODEL_76eaad348586471ba669f574442d9610"
            ]
          }
        },
        "99ba54ff201746b4ac486ccf66586e4b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "550d0bee5ded4be7a6af558ca08df83c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_be756b9fd94941f1a27b40298f145f44",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": "100%",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_76e57b980a804b05bb41c28b46026b2e"
          }
        },
        "ac9406b4e2cd419ca246a62d85d7108d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_e7943c0d372147cca86343f2ffea3c69",
            "_dom_classes": [],
            "description": "",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 2,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 2,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_001a8880d9b0400db7c3174e2e66c1a1"
          }
        },
        "76eaad348586471ba669f574442d9610": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_b56e130d73154bfba7e7f779d1ef2bc2",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 2/2 [00:00&lt;00:00,  5.84it/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_a0ae6bc39a0b4e0d85a01e8a7f3e99ff"
          }
        },
        "be756b9fd94941f1a27b40298f145f44": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "76e57b980a804b05bb41c28b46026b2e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "e7943c0d372147cca86343f2ffea3c69": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "001a8880d9b0400db7c3174e2e66c1a1": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "b56e130d73154bfba7e7f779d1ef2bc2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "a0ae6bc39a0b4e0d85a01e8a7f3e99ff": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "Vme0b7gyj9Co",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "96d954c0-03a5-4779-9f93-46aad65f077e"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "id": "Vme0b7gyj9Co",
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a94a22b1"
      },
      "source": [
        "from transformers import BertTokenizer, BertConfig, BertForSequenceClassification, BertForPreTraining, TrainingArguments, Trainer, DataCollatorWithPadding\n",
        "from datasets import load_dataset\n",
        "import torch\n"
      ],
      "id": "a94a22b1",
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "id": "f90f7fbf"
      },
      "source": [
        "tokenizer = BertTokenizer.from_pretrained('bert-base-cased')\n",
        "config = BertConfig.from_pretrained(\"bert-base-cased\", hidden_dropout_prob=0.5, attention_probs_dropout_prob=0.5, num_labels = 1)\n",
        "model = BertForSequenceClassification(config)\n",
        "\n",
        "\n"
      ],
      "id": "f90f7fbf",
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 83,
          "referenced_widgets": [
            "141cac0152f84a0c81b219f9d0acd2fd",
            "99ba54ff201746b4ac486ccf66586e4b",
            "550d0bee5ded4be7a6af558ca08df83c",
            "ac9406b4e2cd419ca246a62d85d7108d",
            "76eaad348586471ba669f574442d9610",
            "be756b9fd94941f1a27b40298f145f44",
            "76e57b980a804b05bb41c28b46026b2e",
            "e7943c0d372147cca86343f2ffea3c69",
            "001a8880d9b0400db7c3174e2e66c1a1",
            "b56e130d73154bfba7e7f779d1ef2bc2",
            "a0ae6bc39a0b4e0d85a01e8a7f3e99ff"
          ]
        },
        "id": "c61c2e47",
        "outputId": "1da6b3f6-0afc-470f-82d0-0e18f47f9527"
      },
      "source": [
        "dataset = load_dataset('json', data_files={'train': '/content/drive/MyDrive/train.jsonl', 'test': '/content/drive/MyDrive/test.jsonl'})\n"
      ],
      "id": "c61c2e47",
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Using custom data configuration default-c36ab8dfd69bb5fd\n",
            "Reusing dataset json (/root/.cache/huggingface/datasets/json/default-c36ab8dfd69bb5fd/0.0.0/c2d554c3377ea79c7664b93dc65d0803b45e3279000f993c7bfd18937fd7f426)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "141cac0152f84a0c81b219f9d0acd2fd",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "  0%|          | 0/2 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e33b6cd2",
        "outputId": "3162a228-4623-4744-d2dc-8fdaa700df49"
      },
      "source": [
        "def preprocess_data(examples):\n",
        "    # encode a batch of sentences\n",
        "    encoding = tokenizer(examples['sentence1'], examples['sentence2'], truncation=True)\n",
        "    # add labels as a list\n",
        "    encoding[\"labels\"] = float(examples[\"label\"])\n",
        "    #encoding['position_ids'] = examples['idx']\n",
        "    \n",
        "    return encoding\n",
        "data_collator = DataCollatorWithPadding(tokenizer)\n",
        "\n",
        "# tokenize sentences + add labels\n",
        "encoded_dataset = dataset.map(preprocess_data)\n",
        "encoded_dataset = encoded_dataset.remove_columns(['word','sentence1','sentence2', 'idx', 'label', 'start1', 'start2', 'end1', 'end2', 'version'])\n",
        "for k,v in encoded_dataset.items():\n",
        "     print(k, v.shape)\n",
        "# turn into PyTorch dataset\n",
        "encoded_dataset.set_format(\"torch\")\n",
        "print(tokenizer.decode(encoded_dataset['train']['input_ids'][2]))\n",
        "\n",
        "\n",
        "small_train_dataset = encoded_dataset[\"train\"].shuffle(seed=42).select(range(600))\n",
        "small_eval_dataset = encoded_dataset[\"test\"].shuffle(seed=42).select(range(600))\n"
      ],
      "id": "e33b6cd2",
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Loading cached processed dataset at /root/.cache/huggingface/datasets/json/default-c36ab8dfd69bb5fd/0.0.0/c2d554c3377ea79c7664b93dc65d0803b45e3279000f993c7bfd18937fd7f426/cache-0e6111c9aed37798.arrow\n",
            "Loading cached processed dataset at /root/.cache/huggingface/datasets/json/default-c36ab8dfd69bb5fd/0.0.0/c2d554c3377ea79c7664b93dc65d0803b45e3279000f993c7bfd18937fd7f426/cache-fe4729bfa34919b6.arrow\n",
            "Loading cached shuffled indices for dataset at /root/.cache/huggingface/datasets/json/default-c36ab8dfd69bb5fd/0.0.0/c2d554c3377ea79c7664b93dc65d0803b45e3279000f993c7bfd18937fd7f426/cache-28672534af4ecc44.arrow\n",
            "Loading cached shuffled indices for dataset at /root/.cache/huggingface/datasets/json/default-c36ab8dfd69bb5fd/0.0.0/c2d554c3377ea79c7664b93dc65d0803b45e3279000f993c7bfd18937fd7f426/cache-0bf96aaa8bdf04e2.arrow\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "train (5000, 4)\n",
            "test (638, 4)\n",
            "[CLS] Run rogue. [SEP] She ran 10 miles that day. [SEP]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": false,
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "209d8c4c",
        "outputId": "fe9f3b08-4561-4ce0-9772-0bc3bbbbcfc0"
      },
      "source": [
        "import numpy as np\n",
        "from datasets import load_metric\n",
        "\n",
        "metric = load_metric('accuracy')\n",
        "\n",
        "def compute_metrics(eval_pred):\n",
        "    predictions, labels = eval_pred\n",
        "    predictions = predictions[:, 0]\n",
        "    return metric.compute(predictions=predictions, references=labels)\n",
        "\n",
        "\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=\"/content/drive/MyDrive/\",\n",
        "    save_strategy = \"epoch\",\n",
        "    evaluation_strategy=\"epoch\",\n",
        "    per_device_train_batch_size=8,\n",
        "    per_device_eval_batch_size=8,\n",
        "    num_train_epochs=5,\n",
        "    weight_decay = .00005,\n",
        "    learning_rate = .00002,\n",
        "    load_best_model_at_end=True,\n",
        ")\n",
        "\n",
        "trainer = Trainer(\n",
        "    model=model, \n",
        "    args=training_args, \n",
        "    train_dataset = small_train_dataset, \n",
        "    eval_dataset= small_eval_dataset, \n",
        "    data_collator = data_collator,  \n",
        "    compute_metrics=compute_metrics,\n",
        ")\n",
        "\n",
        "trainer.train()"
      ],
      "id": "209d8c4c",
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "PyTorch: setting up devices\n",
            "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n",
            "***** Running training *****\n",
            "  Num examples = 600\n",
            "  Num Epochs = 5\n",
            "  Instantaneous batch size per device = 8\n",
            "  Total train batch size (w. parallel, distributed & accumulation) = 8\n",
            "  Gradient Accumulation steps = 1\n",
            "  Total optimization steps = 375\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='375' max='375' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [375/375 01:12, Epoch 5/5]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: left;\">\n",
              "      <th>Epoch</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "      <th>Accuracy</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>No log</td>\n",
              "      <td>0.292808</td>\n",
              "      <td>0.495000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2</td>\n",
              "      <td>No log</td>\n",
              "      <td>0.249983</td>\n",
              "      <td>0.495000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3</td>\n",
              "      <td>No log</td>\n",
              "      <td>0.260789</td>\n",
              "      <td>0.495000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4</td>\n",
              "      <td>No log</td>\n",
              "      <td>0.331679</td>\n",
              "      <td>0.495000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>5</td>\n",
              "      <td>No log</td>\n",
              "      <td>0.460216</td>\n",
              "      <td>0.500000</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "***** Running Evaluation *****\n",
            "  Num examples = 600\n",
            "  Batch size = 8\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[0.71181035 0.71196043 0.71139836 0.7106704  0.71217597 0.712851\n",
            " 0.712562   0.71185577 0.71270484 0.7109373  0.7133547  0.7127246\n",
            " 0.71050286 0.71183807 0.71071726 0.71188885 0.7122783  0.712643\n",
            " 0.71236    0.71211743 0.7118545  0.71064794 0.71208364 0.71343553\n",
            " 0.712055   0.7126492  0.71290755 0.71162224 0.71108234 0.7110768\n",
            " 0.71196586 0.7109225  0.71247894 0.7116919  0.71284074 0.71156204\n",
            " 0.712222   0.7131896  0.7117051  0.71228755 0.7121426  0.71376026\n",
            " 0.71155924 0.7116823  0.71265906 0.71291286 0.7114619  0.7116993\n",
            " 0.71188724 0.711385   0.71191657 0.71155035 0.7122624  0.71253574\n",
            " 0.7116257  0.7121468  0.71137685 0.71206605 0.71180034 0.71315783\n",
            " 0.710918   0.71139985 0.71155274 0.71100605 0.71172094 0.7115129\n",
            " 0.7128883  0.7131497  0.71113974 0.7113478  0.7129093  0.7115997\n",
            " 0.7115689  0.71220666 0.7118974  0.7123105  0.7118417  0.71055514\n",
            " 0.7116182  0.7116751  0.7120293  0.7126617  0.71220803 0.71225613\n",
            " 0.7120724  0.7121313  0.7128567  0.7123034  0.71160066 0.7119371\n",
            " 0.71106696 0.7115252  0.71176755 0.7110763  0.71164495 0.7123977\n",
            " 0.71228313 0.7117709  0.7116252  0.71103007 0.71199346 0.7126942\n",
            " 0.711767   0.7117019  0.7119961  0.7128635  0.7113677  0.7124294\n",
            " 0.7109001  0.71272564 0.7131072  0.7114002  0.71247935 0.7119657\n",
            " 0.7120405  0.7123697  0.7122554  0.71116793 0.71165633 0.7124571\n",
            " 0.71088827 0.71137875 0.71224517 0.71118313 0.71296424 0.712499\n",
            " 0.7127439  0.7123586  0.71159804 0.7127054  0.7113164  0.71247315\n",
            " 0.7129541  0.71234125 0.7107763  0.71219593 0.71261185 0.71218956\n",
            " 0.7129198  0.71159744 0.71270376 0.7111071  0.71125627 0.7107247\n",
            " 0.71291226 0.71148384 0.7120104  0.7126399  0.7119117  0.7106413\n",
            " 0.71327883 0.7112631  0.7107206  0.71163434 0.7116122  0.71131116\n",
            " 0.7112469  0.71113974 0.71329117 0.71312225 0.71075505 0.7130743\n",
            " 0.7114835  0.71129763 0.71294117 0.71216327 0.7109318  0.7112452\n",
            " 0.71181774 0.7113476  0.71144074 0.71239376 0.71277833 0.71162987\n",
            " 0.71208996 0.7118872  0.71292514 0.7134486  0.7113414  0.71246463\n",
            " 0.7110548  0.7122731  0.7130314  0.7117258  0.7121369  0.7113613\n",
            " 0.7113534  0.711989   0.7118832  0.71280897 0.71269524 0.7115782\n",
            " 0.7123946  0.71273583 0.7116613  0.7111053  0.71208143 0.7132178\n",
            " 0.7116427  0.7113669  0.71300805 0.71173775 0.7119078  0.7110367\n",
            " 0.7121835  0.71296    0.7123733  0.71319693 0.71174115 0.71162397\n",
            " 0.71065813 0.71157    0.71227753 0.7121987  0.7119474  0.7112856\n",
            " 0.7122214  0.7132253  0.71079355 0.7103667  0.71301305 0.71185464\n",
            " 0.7114349  0.71100235 0.712671   0.7118973  0.7123823  0.71281725\n",
            " 0.71150583 0.711091   0.71093804 0.7123542  0.7117042  0.71190375\n",
            " 0.7121142  0.7119803  0.71107537 0.7118494  0.7114486  0.7121511\n",
            " 0.71278405 0.711546   0.71084034 0.71290874 0.71124774 0.7114143\n",
            " 0.712275   0.7131917  0.7113565  0.71207803 0.7119245  0.71273595\n",
            " 0.7116546  0.7123286  0.7132702  0.71249706 0.7125882  0.71298033\n",
            " 0.7125332  0.7134059  0.7111065  0.7119254  0.71283704 0.71101326\n",
            " 0.7121509  0.71144944 0.71238005 0.71097374 0.7131294  0.71310633\n",
            " 0.71291494 0.7128412  0.7119492  0.71115834 0.7108564  0.71117514\n",
            " 0.71156645 0.7134817  0.7129755  0.7111879  0.7124935  0.71270233\n",
            " 0.7106081  0.7132432  0.711468   0.7129112  0.71166134 0.7124989\n",
            " 0.7116925  0.71322554 0.7120081  0.712098   0.7127607  0.71190006\n",
            " 0.71281546 0.7112173  0.71192765 0.71144366 0.7119373  0.71234065\n",
            " 0.71156555 0.7115583  0.7137959  0.71161526 0.7121217  0.7120729\n",
            " 0.7118051  0.7123232  0.71224034 0.7134027  0.7120593  0.7121965\n",
            " 0.71053857 0.71216893 0.71176183 0.71212596 0.7116201  0.7107446\n",
            " 0.7116796  0.7123518  0.71372676 0.71270233 0.71201634 0.7131211\n",
            " 0.712043   0.71089274 0.7125194  0.7105172  0.7126603  0.7118194\n",
            " 0.71179426 0.71105933 0.7122753  0.7125177  0.7122631  0.7114926\n",
            " 0.7112496  0.71169627 0.7112243  0.71193653 0.7127282  0.711155\n",
            " 0.71289486 0.71195984 0.7134161  0.71115047 0.7122215  0.7126065\n",
            " 0.7112982  0.71178687 0.71138334 0.7124484  0.7122787  0.71208566\n",
            " 0.7111365  0.7123784  0.7128948  0.7119292  0.71125644 0.71185493\n",
            " 0.710974   0.71265835 0.71212006 0.7110984  0.7115693  0.7110702\n",
            " 0.7113309  0.7108108  0.71365446 0.7117813  0.71242315 0.71155876\n",
            " 0.7129291  0.7118049  0.7116634  0.71204704 0.7120574  0.7107199\n",
            " 0.71187407 0.71308935 0.71188444 0.7122761  0.71276975 0.7115098\n",
            " 0.7109938  0.71252483 0.7122434  0.7111665  0.71257234 0.71213734\n",
            " 0.7130656  0.7114181  0.7114202  0.711621   0.7125116  0.7109575\n",
            " 0.71138483 0.7116586  0.71112424 0.7120664  0.71217585 0.7121049\n",
            " 0.71140695 0.7113703  0.71177846 0.7110257  0.7119074  0.71250844\n",
            " 0.71129435 0.7127249  0.71222866 0.7114121  0.7114358  0.7122824\n",
            " 0.711591   0.71306556 0.7127582  0.71350056 0.7118845  0.7115642\n",
            " 0.71338147 0.7115925  0.7123687  0.7116883  0.7124221  0.7126431\n",
            " 0.71208745 0.71154267 0.7120199  0.71203876 0.7122801  0.7119732\n",
            " 0.71207255 0.7117858  0.71233314 0.7131874  0.71170324 0.71065265\n",
            " 0.71119875 0.71197236 0.7113038  0.71190417 0.7118719  0.71149063\n",
            " 0.7114006  0.71190816 0.71077794 0.71211165 0.71126777 0.7113528\n",
            " 0.7117801  0.71212345 0.71215963 0.7120855  0.7129732  0.71114606\n",
            " 0.7113913  0.7111364  0.711866   0.71058434 0.71149015 0.7119599\n",
            " 0.7119015  0.7121514  0.71328306 0.7112273  0.71252483 0.7107663\n",
            " 0.71202123 0.71051675 0.71184456 0.7110471  0.71238744 0.7108\n",
            " 0.7128365  0.71057546 0.71175474 0.71204835 0.71235645 0.7110897\n",
            " 0.7113605  0.7116422  0.71172065 0.71113926 0.7117962  0.71231973\n",
            " 0.7111913  0.7111096  0.7116405  0.7129233  0.7122276  0.71139836\n",
            " 0.71238285 0.7124738  0.7114177  0.7110258  0.71213347 0.71154416\n",
            " 0.71113235 0.712689   0.7122256  0.71100837 0.71160007 0.7119797\n",
            " 0.7124888  0.7120854  0.71133727 0.7120644  0.7127037  0.7116022\n",
            " 0.71291083 0.7120548  0.711951   0.7111137  0.7126894  0.7116265\n",
            " 0.71130025 0.7131283  0.7113255  0.7126299  0.7109776  0.7115135\n",
            " 0.71218354 0.71221757 0.71117264 0.71126    0.71155596 0.7124818\n",
            " 0.7114049  0.7121612  0.71199936 0.7114426  0.7116507  0.71178734\n",
            " 0.71173334 0.71128184 0.712674   0.7138362  0.7118094  0.7110085\n",
            " 0.71288437 0.7108624  0.71079355 0.7113499  0.7112034  0.710607\n",
            " 0.7121685  0.71104944 0.7120296  0.7122265  0.7119916  0.7125653\n",
            " 0.7109735  0.71142906 0.71241826 0.71203345 0.71173996 0.7128666\n",
            " 0.7113002  0.7122402  0.71323687 0.7120919  0.7110205  0.7116086\n",
            " 0.7124045  0.7126688  0.7109849  0.71239376 0.71165675 0.71133214\n",
            " 0.7122084  0.71234065 0.7114765  0.71138287 0.7117579  0.71240467\n",
            " 0.71174085 0.7122474  0.71240056 0.7120873  0.7118856  0.711868\n",
            " 0.7110139  0.7125643  0.7115036  0.71159244 0.7113847  0.7120044\n",
            " 0.7115715  0.7117628  0.7117539  0.7120814  0.71237046 0.7123352\n",
            " 0.7130966  0.7105733  0.7112757  0.71189564 0.71122384 0.7115711 ]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Saving model checkpoint to /content/drive/MyDrive/checkpoint-75\n",
            "Configuration saved in /content/drive/MyDrive/checkpoint-75/config.json\n",
            "Model weights saved in /content/drive/MyDrive/checkpoint-75/pytorch_model.bin\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 600\n",
            "  Batch size = 8\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[0.49604106 0.49530867 0.49521512 0.49148253 0.49633965 0.49837735\n",
            " 0.4972286  0.4953443  0.4982454  0.49204758 0.50122344 0.49984616\n",
            " 0.48851737 0.49623358 0.49032107 0.49628526 0.49555403 0.49943954\n",
            " 0.49416265 0.49670994 0.49562237 0.4904513  0.4984675  0.4990095\n",
            " 0.4966282  0.49882945 0.5003596  0.49511486 0.49222216 0.4916072\n",
            " 0.49660504 0.49349123 0.49765605 0.49254638 0.49875945 0.49073493\n",
            " 0.49733314 0.5008506  0.4966455  0.49867925 0.49807024 0.49985588\n",
            " 0.49632552 0.49505168 0.4962924  0.5010338  0.49620035 0.49685723\n",
            " 0.4965236  0.49089286 0.4974896  0.49545822 0.49678466 0.4981721\n",
            " 0.49534827 0.49562427 0.49411258 0.49736524 0.49424705 0.5004557\n",
            " 0.492607   0.4949492  0.493002   0.49190402 0.49505824 0.49425143\n",
            " 0.49634528 0.500166   0.49276295 0.49384254 0.5002364  0.49501494\n",
            " 0.49542174 0.49799347 0.49406445 0.4972664  0.49649432 0.49081734\n",
            " 0.49343345 0.49565348 0.49732807 0.49878976 0.4970534  0.4950496\n",
            " 0.49784324 0.49565446 0.49886754 0.4974294  0.49317437 0.49501097\n",
            " 0.49343264 0.49303225 0.49782282 0.4928615  0.49236166 0.496776\n",
            " 0.49682537 0.49377307 0.49415383 0.49006262 0.49802658 0.49887672\n",
            " 0.49370274 0.49472824 0.49516225 0.49730062 0.49073914 0.49803764\n",
            " 0.49200442 0.49659786 0.4992375  0.4936396  0.49759    0.49594164\n",
            " 0.4946633  0.49715456 0.497687   0.49537745 0.4943215  0.49519968\n",
            " 0.4931171  0.4925493  0.49726477 0.49370393 0.49702197 0.49838448\n",
            " 0.49627364 0.4972764  0.49634746 0.4993952  0.49599692 0.49773267\n",
            " 0.49667224 0.4983626  0.49383903 0.49585328 0.4967629  0.4967794\n",
            " 0.49931544 0.49460623 0.49610627 0.49305212 0.49389803 0.4885462\n",
            " 0.49891022 0.49381948 0.49598473 0.49543586 0.49777266 0.49059606\n",
            " 0.49909958 0.49379998 0.4918249  0.4932523  0.4958827  0.4955057\n",
            " 0.49407503 0.49127924 0.49845156 0.5000357  0.49384448 0.5006026\n",
            " 0.49262848 0.4947083  0.49526063 0.495887   0.49277252 0.49301538\n",
            " 0.49497262 0.49403682 0.49429888 0.49674603 0.4989232  0.49282324\n",
            " 0.49373484 0.4974814  0.50042456 0.4987082  0.49532852 0.4965895\n",
            " 0.49063012 0.49614877 0.49986002 0.49577248 0.49436083 0.49561846\n",
            " 0.4950191  0.4967938  0.49517325 0.49872717 0.49828708 0.49870148\n",
            " 0.49677253 0.49742603 0.49575868 0.4924544  0.4971081  0.49642715\n",
            " 0.49509078 0.49460545 0.49769482 0.49461424 0.4957699  0.49144274\n",
            " 0.4971658  0.4996651  0.49939254 0.50150275 0.49469528 0.49583828\n",
            " 0.49046504 0.4938191  0.49553403 0.49826038 0.4934574  0.49580684\n",
            " 0.49828386 0.50045854 0.4906138  0.4894741  0.49737045 0.4966523\n",
            " 0.49585015 0.4924974  0.49891642 0.494361   0.4956944  0.49770844\n",
            " 0.49633336 0.49587473 0.49020514 0.49653286 0.49637473 0.496817\n",
            " 0.4957809  0.4922752  0.49242854 0.4961971  0.4933429  0.49513516\n",
            " 0.49832872 0.49699706 0.49196038 0.498891   0.49597162 0.49455002\n",
            " 0.4976251  0.4978889  0.4917842  0.49711818 0.4962892  0.49877277\n",
            " 0.49629635 0.4965298  0.49977556 0.4981281  0.4976693  0.49912062\n",
            " 0.49842873 0.49940312 0.4927237  0.49683496 0.49983022 0.49409825\n",
            " 0.49627295 0.49589097 0.49765635 0.49116272 0.49980667 0.4982026\n",
            " 0.49955586 0.49579686 0.49529028 0.4943514  0.49096668 0.49452674\n",
            " 0.4968156  0.50041425 0.49699557 0.49376467 0.4994357  0.4986498\n",
            " 0.49132678 0.5024542  0.49221697 0.4970518  0.49401566 0.4978731\n",
            " 0.49329844 0.49955565 0.49621552 0.49675977 0.49942696 0.49779162\n",
            " 0.49852657 0.49350458 0.49552152 0.49378923 0.4946275  0.4966967\n",
            " 0.49188238 0.49604344 0.49964958 0.49115124 0.49662676 0.49634695\n",
            " 0.4977409  0.4970426  0.49430454 0.49736622 0.49521363 0.49398622\n",
            " 0.49081606 0.49621475 0.49592438 0.49646577 0.4969326  0.49116844\n",
            " 0.4979775  0.49643394 0.5017081  0.5003897  0.4956631  0.5015782\n",
            " 0.4969169  0.49199796 0.49853665 0.49038994 0.50051004 0.49640018\n",
            " 0.4973018  0.4935128  0.4972533  0.496614   0.49746493 0.49581164\n",
            " 0.49371296 0.4959331  0.49415442 0.4964877  0.49973124 0.49272305\n",
            " 0.49622715 0.49562678 0.49959645 0.4927632  0.49889272 0.49456644\n",
            " 0.49050072 0.49450952 0.4941831  0.49575216 0.49562013 0.49801648\n",
            " 0.49197215 0.49637696 0.49618414 0.49559477 0.49363944 0.49654353\n",
            " 0.49375528 0.49874207 0.4941037  0.49184147 0.49480933 0.49264717\n",
            " 0.4914315  0.4919781  0.50031936 0.4945599  0.4987139  0.49599782\n",
            " 0.49770138 0.4927848  0.49542877 0.4980783  0.4959772  0.4921588\n",
            " 0.49405843 0.49736667 0.49627    0.49614447 0.49922016 0.49577293\n",
            " 0.49136165 0.49785298 0.4961816  0.49442163 0.4982339  0.49641347\n",
            " 0.50073785 0.49328244 0.4947554  0.49558222 0.50003946 0.4893906\n",
            " 0.49279052 0.49567026 0.49295047 0.49429032 0.49582997 0.49875247\n",
            " 0.4946038  0.49599823 0.49345505 0.49271327 0.49608138 0.4988385\n",
            " 0.49022213 0.49765164 0.49666834 0.4958762  0.4944728  0.4976467\n",
            " 0.49673384 0.49600792 0.49723426 0.49632326 0.49801132 0.49491414\n",
            " 0.49755627 0.49297324 0.49762452 0.4974504  0.4970358  0.49633655\n",
            " 0.49724734 0.49515668 0.49712682 0.49627265 0.4963226  0.49610287\n",
            " 0.49892762 0.4946992  0.4983959  0.50258356 0.496966   0.49161777\n",
            " 0.49230522 0.49777937 0.49576262 0.49415794 0.4966856  0.49461403\n",
            " 0.49496606 0.49773526 0.49209467 0.49437624 0.4935642  0.49447948\n",
            " 0.49730244 0.49852848 0.49613494 0.49647915 0.49977943 0.49463254\n",
            " 0.49485368 0.4907828  0.49749523 0.49064985 0.49659976 0.49470377\n",
            " 0.49685007 0.49699053 0.501946   0.49453846 0.4988827  0.4926868\n",
            " 0.49576858 0.4902245  0.49638987 0.4928731  0.4927428  0.49287993\n",
            " 0.49905214 0.4883018  0.49546698 0.4950771  0.49723318 0.4930877\n",
            " 0.49435768 0.49494016 0.4966653  0.49310958 0.49522895 0.49662355\n",
            " 0.49532557 0.49455163 0.4938391  0.49841133 0.49694377 0.49371123\n",
            " 0.49811345 0.49750426 0.49553627 0.4941912  0.4963936  0.4958375\n",
            " 0.4917015  0.49894962 0.49632782 0.49037927 0.49546406 0.4968135\n",
            " 0.49789023 0.4971792  0.4933355  0.49721554 0.49544808 0.49473476\n",
            " 0.49609175 0.49687698 0.49839795 0.4935903  0.4979298  0.49678746\n",
            " 0.49489626 0.49635175 0.4941199  0.4977042  0.49124393 0.49555123\n",
            " 0.49682656 0.49730006 0.4949103  0.49565506 0.49612936 0.4961498\n",
            " 0.49357164 0.49425957 0.49582383 0.49499154 0.49585536 0.4955186\n",
            " 0.4950226  0.4926009  0.49749103 0.5031837  0.4929428  0.4933158\n",
            " 0.4994786  0.4882772  0.49058202 0.49454963 0.49377856 0.48852304\n",
            " 0.49747425 0.49488166 0.49745485 0.4979732  0.4957959  0.49611017\n",
            " 0.4922664  0.49265453 0.49841353 0.49535286 0.49322248 0.4983869\n",
            " 0.49529487 0.49451172 0.49832827 0.497454   0.49476343 0.49486068\n",
            " 0.4934847  0.497574   0.49213237 0.49255118 0.496327   0.49527955\n",
            " 0.49681988 0.49717692 0.49018633 0.49680746 0.495315   0.49668184\n",
            " 0.4938255  0.49483335 0.4967673  0.4977254  0.49529713 0.49243262\n",
            " 0.49201685 0.49788442 0.49500304 0.49581987 0.49502403 0.49452916\n",
            " 0.49593607 0.4950957  0.49626595 0.49630263 0.49824497 0.49675524\n",
            " 0.49802586 0.48832673 0.49105012 0.49569553 0.49141514 0.49112067]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Saving model checkpoint to /content/drive/MyDrive/checkpoint-150\n",
            "Configuration saved in /content/drive/MyDrive/checkpoint-150/config.json\n",
            "Model weights saved in /content/drive/MyDrive/checkpoint-150/pytorch_model.bin\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 600\n",
            "  Batch size = 8\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[0.6061428  0.610561   0.6020228  0.5956341  0.6136137  0.63066065\n",
            " 0.6160069  0.61410344 0.6125416  0.5790343  0.6232357  0.62445134\n",
            " 0.58096975 0.6229216  0.58146363 0.59739774 0.60994464 0.63180184\n",
            " 0.59367865 0.60887367 0.6140466  0.5992787  0.61877304 0.6270882\n",
            " 0.61101687 0.62314534 0.6213021  0.6117768  0.5816833  0.5803316\n",
            " 0.6186928  0.5894218  0.62033737 0.5917224  0.6253814  0.58580965\n",
            " 0.6111869  0.62752676 0.605986   0.613935   0.6254478  0.6261935\n",
            " 0.6103256  0.605387   0.61535436 0.63482463 0.606146   0.6055267\n",
            " 0.6142055  0.58900154 0.62341136 0.6135552  0.6162419  0.6133204\n",
            " 0.6053133  0.62638426 0.60667425 0.604705   0.59814537 0.6242548\n",
            " 0.593844   0.5944227  0.59234697 0.59794915 0.60854876 0.60176814\n",
            " 0.6194419  0.6305209  0.59719056 0.5975523  0.6216803  0.61574906\n",
            " 0.5981003  0.61962247 0.59720004 0.613281   0.6127383  0.59405565\n",
            " 0.60666007 0.5999585  0.6164135  0.61359876 0.6056512  0.6046171\n",
            " 0.6227396  0.59777415 0.6240101  0.6184081  0.5955096  0.6146916\n",
            " 0.5922218  0.5991361  0.6153035  0.5914787  0.60391784 0.6223725\n",
            " 0.61100316 0.59362966 0.6118265  0.5890391  0.60959554 0.61224294\n",
            " 0.5990292  0.60085166 0.6085008  0.6121449  0.59908915 0.62352264\n",
            " 0.59676504 0.61715966 0.6261132  0.60294354 0.61470693 0.6129931\n",
            " 0.60271275 0.61821425 0.6154333  0.6008971  0.5968258  0.61046386\n",
            " 0.6021999  0.5902915  0.6223807  0.61139494 0.6144973  0.61822635\n",
            " 0.6145649  0.6078095  0.6105066  0.6216062  0.61120045 0.6114179\n",
            " 0.61792934 0.6112637  0.61290234 0.61161864 0.62198716 0.62491316\n",
            " 0.6205703  0.6007373  0.61515784 0.59600747 0.6083179  0.5766935\n",
            " 0.62344635 0.60710615 0.6044457  0.60407877 0.60587746 0.5841015\n",
            " 0.62796783 0.61090326 0.5953114  0.60828376 0.6159173  0.6045062\n",
            " 0.6084405  0.5989999  0.62095535 0.6227787  0.5982449  0.6256568\n",
            " 0.59017205 0.6058666  0.6141212  0.6073153  0.5955209  0.5997763\n",
            " 0.6091233  0.59130704 0.5959359  0.6203173  0.6257908  0.598398\n",
            " 0.60926056 0.6086472  0.6208566  0.61733913 0.610694   0.6199274\n",
            " 0.588115   0.6122514  0.6283203  0.6066811  0.60375065 0.6071192\n",
            " 0.5953546  0.6173085  0.60742825 0.6250871  0.6163842  0.609186\n",
            " 0.61581016 0.60978323 0.6094789  0.5953098  0.62209284 0.6163602\n",
            " 0.61076844 0.60311306 0.6153329  0.6028125  0.6175028  0.5836166\n",
            " 0.62861335 0.6248035  0.6211345  0.6389495  0.59664404 0.60768193\n",
            " 0.5798999  0.61012435 0.6152902  0.6234622  0.58902025 0.6100707\n",
            " 0.62176925 0.6250595  0.5877756  0.58424014 0.61564255 0.60954344\n",
            " 0.59554315 0.59168    0.6279133  0.59440213 0.6200394  0.61971736\n",
            " 0.61179024 0.5920354  0.5924234  0.62236553 0.60850346 0.6088235\n",
            " 0.6142837  0.59622645 0.5949722  0.61920446 0.5997303  0.61759895\n",
            " 0.6197004  0.5978453  0.5874234  0.6236037  0.6039963  0.5977593\n",
            " 0.6109289  0.6171725  0.5920304  0.6188463  0.6012408  0.6193197\n",
            " 0.6027376  0.6238032  0.6314677  0.62565374 0.6113693  0.6218928\n",
            " 0.6168923  0.62067366 0.6045427  0.6205651  0.62670124 0.5986501\n",
            " 0.60938454 0.6175679  0.6164821  0.59278333 0.6212877  0.6198903\n",
            " 0.61542916 0.6122206  0.60790193 0.59401816 0.5794507  0.61271906\n",
            " 0.6017463  0.62351507 0.62020564 0.59659827 0.6151014  0.6170236\n",
            " 0.59650785 0.6354309  0.591204   0.6277551  0.5931953  0.6177169\n",
            " 0.6140262  0.6226635  0.61745435 0.59689206 0.62196827 0.61128336\n",
            " 0.6231732  0.58270574 0.60847384 0.604203   0.6088661  0.6088506\n",
            " 0.5958867  0.6041989  0.6223613  0.59112424 0.6141128  0.6121958\n",
            " 0.61212015 0.61714935 0.59889144 0.61725175 0.59579015 0.5997282\n",
            " 0.5884485  0.6046157  0.61315197 0.6205187  0.6150828  0.5923829\n",
            " 0.6176757  0.61380035 0.63532794 0.63215166 0.61727864 0.63021165\n",
            " 0.6094195  0.59958124 0.61741126 0.60061115 0.6327419  0.6011775\n",
            " 0.608184   0.6042426  0.6113896  0.6184776  0.6160553  0.60276014\n",
            " 0.6090373  0.61585975 0.5955174  0.611449   0.62466896 0.60033697\n",
            " 0.612793   0.61691415 0.61788696 0.59283507 0.62519765 0.6105917\n",
            " 0.5966916  0.61294645 0.60870767 0.6095719  0.61026317 0.61618394\n",
            " 0.5978879  0.6138258  0.615471   0.6151167  0.59809107 0.6124594\n",
            " 0.601437   0.625008   0.6040635  0.59228545 0.61108965 0.59975576\n",
            " 0.58794504 0.59555554 0.6231952  0.6138245  0.61660486 0.6014945\n",
            " 0.6268727  0.6006671  0.607597   0.6217172  0.6119519  0.6000239\n",
            " 0.604975   0.6185041  0.6011458  0.6072379  0.6162837  0.6027498\n",
            " 0.58119476 0.62045854 0.608361   0.60130835 0.62323856 0.6092581\n",
            " 0.62890136 0.60882056 0.5958013  0.60769606 0.61989814 0.58493114\n",
            " 0.59708625 0.6114634  0.5977752  0.61071324 0.6062187  0.6167033\n",
            " 0.6038147  0.60527617 0.598919   0.5835867  0.6118478  0.6096974\n",
            " 0.5885306  0.61609733 0.6129007  0.60717994 0.61220825 0.60804296\n",
            " 0.5957451  0.6136409  0.6140182  0.60471994 0.6194232  0.61093295\n",
            " 0.62562025 0.6071963  0.622903   0.6229407  0.6209582  0.6187711\n",
            " 0.6159528  0.6138998  0.61348724 0.6150207  0.6127783  0.60361385\n",
            " 0.61961436 0.6050545  0.6153068  0.6300065  0.6105211  0.5893556\n",
            " 0.5920478  0.6093644  0.601552   0.60786146 0.6071402  0.5997784\n",
            " 0.6152248  0.60266507 0.59340674 0.6052698  0.6056099  0.600551\n",
            " 0.61754084 0.60882825 0.6109003  0.6117173  0.62406206 0.59523374\n",
            " 0.59690577 0.5839772  0.6095181  0.58683103 0.6146424  0.6146546\n",
            " 0.6196231  0.6113095  0.62482715 0.60302347 0.6150472  0.59545314\n",
            " 0.60153174 0.5967175  0.6067463  0.59571487 0.59041065 0.6090139\n",
            " 0.6346663  0.576674   0.61412233 0.5995246  0.60877    0.5974733\n",
            " 0.6014744  0.61107117 0.6176419  0.5956152  0.60916466 0.6184924\n",
            " 0.59736836 0.6016322  0.6001411  0.6216708  0.6114308  0.6015139\n",
            " 0.6183383  0.6117244  0.6060334  0.5993531  0.61346483 0.60704684\n",
            " 0.58808964 0.62598276 0.61771667 0.5905369  0.6035184  0.6069723\n",
            " 0.6108757  0.6027965  0.58751225 0.61414987 0.61667925 0.60418123\n",
            " 0.61444265 0.6152229  0.61467665 0.59703773 0.62731993 0.61279106\n",
            " 0.61150664 0.61369646 0.5993379  0.63010865 0.5799318  0.6151233\n",
            " 0.62504953 0.6049822  0.58979553 0.6042541  0.606561   0.61774606\n",
            " 0.59299314 0.6105508  0.61176604 0.607711   0.5957407  0.6021621\n",
            " 0.6053393  0.5941358  0.6235111  0.6333436  0.6095302  0.60074556\n",
            " 0.62335604 0.58617306 0.5812215  0.59384584 0.60105383 0.5907365\n",
            " 0.61973506 0.60653895 0.6171995  0.6179317  0.6090927  0.61276865\n",
            " 0.5920224  0.6119105  0.61407244 0.61606365 0.59292823 0.6152616\n",
            " 0.62266177 0.6126281  0.6228788  0.6031664  0.60678506 0.6163428\n",
            " 0.60114044 0.61861086 0.5876104  0.60004216 0.5991734  0.6131578\n",
            " 0.59996885 0.62079304 0.5860363  0.60986114 0.60647064 0.6213853\n",
            " 0.6154499  0.6039625  0.62223375 0.6209807  0.61882704 0.5877601\n",
            " 0.58641344 0.61305016 0.6015725  0.61525303 0.60389805 0.61201465\n",
            " 0.6077926  0.60147524 0.61211425 0.605387   0.62338024 0.6181951\n",
            " 0.61009556 0.5908253  0.58657616 0.5987067  0.5997397  0.600548  ]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Saving model checkpoint to /content/drive/MyDrive/checkpoint-225\n",
            "Configuration saved in /content/drive/MyDrive/checkpoint-225/config.json\n",
            "Model weights saved in /content/drive/MyDrive/checkpoint-225/pytorch_model.bin\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 600\n",
            "  Batch size = 8\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[0.7907864  0.7693038  0.77910453 0.8354303  0.8004541  0.7856768\n",
            " 0.7867111  0.7378673  0.8433913  0.8454407  0.76291174 0.74260694\n",
            " 0.8425393  0.7753131  0.869392   0.7789111  0.7849271  0.77446514\n",
            " 0.8129709  0.790763   0.7767129  0.8748028  0.74664766 0.82341105\n",
            " 0.78672063 0.775474   0.80844927 0.76281613 0.84732693 0.8441023\n",
            " 0.76572645 0.8069097  0.8030061  0.808015   0.78787607 0.786453\n",
            " 0.7842104  0.78178    0.7952957  0.82008207 0.7448947  0.77633333\n",
            " 0.8178788  0.81385034 0.7941089  0.7716964  0.7237737  0.7516978\n",
            " 0.7883213  0.7622635  0.7622427  0.7610239  0.7657693  0.8082181\n",
            " 0.82380193 0.75601596 0.7960599  0.69971216 0.75414383 0.7788166\n",
            " 0.8132168  0.79911846 0.8509171  0.8092199  0.84617555 0.8345424\n",
            " 0.78822243 0.75595456 0.8081868  0.80339974 0.73474157 0.8029581\n",
            " 0.8185637  0.7575072  0.7951936  0.79077715 0.78127855 0.84909916\n",
            " 0.7689476  0.7844475  0.7338511  0.7895025  0.76767963 0.79228514\n",
            " 0.73417705 0.7573265  0.8133088  0.781023   0.82817364 0.8280764\n",
            " 0.84028685 0.73613346 0.77084106 0.78741777 0.7870868  0.79872364\n",
            " 0.8160153  0.7425068  0.8107358  0.7784039  0.77279377 0.80743784\n",
            " 0.84985214 0.78992957 0.78775156 0.8015979  0.78340536 0.7731158\n",
            " 0.78987646 0.7759223  0.7507708  0.7997933  0.8075032  0.6728936\n",
            " 0.8051659  0.72764105 0.7861834  0.79701185 0.7841343  0.8047124\n",
            " 0.7594896  0.81629837 0.7763675  0.72720873 0.7959053  0.81568575\n",
            " 0.83241385 0.74169403 0.74964476 0.747407   0.7753428  0.8128011\n",
            " 0.80666935 0.77558553 0.7564955  0.74902564 0.79905915 0.7762126\n",
            " 0.7825114  0.79938275 0.77891165 0.78862745 0.8096758  0.8342614\n",
            " 0.73132795 0.766634   0.8498182  0.74761736 0.8025108  0.8570709\n",
            " 0.8049924  0.7338898  0.74627    0.77190226 0.8046317  0.77194136\n",
            " 0.7767603  0.7445994  0.79392076 0.82136464 0.81116754 0.7566073\n",
            " 0.8140101  0.74800205 0.7737242  0.7573044  0.7486559  0.8023545\n",
            " 0.7734305  0.81191283 0.80428463 0.7944576  0.7856187  0.7817031\n",
            " 0.8388839  0.7778352  0.8070711  0.7809386  0.83085674 0.8266267\n",
            " 0.78462905 0.83069897 0.79035324 0.77380115 0.7887138  0.7976411\n",
            " 0.7509226  0.7404158  0.7630516  0.8025507  0.78680223 0.8463142\n",
            " 0.8160842  0.81762654 0.78713787 0.78099906 0.6550492  0.7672294\n",
            " 0.76591533 0.8329578  0.8220709  0.8226593  0.76388544 0.8174883\n",
            " 0.75288796 0.80512595 0.76052    0.7585438  0.78898627 0.76322883\n",
            " 0.8598884  0.8197784  0.81509435 0.82871747 0.76662153 0.81415397\n",
            " 0.7101161  0.7717049  0.7875632  0.80540043 0.8251324  0.78423357\n",
            " 0.8147352  0.8153278  0.75979435 0.81317514 0.82264596 0.7707451\n",
            " 0.7835025  0.80383426 0.7746337  0.7902843  0.8260909  0.7822595\n",
            " 0.77188164 0.7678651  0.78967744 0.8055272  0.82753223 0.79991966\n",
            " 0.73954874 0.8135458  0.85554546 0.7766258  0.80276984 0.79345185\n",
            " 0.80949783 0.8157905  0.90223837 0.8248421  0.7883273  0.8394297\n",
            " 0.78214896 0.8076409  0.7974522  0.7525127  0.8035823  0.8222754\n",
            " 0.8180524  0.8012547  0.80536586 0.72911674 0.7174349  0.8466367\n",
            " 0.77911097 0.77089477 0.773125   0.82287425 0.77105415 0.8225771\n",
            " 0.810544   0.8080065  0.78522015 0.78241426 0.811339   0.7792048\n",
            " 0.8331445  0.79499596 0.8177909  0.75212115 0.7697497  0.7672496\n",
            " 0.8178331  0.73103744 0.7560943  0.8118679  0.8703253  0.8298961\n",
            " 0.7962569  0.8013651  0.81934005 0.80525285 0.76455027 0.8181345\n",
            " 0.8078017  0.7929586  0.7673423  0.7776667  0.7950403  0.782517\n",
            " 0.77815014 0.7953754  0.80446935 0.7950774  0.7977889  0.8044877\n",
            " 0.77385825 0.8176216  0.7622445  0.8170142  0.7671241  0.8482385\n",
            " 0.8329868  0.8169789  0.79602563 0.7837761  0.7498474  0.815566\n",
            " 0.71950084 0.77630204 0.7726881  0.7589637  0.7836202  0.7688958\n",
            " 0.7788421  0.8109245  0.77110356 0.75172174 0.7758659  0.78335583\n",
            " 0.7727726  0.7844035  0.7475529  0.74605596 0.772435   0.7712561\n",
            " 0.77115995 0.7673995  0.82758117 0.8332308  0.74342114 0.7799745\n",
            " 0.78165036 0.757575   0.85354817 0.8093642  0.7486459  0.74723774\n",
            " 0.81452215 0.7887725  0.7793343  0.8202433  0.80514526 0.7785064\n",
            " 0.8061249  0.7964863  0.7724242  0.74480546 0.7834555  0.8111422\n",
            " 0.7688872  0.76444346 0.7860115  0.8343679  0.79951876 0.81209284\n",
            " 0.80093807 0.7994433  0.8028846  0.82673204 0.753232   0.7540839\n",
            " 0.80176497 0.74492896 0.788078   0.7701004  0.8016507  0.85491663\n",
            " 0.7842153  0.80229557 0.764981   0.79877603 0.7628324  0.79039675\n",
            " 0.7819816  0.7751802  0.8283366  0.7836283  0.77751327 0.8146045\n",
            " 0.74130434 0.78686816 0.8673505  0.7715468  0.7838159  0.8501492\n",
            " 0.8577517  0.8036867  0.8042578  0.7731758  0.8391829  0.73823243\n",
            " 0.8031319  0.7774974  0.8134689  0.8247818  0.7803703  0.80617464\n",
            " 0.8190841  0.79172844 0.79539585 0.72278816 0.72838676 0.7894417\n",
            " 0.80999815 0.78259015 0.80240923 0.8011076  0.74084544 0.78134006\n",
            " 0.7955799  0.7411107  0.8193699  0.74960023 0.7683999  0.79170966\n",
            " 0.79796416 0.8235855  0.7698066  0.7756529  0.81088907 0.7633292\n",
            " 0.81440943 0.829884   0.77286965 0.69952905 0.7772398  0.8490511\n",
            " 0.8393956  0.7968573  0.74614364 0.82879525 0.72402304 0.7872439\n",
            " 0.80981994 0.8164922  0.7839522  0.78726447 0.7887363  0.81599814\n",
            " 0.7640252  0.7947084  0.8002268  0.78412706 0.79342574 0.80271083\n",
            " 0.81332743 0.8339243  0.7187339  0.8518955  0.763475   0.8686395\n",
            " 0.8005641  0.75330424 0.7368621  0.83642423 0.7396469  0.812341\n",
            " 0.82419556 0.78429055 0.79370475 0.7866915  0.8026833  0.7748036\n",
            " 0.7535203  0.8565173  0.7853838  0.7724071  0.7992773  0.80892587\n",
            " 0.79292303 0.82025737 0.7965463  0.8390731  0.77648914 0.81715196\n",
            " 0.8350805  0.7856576  0.8412718  0.7504169  0.78437734 0.845584\n",
            " 0.7892831  0.78966683 0.7891844  0.7381743  0.7784335  0.80135816\n",
            " 0.82365847 0.7562098  0.7625881  0.8027176  0.76126724 0.76840353\n",
            " 0.80569214 0.7571319  0.844907   0.78582865 0.7833279  0.83356315\n",
            " 0.81617016 0.6979262  0.71256095 0.7977683  0.7548365  0.770346\n",
            " 0.8143425  0.7707596  0.77015835 0.77503014 0.8618502  0.82200015\n",
            " 0.783239   0.8311918  0.8035901  0.74913543 0.7910371  0.81907934\n",
            " 0.7945171  0.77391446 0.7765593  0.79535097 0.7233169  0.78767186\n",
            " 0.77292526 0.7550608  0.78501284 0.8340023  0.7989088  0.8081521\n",
            " 0.7921539  0.80721074 0.85862935 0.79777884 0.7761033  0.81602776\n",
            " 0.771617   0.7608984  0.79117525 0.7578684  0.7885004  0.7990276\n",
            " 0.8298856  0.79416376 0.7633203  0.82017875 0.83542544 0.74728924\n",
            " 0.73247653 0.82061696 0.7878836  0.7485633  0.7442016  0.82154137\n",
            " 0.82397527 0.79954576 0.7884111  0.7682794  0.80816466 0.77296513\n",
            " 0.78401405 0.74709684 0.8103489  0.75245374 0.828365   0.7573032\n",
            " 0.78601676 0.7911435  0.8238491  0.7709144  0.7954014  0.730704\n",
            " 0.82631344 0.81505233 0.78310597 0.75386244 0.8141549  0.7681301\n",
            " 0.7277847  0.7725374  0.788047   0.8364466  0.8000488  0.6999787\n",
            " 0.81815857 0.7727874  0.82531744 0.7803356  0.73037934 0.7826874 ]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Saving model checkpoint to /content/drive/MyDrive/checkpoint-300\n",
            "Configuration saved in /content/drive/MyDrive/checkpoint-300/config.json\n",
            "Model weights saved in /content/drive/MyDrive/checkpoint-300/pytorch_model.bin\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 600\n",
            "  Batch size = 8\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[0.95884913 0.9362701  0.946042   1.0010352  0.98062605 0.9752625\n",
            " 0.9806158  0.92177624 1.0212125  0.9908372  0.9544889  0.94069326\n",
            " 0.9981889  0.971331   1.0087141  0.9535654  0.96500754 0.97795594\n",
            " 0.970015   0.97175145 0.956248   1.0230052  0.93130714 0.9951747\n",
            " 0.97432643 0.96753496 0.9870624  0.94235504 0.99520916 0.99789065\n",
            " 0.95395017 0.9699305  0.9819696  0.9697532  0.98220044 0.93966967\n",
            " 0.971958   0.9580221  0.9714167  0.99540454 0.94634134 0.96172225\n",
            " 0.99348825 0.9873543  0.97909075 0.9726418  0.8975039  0.9287923\n",
            " 0.97484607 0.9178661  0.94436276 0.9451091  0.94250536 0.9777749\n",
            " 0.9990448  0.93908507 0.9604432  0.8764992  0.91816187 0.9639073\n",
            " 0.9849275  0.9587651  0.9978423  0.981145   1.0065327  0.9895745\n",
            " 0.9603948  0.9403666  0.98132783 0.9747945  0.915248   0.98932856\n",
            " 0.98840344 0.9464731  0.9491453  0.9758887  0.96437544 1.0149089\n",
            " 0.937961   0.9567885  0.92662615 0.96633875 0.94185674 0.9567555\n",
            " 0.92466617 0.93250805 0.9899011  0.98614335 0.9760566  1.005091\n",
            " 0.99275535 0.90972286 0.9426507  0.96281195 0.96565163 0.99515307\n",
            " 0.9912622  0.9058849  0.9990768  0.92849    0.94916713 0.9798173\n",
            " 0.9963585  0.9640469  0.96224785 0.9573348  0.94576764 0.95999765\n",
            " 0.96176326 0.9584263  0.95316535 0.9614415  0.9884719  0.86136556\n",
            " 0.9639137  0.9178488  0.9710342  0.97014534 0.95120573 0.9694807\n",
            " 0.9476154  0.98259497 0.962068   0.9111873  0.9753313  0.9919696\n",
            " 0.99956894 0.92290074 0.93724024 0.9454682  0.9546403  0.9907412\n",
            " 0.98080516 0.94401884 0.9459494  0.9298004  0.9835143  0.9648135\n",
            " 0.9617369  0.96510553 0.9483963  0.9598511  0.98737377 0.9642059\n",
            " 0.9248152  0.94246626 1.0012158  0.90363395 0.9907662  1.00608\n",
            " 0.99237764 0.92310375 0.9193764  0.94218045 0.98602855 0.94881004\n",
            " 0.95504105 0.91219646 0.9749297  0.99343985 0.9758949  0.9498328\n",
            " 0.96295154 0.9310187  0.9336344  0.9340352  0.92861193 0.9567141\n",
            " 0.9446032  0.9700886  0.9668714  0.96776164 0.97215784 0.94805974\n",
            " 0.9981258  0.95453256 0.9927541  0.94919723 0.9946867  0.996886\n",
            " 0.9376726  1.0151187  0.9839389  0.9469967  0.9611919  0.9824393\n",
            " 0.91807425 0.9280914  0.9382274  0.98626935 0.977125   1.0000567\n",
            " 0.9933068  0.98011124 0.9627346  0.9585898  0.8495781  0.92282414\n",
            " 0.9432253  0.9928876  1.0108165  0.9932196  0.94211954 0.94939053\n",
            " 0.95200473 0.9898449  0.94688517 0.9618219  0.9559991  0.94171816\n",
            " 0.9944877  0.9922315  0.97984385 0.99586475 0.9205849  0.9874051\n",
            " 0.91027707 0.9565139  0.94050294 0.96362627 0.9983028  0.95909333\n",
            " 0.98595166 0.97295386 0.94441134 0.9500022  0.99929    0.94015574\n",
            " 0.96948993 0.96391946 0.9359406  0.96258855 0.9884176  0.97020036\n",
            " 0.96365947 0.9189624  0.95947504 0.9867775  0.98588693 0.9671864\n",
            " 0.9337633  0.9863698  0.99863356 0.9545804  0.98023796 0.9567535\n",
            " 0.98307574 0.98023576 1.0359849  1.0157255  0.9573612  1.023063\n",
            " 0.96249205 0.9910934  0.97365373 0.9404664  0.9733367  1.0050853\n",
            " 0.99136084 0.9815464  0.98290056 0.9250813  0.913393   1.0078286\n",
            " 0.9582886  0.9538728  0.958761   0.97790354 0.9573636  0.9918896\n",
            " 0.98032975 0.9679955  0.96114904 0.9506212  0.97460216 0.96488523\n",
            " 0.9956939  0.9734609  0.9836453  0.92609197 0.95247656 0.95649874\n",
            " 0.9760183  0.92199624 0.9118821  0.98351973 1.0017657  1.0006717\n",
            " 0.9856105  0.97374225 1.0040311  0.9651764  0.9492235  0.99184585\n",
            " 0.9832682  0.9593841  0.9497289  0.9544051  0.9623791  0.96103406\n",
            " 0.94479907 0.9623467  0.97311336 0.9378977  0.96316993 0.97597164\n",
            " 0.9624808  0.9935821  0.92839473 0.9689364  0.93098134 1.0089095\n",
            " 0.98731005 0.9767146  0.96671146 0.9721242  0.94093335 0.9679381\n",
            " 0.90428823 0.94199777 0.9663362  0.9658502  0.97247016 0.9543624\n",
            " 0.94559044 0.9749987  0.952177   0.9396449  0.97047484 0.9610794\n",
            " 0.9516902  0.9662465  0.9340439  0.9340612  0.9627116  0.93870735\n",
            " 0.95211506 0.949135   0.99724555 1.0014662  0.93609494 0.95802224\n",
            " 0.95462537 0.9310597  1.0241827  0.97756    0.9511125  0.9142262\n",
            " 0.9695105  0.9604498  0.95271236 0.98601514 0.96703297 0.9666341\n",
            " 0.9751223  0.9650143  0.93497497 0.92681235 0.9537831  0.98880774\n",
            " 0.95294094 0.94385326 0.958563   0.9874626  0.97916734 0.9673224\n",
            " 0.9525301  0.96280503 0.970145   0.9844321  0.94416106 0.9238681\n",
            " 0.9846917  0.90673184 0.9701701  0.9579066  0.98495895 1.0109732\n",
            " 0.95443684 0.97933733 0.94208395 0.97312087 0.95173407 0.95420223\n",
            " 0.94501173 0.9616715  1.0043994  0.94754875 0.95914745 0.9791007\n",
            " 0.9421111  0.9695239  1.0193814  0.949792   0.9648259  0.9824556\n",
            " 1.0058213  0.9819927  0.97122085 0.94996953 1.0099984  0.9237738\n",
            " 0.95991206 0.9579966  0.9785552  0.97537744 0.9646596  0.9773834\n",
            " 0.9684617  0.9715586  0.9596844  0.9018408  0.9136296  0.968158\n",
            " 0.971182   0.93647367 0.98184407 0.9568812  0.9386613  0.9634441\n",
            " 0.96853125 0.9192355  1.0056186  0.93323576 0.95978147 0.9659209\n",
            " 0.9656268  1.007572   0.9528225  0.9598285  0.99340755 0.94310486\n",
            " 0.9928897  0.9972955  0.94954115 0.89781505 0.95056075 0.9917808\n",
            " 0.9943721  0.9818711  0.92388743 0.98619974 0.9068654  0.96318203\n",
            " 0.97897166 0.9867398  0.95181125 0.9514682  0.9630643  0.98287135\n",
            " 0.9486617  0.96839726 0.9796412  0.9582392  0.9755192  0.9735155\n",
            " 0.97861516 0.9714532  0.90678865 0.9929786  0.94741094 1.0371106\n",
            " 0.9859738  0.9363821  0.91215914 0.9940775  0.9312614  0.98418975\n",
            " 1.0012215  0.9663617  0.9684024  0.9614351  0.9386757  0.95193034\n",
            " 0.94533354 0.98697925 0.9719909  0.9366538  0.96096045 0.9792288\n",
            " 0.9683125  0.9874451  0.972004   0.996128   0.9550607  0.9973302\n",
            " 0.9887105  0.9644927  0.9919732  0.9375139  0.9586342  1.018056\n",
            " 0.9737922  0.9693655  0.96590877 0.91840833 0.94904095 0.9860093\n",
            " 0.9755983  0.95205903 0.951959   0.96344656 0.93480694 0.9458702\n",
            " 0.9779209  0.9392494  0.9987182  0.97273076 0.9491734  1.0004332\n",
            " 0.98850495 0.8917237  0.90036106 0.9658154  0.94354165 0.9591254\n",
            " 0.99377894 0.9296365  0.94168293 0.9698062  0.9907086  1.0064\n",
            " 0.9747192  1.0007526  0.9518619  0.9361864  0.9654745  0.99009264\n",
            " 0.9555112  0.9440199  0.9485426  0.9624002  0.9108129  0.9617132\n",
            " 0.9506047  0.9181041  0.964291   1.0159352  0.96754056 0.9815962\n",
            " 0.9672513  0.9504498  0.9910444  0.9566556  0.9614648  0.9764074\n",
            " 0.96826154 0.93727934 0.9697995  0.9489101  0.96926826 0.9743686\n",
            " 0.9843251  0.97726697 0.94223607 0.9901543  0.9682931  0.92426145\n",
            " 0.9311396  0.9841879  0.95724624 0.9203198  0.9223299  0.99138\n",
            " 0.97356415 0.98055345 0.93970555 0.9155717  0.97548795 0.95327294\n",
            " 0.9545346  0.9461933  0.9586206  0.9469726  0.9978785  0.945069\n",
            " 0.95108366 0.95443094 0.99617076 0.95889425 0.9808891  0.89338636\n",
            " 0.9850269  0.9817317  0.948286   0.9433789  0.9890579  0.94135755\n",
            " 0.9105557  0.9421287  0.95886415 0.9965162  0.9902921  0.896182\n",
            " 0.9829829  0.9234183  0.9714166  0.9400381  0.9022728  0.9350837 ]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Saving model checkpoint to /content/drive/MyDrive/checkpoint-375\n",
            "Configuration saved in /content/drive/MyDrive/checkpoint-375/config.json\n",
            "Model weights saved in /content/drive/MyDrive/checkpoint-375/pytorch_model.bin\n",
            "\n",
            "\n",
            "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
            "\n",
            "\n",
            "Loading best model from /content/drive/MyDrive/checkpoint-150 (score: 0.2499827891588211).\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "TrainOutput(global_step=375, training_loss=0.10179373168945313, metrics={'train_runtime': 72.2998, 'train_samples_per_second': 41.494, 'train_steps_per_second': 5.187, 'total_flos': 47454119425104.0, 'train_loss': 0.10179373168945313, 'epoch': 5.0})"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "CCNC_utKi40g",
        "outputId": "5e2e4c02-2e72-4c17-8545-a216cd3334cc"
      },
      "source": [
        "def model_init():\n",
        "    return BertForSequenceClassification.from_pretrained( \"distilbert-base-uncased\", num_labels=1)\n",
        "trainer = Trainer(\n",
        "    model_init=model_init,\n",
        "    args=training_args,\n",
        "    train_dataset=small_train_dataset,\n",
        "    eval_dataset=small_eval_dataset,\n",
        "    tokenizer=tokenizer,\n",
        "    compute_metrics=compute_metrics\n",
        ")\n",
        "best_run = trainer.hyperparameter_search(n_trials=10, direction=\"maximize\")\n"
      ],
      "id": "CCNC_utKi40g",
      "execution_count": 7,
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "loading configuration file https://huggingface.co/distilbert-base-uncased/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/23454919702d26495337f3da04d1655c7ee010d5ec9d77bdb9e399e00302c0a1.91b885ab15d631bf9cee9dc9d25ece0afd932f2f5130eba28f2055b2220c0333\n",
            "You are using a model of type distilbert to instantiate a model of type bert. This is not supported for all configurations of models and can yield errors.\n",
            "Model config BertConfig {\n",
            "  \"activation\": \"gelu\",\n",
            "  \"architectures\": [\n",
            "    \"DistilBertForMaskedLM\"\n",
            "  ],\n",
            "  \"attention_dropout\": 0.1,\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"classifier_dropout\": null,\n",
            "  \"dim\": 768,\n",
            "  \"dropout\": 0.1,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dim\": 3072,\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 768,\n",
            "  \"id2label\": {\n",
            "    \"0\": \"LABEL_0\"\n",
            "  },\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 3072,\n",
            "  \"label2id\": {\n",
            "    \"LABEL_0\": 0\n",
            "  },\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"bert\",\n",
            "  \"n_heads\": 12,\n",
            "  \"n_layers\": 6,\n",
            "  \"num_attention_heads\": 12,\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"qa_dropout\": 0.1,\n",
            "  \"seq_classif_dropout\": 0.2,\n",
            "  \"sinusoidal_pos_embds\": false,\n",
            "  \"tie_weights_\": true,\n",
            "  \"transformers_version\": \"4.12.5\",\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 30522\n",
            "}\n",
            "\n",
            "loading weights file https://huggingface.co/distilbert-base-uncased/resolve/main/pytorch_model.bin from cache at /root/.cache/huggingface/transformers/9c169103d7e5a73936dd2b627e42851bec0831212b677c637033ee4bce9ab5ee.126183e36667471617ae2f0835fab707baa54b731f991507ebbb55ea85adb12a\n",
            "Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing BertForSequenceClassification: ['distilbert.transformer.layer.3.ffn.lin1.bias', 'distilbert.embeddings.position_embeddings.weight', 'distilbert.transformer.layer.1.attention.v_lin.bias', 'distilbert.transformer.layer.1.attention.q_lin.weight', 'distilbert.transformer.layer.0.attention.q_lin.weight', 'distilbert.embeddings.LayerNorm.weight', 'distilbert.transformer.layer.2.sa_layer_norm.bias', 'distilbert.transformer.layer.3.attention.v_lin.bias', 'distilbert.transformer.layer.5.ffn.lin1.bias', 'distilbert.transformer.layer.1.output_layer_norm.weight', 'distilbert.transformer.layer.5.attention.q_lin.weight', 'distilbert.transformer.layer.4.ffn.lin2.bias', 'distilbert.transformer.layer.0.attention.k_lin.bias', 'distilbert.transformer.layer.3.attention.v_lin.weight', 'distilbert.transformer.layer.0.ffn.lin2.weight', 'distilbert.transformer.layer.4.output_layer_norm.bias', 'distilbert.transformer.layer.2.output_layer_norm.weight', 'distilbert.transformer.layer.2.attention.k_lin.weight', 'distilbert.transformer.layer.4.ffn.lin1.weight', 'distilbert.transformer.layer.1.ffn.lin2.weight', 'distilbert.transformer.layer.5.attention.q_lin.bias', 'distilbert.transformer.layer.0.attention.v_lin.bias', 'distilbert.transformer.layer.2.ffn.lin1.weight', 'distilbert.transformer.layer.1.attention.out_lin.weight', 'distilbert.transformer.layer.5.ffn.lin2.bias', 'distilbert.transformer.layer.3.ffn.lin2.bias', 'distilbert.transformer.layer.1.attention.q_lin.bias', 'vocab_projector.bias', 'distilbert.transformer.layer.2.attention.out_lin.bias', 'distilbert.transformer.layer.0.attention.v_lin.weight', 'distilbert.transformer.layer.0.output_layer_norm.bias', 'distilbert.transformer.layer.0.attention.q_lin.bias', 'distilbert.transformer.layer.0.attention.k_lin.weight', 'distilbert.transformer.layer.0.ffn.lin2.bias', 'distilbert.transformer.layer.0.sa_layer_norm.bias', 'distilbert.transformer.layer.3.attention.out_lin.bias', 'distilbert.transformer.layer.4.attention.k_lin.bias', 'distilbert.transformer.layer.2.output_layer_norm.bias', 'distilbert.transformer.layer.4.ffn.lin2.weight', 'distilbert.transformer.layer.5.attention.v_lin.weight', 'distilbert.transformer.layer.5.sa_layer_norm.bias', 'distilbert.transformer.layer.1.attention.k_lin.bias', 'distilbert.transformer.layer.4.attention.k_lin.weight', 'distilbert.transformer.layer.3.attention.q_lin.bias', 'distilbert.transformer.layer.1.ffn.lin2.bias', 'distilbert.transformer.layer.3.ffn.lin1.weight', 'distilbert.transformer.layer.4.attention.q_lin.bias', 'distilbert.transformer.layer.4.attention.v_lin.weight', 'distilbert.transformer.layer.5.output_layer_norm.weight', 'distilbert.transformer.layer.2.attention.q_lin.weight', 'distilbert.transformer.layer.1.sa_layer_norm.bias', 'distilbert.transformer.layer.4.ffn.lin1.bias', 'distilbert.transformer.layer.5.ffn.lin2.weight', 'distilbert.transformer.layer.1.ffn.lin1.bias', 'distilbert.transformer.layer.3.attention.out_lin.weight', 'distilbert.transformer.layer.4.sa_layer_norm.bias', 'distilbert.transformer.layer.4.output_layer_norm.weight', 'distilbert.transformer.layer.5.attention.k_lin.bias', 'distilbert.transformer.layer.0.output_layer_norm.weight', 'distilbert.transformer.layer.3.attention.k_lin.weight', 'vocab_transform.weight', 'distilbert.transformer.layer.4.attention.out_lin.bias', 'distilbert.transformer.layer.4.attention.out_lin.weight', 'distilbert.transformer.layer.2.attention.q_lin.bias', 'distilbert.transformer.layer.2.ffn.lin1.bias', 'distilbert.transformer.layer.5.attention.out_lin.weight', 'vocab_projector.weight', 'distilbert.transformer.layer.2.attention.v_lin.weight', 'distilbert.transformer.layer.3.ffn.lin2.weight', 'distilbert.transformer.layer.3.output_layer_norm.bias', 'distilbert.transformer.layer.5.attention.k_lin.weight', 'distilbert.transformer.layer.1.attention.k_lin.weight', 'distilbert.transformer.layer.0.ffn.lin1.bias', 'distilbert.transformer.layer.0.ffn.lin1.weight', 'distilbert.transformer.layer.2.attention.k_lin.bias', 'distilbert.transformer.layer.1.attention.out_lin.bias', 'distilbert.transformer.layer.0.sa_layer_norm.weight', 'vocab_layer_norm.bias', 'distilbert.transformer.layer.4.sa_layer_norm.weight', 'distilbert.embeddings.word_embeddings.weight', 'distilbert.transformer.layer.2.ffn.lin2.weight', 'distilbert.transformer.layer.5.sa_layer_norm.weight', 'distilbert.transformer.layer.1.attention.v_lin.weight', 'distilbert.transformer.layer.3.sa_layer_norm.weight', 'distilbert.transformer.layer.5.ffn.lin1.weight', 'distilbert.transformer.layer.3.attention.q_lin.weight', 'distilbert.transformer.layer.0.attention.out_lin.weight', 'distilbert.transformer.layer.2.attention.out_lin.weight', 'distilbert.transformer.layer.2.sa_layer_norm.weight', 'distilbert.transformer.layer.3.output_layer_norm.weight', 'distilbert.transformer.layer.5.attention.out_lin.bias', 'distilbert.transformer.layer.1.sa_layer_norm.weight', 'distilbert.transformer.layer.3.attention.k_lin.bias', 'distilbert.transformer.layer.1.output_layer_norm.bias', 'vocab_layer_norm.weight', 'distilbert.transformer.layer.4.attention.q_lin.weight', 'distilbert.transformer.layer.1.ffn.lin1.weight', 'distilbert.transformer.layer.2.attention.v_lin.bias', 'distilbert.transformer.layer.4.attention.v_lin.bias', 'distilbert.transformer.layer.5.output_layer_norm.bias', 'distilbert.embeddings.LayerNorm.bias', 'distilbert.transformer.layer.5.attention.v_lin.bias', 'distilbert.transformer.layer.0.attention.out_lin.bias', 'vocab_transform.bias', 'distilbert.transformer.layer.3.sa_layer_norm.bias', 'distilbert.transformer.layer.2.ffn.lin2.bias']\n",
            "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['encoder.layer.3.attention.output.dense.weight', 'encoder.layer.7.attention.output.LayerNorm.bias', 'encoder.layer.1.attention.output.LayerNorm.weight', 'encoder.layer.6.output.dense.bias', 'encoder.layer.3.attention.output.LayerNorm.weight', 'encoder.layer.1.attention.self.key.weight', 'encoder.layer.0.attention.self.query.bias', 'encoder.layer.10.output.dense.weight', 'encoder.layer.11.attention.self.value.weight', 'encoder.layer.11.attention.output.dense.weight', 'classifier.weight', 'encoder.layer.4.attention.self.query.weight', 'encoder.layer.4.attention.self.value.bias', 'encoder.layer.9.intermediate.dense.weight', 'encoder.layer.10.attention.output.dense.bias', 'encoder.layer.2.attention.self.query.weight', 'encoder.layer.3.attention.self.query.weight', 'encoder.layer.9.attention.output.LayerNorm.weight', 'encoder.layer.5.attention.self.value.bias', 'encoder.layer.1.intermediate.dense.bias', 'encoder.layer.1.attention.output.LayerNorm.bias', 'encoder.layer.7.attention.self.key.weight', 'encoder.layer.0.attention.output.LayerNorm.weight', 'encoder.layer.8.output.dense.weight', 'encoder.layer.11.attention.self.query.bias', 'encoder.layer.3.attention.output.dense.bias', 'encoder.layer.4.output.dense.bias', 'encoder.layer.2.attention.self.query.bias', 'encoder.layer.8.attention.self.value.bias', 'encoder.layer.8.intermediate.dense.weight', 'encoder.layer.7.output.dense.weight', 'encoder.layer.6.attention.output.dense.weight', 'encoder.layer.9.output.LayerNorm.bias', 'encoder.layer.11.attention.self.query.weight', 'encoder.layer.3.intermediate.dense.bias', 'encoder.layer.8.intermediate.dense.bias', 'encoder.layer.9.attention.self.value.bias', 'encoder.layer.4.attention.output.LayerNorm.weight', 'encoder.layer.9.output.LayerNorm.weight', 'encoder.layer.7.attention.self.value.bias', 'encoder.layer.5.output.dense.weight', 'encoder.layer.4.attention.self.value.weight', 'encoder.layer.11.output.dense.weight', 'pooler.dense.weight', 'encoder.layer.0.attention.output.LayerNorm.bias', 'embeddings.LayerNorm.weight', 'encoder.layer.5.intermediate.dense.weight', 'encoder.layer.6.attention.output.LayerNorm.bias', 'encoder.layer.4.attention.output.dense.weight', 'encoder.layer.3.attention.output.LayerNorm.bias', 'encoder.layer.8.output.LayerNorm.bias', 'encoder.layer.6.output.LayerNorm.weight', 'encoder.layer.11.output.LayerNorm.bias', 'encoder.layer.9.attention.self.query.bias', 'encoder.layer.2.output.dense.bias', 'encoder.layer.5.output.LayerNorm.weight', 'encoder.layer.2.intermediate.dense.bias', 'encoder.layer.7.attention.self.key.bias', 'encoder.layer.9.attention.self.value.weight', 'encoder.layer.1.attention.self.key.bias', 'encoder.layer.7.attention.output.dense.weight', 'encoder.layer.5.attention.output.LayerNorm.weight', 'encoder.layer.10.attention.self.key.weight', 'encoder.layer.0.attention.self.key.bias', 'encoder.layer.2.attention.self.key.bias', 'encoder.layer.6.output.LayerNorm.bias', 'encoder.layer.10.output.dense.bias', 'encoder.layer.5.output.dense.bias', 'encoder.layer.1.attention.self.query.weight', 'encoder.layer.0.output.dense.bias', 'encoder.layer.7.attention.self.query.weight', 'encoder.layer.0.output.dense.weight', 'encoder.layer.10.output.LayerNorm.bias', 'encoder.layer.8.output.dense.bias', 'encoder.layer.7.output.LayerNorm.weight', 'encoder.layer.10.intermediate.dense.bias', 'encoder.layer.6.attention.self.value.weight', 'encoder.layer.2.attention.output.LayerNorm.bias', 'encoder.layer.5.attention.output.dense.bias', 'encoder.layer.1.attention.self.query.bias', 'encoder.layer.4.output.LayerNorm.weight', 'encoder.layer.8.attention.self.query.bias', 'encoder.layer.4.attention.self.key.weight', 'encoder.layer.6.attention.self.query.bias', 'encoder.layer.0.attention.self.value.weight', 'encoder.layer.2.attention.output.dense.weight', 'encoder.layer.7.attention.self.query.bias', 'encoder.layer.0.output.LayerNorm.weight', 'encoder.layer.5.attention.output.LayerNorm.bias', 'encoder.layer.6.attention.self.value.bias', 'encoder.layer.6.attention.self.query.weight', 'encoder.layer.7.attention.output.LayerNorm.weight', 'encoder.layer.11.attention.output.dense.bias', 'encoder.layer.7.intermediate.dense.weight', 'encoder.layer.2.intermediate.dense.weight', 'encoder.layer.10.attention.output.LayerNorm.weight', 'encoder.layer.11.attention.output.LayerNorm.weight', 'encoder.layer.7.attention.self.value.weight', 'encoder.layer.8.attention.output.LayerNorm.bias', 'encoder.layer.11.attention.self.value.bias', 'encoder.layer.0.output.LayerNorm.bias', 'encoder.layer.5.attention.self.query.weight', 'encoder.layer.2.output.dense.weight', 'encoder.layer.10.attention.self.key.bias', 'encoder.layer.9.attention.self.key.bias', 'encoder.layer.4.attention.self.query.bias', 'encoder.layer.6.intermediate.dense.weight', 'encoder.layer.6.intermediate.dense.bias', 'encoder.layer.1.attention.output.dense.weight', 'classifier.bias', 'encoder.layer.3.attention.self.value.weight', 'encoder.layer.2.attention.self.key.weight', 'encoder.layer.0.attention.output.dense.bias', 'encoder.layer.0.intermediate.dense.bias', 'encoder.layer.1.output.LayerNorm.weight', 'encoder.layer.6.attention.output.LayerNorm.weight', 'encoder.layer.8.attention.output.dense.weight', 'encoder.layer.5.output.LayerNorm.bias', 'encoder.layer.10.attention.self.query.weight', 'encoder.layer.9.attention.output.dense.bias', 'encoder.layer.3.output.dense.weight', 'encoder.layer.7.output.dense.bias', 'encoder.layer.2.output.LayerNorm.bias', 'encoder.layer.11.attention.output.LayerNorm.bias', 'encoder.layer.0.attention.self.query.weight', 'pooler.dense.bias', 'encoder.layer.3.output.dense.bias', 'encoder.layer.3.attention.self.key.weight', 'encoder.layer.3.output.LayerNorm.bias', 'encoder.layer.2.attention.self.value.bias', 'encoder.layer.10.attention.self.value.weight', 'encoder.layer.10.attention.output.LayerNorm.bias', 'encoder.layer.9.attention.self.key.weight', 'encoder.layer.5.attention.self.key.bias', 'encoder.layer.3.intermediate.dense.weight', 'encoder.layer.7.intermediate.dense.bias', 'encoder.layer.8.attention.output.LayerNorm.weight', 'encoder.layer.9.attention.output.dense.weight', 'encoder.layer.0.attention.self.key.weight', 'encoder.layer.1.intermediate.dense.weight', 'encoder.layer.4.intermediate.dense.weight', 'embeddings.position_embeddings.weight', 'encoder.layer.8.attention.output.dense.bias', 'encoder.layer.9.output.dense.weight', 'encoder.layer.11.attention.self.key.bias', 'encoder.layer.6.attention.self.key.bias', 'encoder.layer.8.attention.self.value.weight', 'encoder.layer.7.attention.output.dense.bias', 'encoder.layer.1.attention.self.value.weight', 'encoder.layer.1.output.dense.bias', 'encoder.layer.4.attention.self.key.bias', 'encoder.layer.10.attention.self.value.bias', 'encoder.layer.4.intermediate.dense.bias', 'encoder.layer.6.attention.output.dense.bias', 'encoder.layer.7.output.LayerNorm.bias', 'encoder.layer.1.attention.self.value.bias', 'encoder.layer.8.attention.self.key.weight', 'encoder.layer.1.output.dense.weight', 'encoder.layer.11.output.LayerNorm.weight', 'encoder.layer.11.output.dense.bias', 'encoder.layer.9.output.dense.bias', 'encoder.layer.9.intermediate.dense.bias', 'encoder.layer.5.intermediate.dense.bias', 'encoder.layer.11.intermediate.dense.bias', 'encoder.layer.1.output.LayerNorm.bias', 'encoder.layer.2.attention.self.value.weight', 'encoder.layer.2.attention.output.dense.bias', 'encoder.layer.0.intermediate.dense.weight', 'encoder.layer.0.attention.output.dense.weight', 'encoder.layer.3.attention.self.query.bias', 'encoder.layer.6.output.dense.weight', 'encoder.layer.3.attention.self.value.bias', 'encoder.layer.5.attention.self.query.bias', 'encoder.layer.10.output.LayerNorm.weight', 'encoder.layer.3.attention.self.key.bias', 'embeddings.word_embeddings.weight', 'encoder.layer.4.attention.output.dense.bias', 'encoder.layer.4.output.LayerNorm.bias', 'encoder.layer.9.attention.self.query.weight', 'encoder.layer.0.attention.self.value.bias', 'encoder.layer.4.output.dense.weight', 'encoder.layer.10.intermediate.dense.weight', 'embeddings.LayerNorm.bias', 'encoder.layer.4.attention.output.LayerNorm.bias', 'encoder.layer.8.attention.self.query.weight', 'encoder.layer.8.output.LayerNorm.weight', 'encoder.layer.1.attention.output.dense.bias', 'encoder.layer.2.attention.output.LayerNorm.weight', 'encoder.layer.9.attention.output.LayerNorm.bias', 'embeddings.token_type_embeddings.weight', 'encoder.layer.6.attention.self.key.weight', 'encoder.layer.10.attention.self.query.bias', 'encoder.layer.3.output.LayerNorm.weight', 'encoder.layer.5.attention.output.dense.weight', 'encoder.layer.11.intermediate.dense.weight', 'encoder.layer.5.attention.self.key.weight', 'encoder.layer.10.attention.output.dense.weight', 'encoder.layer.11.attention.self.key.weight', 'encoder.layer.2.output.LayerNorm.weight', 'encoder.layer.8.attention.self.key.bias', 'encoder.layer.5.attention.self.value.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "\u001b[32m[I 2021-11-27 22:48:22,774]\u001b[0m A new study created in memory with name: no-name-2b67a673-5e98-4043-bb18-9d8debf08f70\u001b[0m\n",
            "Trial:\n",
            "loading configuration file https://huggingface.co/distilbert-base-uncased/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/23454919702d26495337f3da04d1655c7ee010d5ec9d77bdb9e399e00302c0a1.91b885ab15d631bf9cee9dc9d25ece0afd932f2f5130eba28f2055b2220c0333\n",
            "You are using a model of type distilbert to instantiate a model of type bert. This is not supported for all configurations of models and can yield errors.\n",
            "Model config BertConfig {\n",
            "  \"activation\": \"gelu\",\n",
            "  \"architectures\": [\n",
            "    \"DistilBertForMaskedLM\"\n",
            "  ],\n",
            "  \"attention_dropout\": 0.1,\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"classifier_dropout\": null,\n",
            "  \"dim\": 768,\n",
            "  \"dropout\": 0.1,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dim\": 3072,\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 768,\n",
            "  \"id2label\": {\n",
            "    \"0\": \"LABEL_0\"\n",
            "  },\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 3072,\n",
            "  \"label2id\": {\n",
            "    \"LABEL_0\": 0\n",
            "  },\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"bert\",\n",
            "  \"n_heads\": 12,\n",
            "  \"n_layers\": 6,\n",
            "  \"num_attention_heads\": 12,\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"qa_dropout\": 0.1,\n",
            "  \"seq_classif_dropout\": 0.2,\n",
            "  \"sinusoidal_pos_embds\": false,\n",
            "  \"tie_weights_\": true,\n",
            "  \"transformers_version\": \"4.12.5\",\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 30522\n",
            "}\n",
            "\n",
            "loading weights file https://huggingface.co/distilbert-base-uncased/resolve/main/pytorch_model.bin from cache at /root/.cache/huggingface/transformers/9c169103d7e5a73936dd2b627e42851bec0831212b677c637033ee4bce9ab5ee.126183e36667471617ae2f0835fab707baa54b731f991507ebbb55ea85adb12a\n",
            "Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing BertForSequenceClassification: ['distilbert.transformer.layer.3.ffn.lin1.bias', 'distilbert.embeddings.position_embeddings.weight', 'distilbert.transformer.layer.1.attention.v_lin.bias', 'distilbert.transformer.layer.1.attention.q_lin.weight', 'distilbert.transformer.layer.0.attention.q_lin.weight', 'distilbert.embeddings.LayerNorm.weight', 'distilbert.transformer.layer.2.sa_layer_norm.bias', 'distilbert.transformer.layer.3.attention.v_lin.bias', 'distilbert.transformer.layer.5.ffn.lin1.bias', 'distilbert.transformer.layer.1.output_layer_norm.weight', 'distilbert.transformer.layer.5.attention.q_lin.weight', 'distilbert.transformer.layer.4.ffn.lin2.bias', 'distilbert.transformer.layer.0.attention.k_lin.bias', 'distilbert.transformer.layer.3.attention.v_lin.weight', 'distilbert.transformer.layer.0.ffn.lin2.weight', 'distilbert.transformer.layer.4.output_layer_norm.bias', 'distilbert.transformer.layer.2.output_layer_norm.weight', 'distilbert.transformer.layer.2.attention.k_lin.weight', 'distilbert.transformer.layer.4.ffn.lin1.weight', 'distilbert.transformer.layer.1.ffn.lin2.weight', 'distilbert.transformer.layer.5.attention.q_lin.bias', 'distilbert.transformer.layer.0.attention.v_lin.bias', 'distilbert.transformer.layer.2.ffn.lin1.weight', 'distilbert.transformer.layer.1.attention.out_lin.weight', 'distilbert.transformer.layer.5.ffn.lin2.bias', 'distilbert.transformer.layer.3.ffn.lin2.bias', 'distilbert.transformer.layer.1.attention.q_lin.bias', 'vocab_projector.bias', 'distilbert.transformer.layer.2.attention.out_lin.bias', 'distilbert.transformer.layer.0.attention.v_lin.weight', 'distilbert.transformer.layer.0.output_layer_norm.bias', 'distilbert.transformer.layer.0.attention.q_lin.bias', 'distilbert.transformer.layer.0.attention.k_lin.weight', 'distilbert.transformer.layer.0.ffn.lin2.bias', 'distilbert.transformer.layer.0.sa_layer_norm.bias', 'distilbert.transformer.layer.3.attention.out_lin.bias', 'distilbert.transformer.layer.4.attention.k_lin.bias', 'distilbert.transformer.layer.2.output_layer_norm.bias', 'distilbert.transformer.layer.4.ffn.lin2.weight', 'distilbert.transformer.layer.5.attention.v_lin.weight', 'distilbert.transformer.layer.5.sa_layer_norm.bias', 'distilbert.transformer.layer.1.attention.k_lin.bias', 'distilbert.transformer.layer.4.attention.k_lin.weight', 'distilbert.transformer.layer.3.attention.q_lin.bias', 'distilbert.transformer.layer.1.ffn.lin2.bias', 'distilbert.transformer.layer.3.ffn.lin1.weight', 'distilbert.transformer.layer.4.attention.q_lin.bias', 'distilbert.transformer.layer.4.attention.v_lin.weight', 'distilbert.transformer.layer.5.output_layer_norm.weight', 'distilbert.transformer.layer.2.attention.q_lin.weight', 'distilbert.transformer.layer.1.sa_layer_norm.bias', 'distilbert.transformer.layer.4.ffn.lin1.bias', 'distilbert.transformer.layer.5.ffn.lin2.weight', 'distilbert.transformer.layer.1.ffn.lin1.bias', 'distilbert.transformer.layer.3.attention.out_lin.weight', 'distilbert.transformer.layer.4.sa_layer_norm.bias', 'distilbert.transformer.layer.4.output_layer_norm.weight', 'distilbert.transformer.layer.5.attention.k_lin.bias', 'distilbert.transformer.layer.0.output_layer_norm.weight', 'distilbert.transformer.layer.3.attention.k_lin.weight', 'vocab_transform.weight', 'distilbert.transformer.layer.4.attention.out_lin.bias', 'distilbert.transformer.layer.4.attention.out_lin.weight', 'distilbert.transformer.layer.2.attention.q_lin.bias', 'distilbert.transformer.layer.2.ffn.lin1.bias', 'distilbert.transformer.layer.5.attention.out_lin.weight', 'vocab_projector.weight', 'distilbert.transformer.layer.2.attention.v_lin.weight', 'distilbert.transformer.layer.3.ffn.lin2.weight', 'distilbert.transformer.layer.3.output_layer_norm.bias', 'distilbert.transformer.layer.5.attention.k_lin.weight', 'distilbert.transformer.layer.1.attention.k_lin.weight', 'distilbert.transformer.layer.0.ffn.lin1.bias', 'distilbert.transformer.layer.0.ffn.lin1.weight', 'distilbert.transformer.layer.2.attention.k_lin.bias', 'distilbert.transformer.layer.1.attention.out_lin.bias', 'distilbert.transformer.layer.0.sa_layer_norm.weight', 'vocab_layer_norm.bias', 'distilbert.transformer.layer.4.sa_layer_norm.weight', 'distilbert.embeddings.word_embeddings.weight', 'distilbert.transformer.layer.2.ffn.lin2.weight', 'distilbert.transformer.layer.5.sa_layer_norm.weight', 'distilbert.transformer.layer.1.attention.v_lin.weight', 'distilbert.transformer.layer.3.sa_layer_norm.weight', 'distilbert.transformer.layer.5.ffn.lin1.weight', 'distilbert.transformer.layer.3.attention.q_lin.weight', 'distilbert.transformer.layer.0.attention.out_lin.weight', 'distilbert.transformer.layer.2.attention.out_lin.weight', 'distilbert.transformer.layer.2.sa_layer_norm.weight', 'distilbert.transformer.layer.3.output_layer_norm.weight', 'distilbert.transformer.layer.5.attention.out_lin.bias', 'distilbert.transformer.layer.1.sa_layer_norm.weight', 'distilbert.transformer.layer.3.attention.k_lin.bias', 'distilbert.transformer.layer.1.output_layer_norm.bias', 'vocab_layer_norm.weight', 'distilbert.transformer.layer.4.attention.q_lin.weight', 'distilbert.transformer.layer.1.ffn.lin1.weight', 'distilbert.transformer.layer.2.attention.v_lin.bias', 'distilbert.transformer.layer.4.attention.v_lin.bias', 'distilbert.transformer.layer.5.output_layer_norm.bias', 'distilbert.embeddings.LayerNorm.bias', 'distilbert.transformer.layer.5.attention.v_lin.bias', 'distilbert.transformer.layer.0.attention.out_lin.bias', 'vocab_transform.bias', 'distilbert.transformer.layer.3.sa_layer_norm.bias', 'distilbert.transformer.layer.2.ffn.lin2.bias']\n",
            "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['encoder.layer.3.attention.output.dense.weight', 'encoder.layer.7.attention.output.LayerNorm.bias', 'encoder.layer.1.attention.output.LayerNorm.weight', 'encoder.layer.6.output.dense.bias', 'encoder.layer.3.attention.output.LayerNorm.weight', 'encoder.layer.1.attention.self.key.weight', 'encoder.layer.0.attention.self.query.bias', 'encoder.layer.10.output.dense.weight', 'encoder.layer.11.attention.self.value.weight', 'encoder.layer.11.attention.output.dense.weight', 'classifier.weight', 'encoder.layer.4.attention.self.query.weight', 'encoder.layer.4.attention.self.value.bias', 'encoder.layer.9.intermediate.dense.weight', 'encoder.layer.10.attention.output.dense.bias', 'encoder.layer.2.attention.self.query.weight', 'encoder.layer.3.attention.self.query.weight', 'encoder.layer.9.attention.output.LayerNorm.weight', 'encoder.layer.5.attention.self.value.bias', 'encoder.layer.1.intermediate.dense.bias', 'encoder.layer.1.attention.output.LayerNorm.bias', 'encoder.layer.7.attention.self.key.weight', 'encoder.layer.0.attention.output.LayerNorm.weight', 'encoder.layer.8.output.dense.weight', 'encoder.layer.11.attention.self.query.bias', 'encoder.layer.3.attention.output.dense.bias', 'encoder.layer.4.output.dense.bias', 'encoder.layer.2.attention.self.query.bias', 'encoder.layer.8.attention.self.value.bias', 'encoder.layer.8.intermediate.dense.weight', 'encoder.layer.7.output.dense.weight', 'encoder.layer.6.attention.output.dense.weight', 'encoder.layer.9.output.LayerNorm.bias', 'encoder.layer.11.attention.self.query.weight', 'encoder.layer.3.intermediate.dense.bias', 'encoder.layer.8.intermediate.dense.bias', 'encoder.layer.9.attention.self.value.bias', 'encoder.layer.4.attention.output.LayerNorm.weight', 'encoder.layer.9.output.LayerNorm.weight', 'encoder.layer.7.attention.self.value.bias', 'encoder.layer.5.output.dense.weight', 'encoder.layer.4.attention.self.value.weight', 'encoder.layer.11.output.dense.weight', 'pooler.dense.weight', 'encoder.layer.0.attention.output.LayerNorm.bias', 'embeddings.LayerNorm.weight', 'encoder.layer.5.intermediate.dense.weight', 'encoder.layer.6.attention.output.LayerNorm.bias', 'encoder.layer.4.attention.output.dense.weight', 'encoder.layer.3.attention.output.LayerNorm.bias', 'encoder.layer.8.output.LayerNorm.bias', 'encoder.layer.6.output.LayerNorm.weight', 'encoder.layer.11.output.LayerNorm.bias', 'encoder.layer.9.attention.self.query.bias', 'encoder.layer.2.output.dense.bias', 'encoder.layer.5.output.LayerNorm.weight', 'encoder.layer.2.intermediate.dense.bias', 'encoder.layer.7.attention.self.key.bias', 'encoder.layer.9.attention.self.value.weight', 'encoder.layer.1.attention.self.key.bias', 'encoder.layer.7.attention.output.dense.weight', 'encoder.layer.5.attention.output.LayerNorm.weight', 'encoder.layer.10.attention.self.key.weight', 'encoder.layer.0.attention.self.key.bias', 'encoder.layer.2.attention.self.key.bias', 'encoder.layer.6.output.LayerNorm.bias', 'encoder.layer.10.output.dense.bias', 'encoder.layer.5.output.dense.bias', 'encoder.layer.1.attention.self.query.weight', 'encoder.layer.0.output.dense.bias', 'encoder.layer.7.attention.self.query.weight', 'encoder.layer.0.output.dense.weight', 'encoder.layer.10.output.LayerNorm.bias', 'encoder.layer.8.output.dense.bias', 'encoder.layer.7.output.LayerNorm.weight', 'encoder.layer.10.intermediate.dense.bias', 'encoder.layer.6.attention.self.value.weight', 'encoder.layer.2.attention.output.LayerNorm.bias', 'encoder.layer.5.attention.output.dense.bias', 'encoder.layer.1.attention.self.query.bias', 'encoder.layer.4.output.LayerNorm.weight', 'encoder.layer.8.attention.self.query.bias', 'encoder.layer.4.attention.self.key.weight', 'encoder.layer.6.attention.self.query.bias', 'encoder.layer.0.attention.self.value.weight', 'encoder.layer.2.attention.output.dense.weight', 'encoder.layer.7.attention.self.query.bias', 'encoder.layer.0.output.LayerNorm.weight', 'encoder.layer.5.attention.output.LayerNorm.bias', 'encoder.layer.6.attention.self.value.bias', 'encoder.layer.6.attention.self.query.weight', 'encoder.layer.7.attention.output.LayerNorm.weight', 'encoder.layer.11.attention.output.dense.bias', 'encoder.layer.7.intermediate.dense.weight', 'encoder.layer.2.intermediate.dense.weight', 'encoder.layer.10.attention.output.LayerNorm.weight', 'encoder.layer.11.attention.output.LayerNorm.weight', 'encoder.layer.7.attention.self.value.weight', 'encoder.layer.8.attention.output.LayerNorm.bias', 'encoder.layer.11.attention.self.value.bias', 'encoder.layer.0.output.LayerNorm.bias', 'encoder.layer.5.attention.self.query.weight', 'encoder.layer.2.output.dense.weight', 'encoder.layer.10.attention.self.key.bias', 'encoder.layer.9.attention.self.key.bias', 'encoder.layer.4.attention.self.query.bias', 'encoder.layer.6.intermediate.dense.weight', 'encoder.layer.6.intermediate.dense.bias', 'encoder.layer.1.attention.output.dense.weight', 'classifier.bias', 'encoder.layer.3.attention.self.value.weight', 'encoder.layer.2.attention.self.key.weight', 'encoder.layer.0.attention.output.dense.bias', 'encoder.layer.0.intermediate.dense.bias', 'encoder.layer.1.output.LayerNorm.weight', 'encoder.layer.6.attention.output.LayerNorm.weight', 'encoder.layer.8.attention.output.dense.weight', 'encoder.layer.5.output.LayerNorm.bias', 'encoder.layer.10.attention.self.query.weight', 'encoder.layer.9.attention.output.dense.bias', 'encoder.layer.3.output.dense.weight', 'encoder.layer.7.output.dense.bias', 'encoder.layer.2.output.LayerNorm.bias', 'encoder.layer.11.attention.output.LayerNorm.bias', 'encoder.layer.0.attention.self.query.weight', 'pooler.dense.bias', 'encoder.layer.3.output.dense.bias', 'encoder.layer.3.attention.self.key.weight', 'encoder.layer.3.output.LayerNorm.bias', 'encoder.layer.2.attention.self.value.bias', 'encoder.layer.10.attention.self.value.weight', 'encoder.layer.10.attention.output.LayerNorm.bias', 'encoder.layer.9.attention.self.key.weight', 'encoder.layer.5.attention.self.key.bias', 'encoder.layer.3.intermediate.dense.weight', 'encoder.layer.7.intermediate.dense.bias', 'encoder.layer.8.attention.output.LayerNorm.weight', 'encoder.layer.9.attention.output.dense.weight', 'encoder.layer.0.attention.self.key.weight', 'encoder.layer.1.intermediate.dense.weight', 'encoder.layer.4.intermediate.dense.weight', 'embeddings.position_embeddings.weight', 'encoder.layer.8.attention.output.dense.bias', 'encoder.layer.9.output.dense.weight', 'encoder.layer.11.attention.self.key.bias', 'encoder.layer.6.attention.self.key.bias', 'encoder.layer.8.attention.self.value.weight', 'encoder.layer.7.attention.output.dense.bias', 'encoder.layer.1.attention.self.value.weight', 'encoder.layer.1.output.dense.bias', 'encoder.layer.4.attention.self.key.bias', 'encoder.layer.10.attention.self.value.bias', 'encoder.layer.4.intermediate.dense.bias', 'encoder.layer.6.attention.output.dense.bias', 'encoder.layer.7.output.LayerNorm.bias', 'encoder.layer.1.attention.self.value.bias', 'encoder.layer.8.attention.self.key.weight', 'encoder.layer.1.output.dense.weight', 'encoder.layer.11.output.LayerNorm.weight', 'encoder.layer.11.output.dense.bias', 'encoder.layer.9.output.dense.bias', 'encoder.layer.9.intermediate.dense.bias', 'encoder.layer.5.intermediate.dense.bias', 'encoder.layer.11.intermediate.dense.bias', 'encoder.layer.1.output.LayerNorm.bias', 'encoder.layer.2.attention.self.value.weight', 'encoder.layer.2.attention.output.dense.bias', 'encoder.layer.0.intermediate.dense.weight', 'encoder.layer.0.attention.output.dense.weight', 'encoder.layer.3.attention.self.query.bias', 'encoder.layer.6.output.dense.weight', 'encoder.layer.3.attention.self.value.bias', 'encoder.layer.5.attention.self.query.bias', 'encoder.layer.10.output.LayerNorm.weight', 'encoder.layer.3.attention.self.key.bias', 'embeddings.word_embeddings.weight', 'encoder.layer.4.attention.output.dense.bias', 'encoder.layer.4.output.LayerNorm.bias', 'encoder.layer.9.attention.self.query.weight', 'encoder.layer.0.attention.self.value.bias', 'encoder.layer.4.output.dense.weight', 'encoder.layer.10.intermediate.dense.weight', 'embeddings.LayerNorm.bias', 'encoder.layer.4.attention.output.LayerNorm.bias', 'encoder.layer.8.attention.self.query.weight', 'encoder.layer.8.output.LayerNorm.weight', 'encoder.layer.1.attention.output.dense.bias', 'encoder.layer.2.attention.output.LayerNorm.weight', 'encoder.layer.9.attention.output.LayerNorm.bias', 'embeddings.token_type_embeddings.weight', 'encoder.layer.6.attention.self.key.weight', 'encoder.layer.10.attention.self.query.bias', 'encoder.layer.3.output.LayerNorm.weight', 'encoder.layer.5.attention.output.dense.weight', 'encoder.layer.11.intermediate.dense.weight', 'encoder.layer.5.attention.self.key.weight', 'encoder.layer.10.attention.output.dense.weight', 'encoder.layer.11.attention.self.key.weight', 'encoder.layer.2.output.LayerNorm.weight', 'encoder.layer.8.attention.self.key.bias', 'encoder.layer.5.attention.self.value.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "***** Running training *****\n",
            "  Num examples = 600\n",
            "  Num Epochs = 1\n",
            "  Instantaneous batch size per device = 16\n",
            "  Total train batch size (w. parallel, distributed & accumulation) = 16\n",
            "  Gradient Accumulation steps = 1\n",
            "  Total optimization steps = 38\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='38' max='38' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [38/38 00:14, Epoch 1/1]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: left;\">\n",
              "      <th>Epoch</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "      <th>Accuracy</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>No log</td>\n",
              "      <td>0.263877</td>\n",
              "      <td>0.495000</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "***** Running Evaluation *****\n",
            "  Num examples = 600\n",
            "  Batch size = 8\n",
            "Saving model checkpoint to /content/drive/MyDrive/run-0/checkpoint-38\n",
            "Configuration saved in /content/drive/MyDrive/run-0/checkpoint-38/config.json\n",
            "Model weights saved in /content/drive/MyDrive/run-0/checkpoint-38/pytorch_model.bin\n",
            "tokenizer config file saved in /content/drive/MyDrive/run-0/checkpoint-38/tokenizer_config.json\n",
            "Special tokens file saved in /content/drive/MyDrive/run-0/checkpoint-38/special_tokens_map.json\n",
            "\n",
            "\n",
            "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
            "\n",
            "\n",
            "Loading best model from /content/drive/MyDrive/run-0/checkpoint-38 (score: 0.26387715339660645).\n",
            "\u001b[32m[I 2021-11-27 22:48:39,548]\u001b[0m Trial 0 finished with value: 0.495 and parameters: {'learning_rate': 1.936354435181433e-06, 'num_train_epochs': 1, 'seed': 5, 'per_device_train_batch_size': 16}. Best is trial 0 with value: 0.495.\u001b[0m\n",
            "Trial:\n",
            "loading configuration file https://huggingface.co/distilbert-base-uncased/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/23454919702d26495337f3da04d1655c7ee010d5ec9d77bdb9e399e00302c0a1.91b885ab15d631bf9cee9dc9d25ece0afd932f2f5130eba28f2055b2220c0333\n",
            "You are using a model of type distilbert to instantiate a model of type bert. This is not supported for all configurations of models and can yield errors.\n",
            "Model config BertConfig {\n",
            "  \"activation\": \"gelu\",\n",
            "  \"architectures\": [\n",
            "    \"DistilBertForMaskedLM\"\n",
            "  ],\n",
            "  \"attention_dropout\": 0.1,\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"classifier_dropout\": null,\n",
            "  \"dim\": 768,\n",
            "  \"dropout\": 0.1,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dim\": 3072,\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 768,\n",
            "  \"id2label\": {\n",
            "    \"0\": \"LABEL_0\"\n",
            "  },\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 3072,\n",
            "  \"label2id\": {\n",
            "    \"LABEL_0\": 0\n",
            "  },\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"bert\",\n",
            "  \"n_heads\": 12,\n",
            "  \"n_layers\": 6,\n",
            "  \"num_attention_heads\": 12,\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"qa_dropout\": 0.1,\n",
            "  \"seq_classif_dropout\": 0.2,\n",
            "  \"sinusoidal_pos_embds\": false,\n",
            "  \"tie_weights_\": true,\n",
            "  \"transformers_version\": \"4.12.5\",\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 30522\n",
            "}\n",
            "\n",
            "loading weights file https://huggingface.co/distilbert-base-uncased/resolve/main/pytorch_model.bin from cache at /root/.cache/huggingface/transformers/9c169103d7e5a73936dd2b627e42851bec0831212b677c637033ee4bce9ab5ee.126183e36667471617ae2f0835fab707baa54b731f991507ebbb55ea85adb12a\n",
            "Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing BertForSequenceClassification: ['distilbert.transformer.layer.3.ffn.lin1.bias', 'distilbert.embeddings.position_embeddings.weight', 'distilbert.transformer.layer.1.attention.v_lin.bias', 'distilbert.transformer.layer.1.attention.q_lin.weight', 'distilbert.transformer.layer.0.attention.q_lin.weight', 'distilbert.embeddings.LayerNorm.weight', 'distilbert.transformer.layer.2.sa_layer_norm.bias', 'distilbert.transformer.layer.3.attention.v_lin.bias', 'distilbert.transformer.layer.5.ffn.lin1.bias', 'distilbert.transformer.layer.1.output_layer_norm.weight', 'distilbert.transformer.layer.5.attention.q_lin.weight', 'distilbert.transformer.layer.4.ffn.lin2.bias', 'distilbert.transformer.layer.0.attention.k_lin.bias', 'distilbert.transformer.layer.3.attention.v_lin.weight', 'distilbert.transformer.layer.0.ffn.lin2.weight', 'distilbert.transformer.layer.4.output_layer_norm.bias', 'distilbert.transformer.layer.2.output_layer_norm.weight', 'distilbert.transformer.layer.2.attention.k_lin.weight', 'distilbert.transformer.layer.4.ffn.lin1.weight', 'distilbert.transformer.layer.1.ffn.lin2.weight', 'distilbert.transformer.layer.5.attention.q_lin.bias', 'distilbert.transformer.layer.0.attention.v_lin.bias', 'distilbert.transformer.layer.2.ffn.lin1.weight', 'distilbert.transformer.layer.1.attention.out_lin.weight', 'distilbert.transformer.layer.5.ffn.lin2.bias', 'distilbert.transformer.layer.3.ffn.lin2.bias', 'distilbert.transformer.layer.1.attention.q_lin.bias', 'vocab_projector.bias', 'distilbert.transformer.layer.2.attention.out_lin.bias', 'distilbert.transformer.layer.0.attention.v_lin.weight', 'distilbert.transformer.layer.0.output_layer_norm.bias', 'distilbert.transformer.layer.0.attention.q_lin.bias', 'distilbert.transformer.layer.0.attention.k_lin.weight', 'distilbert.transformer.layer.0.ffn.lin2.bias', 'distilbert.transformer.layer.0.sa_layer_norm.bias', 'distilbert.transformer.layer.3.attention.out_lin.bias', 'distilbert.transformer.layer.4.attention.k_lin.bias', 'distilbert.transformer.layer.2.output_layer_norm.bias', 'distilbert.transformer.layer.4.ffn.lin2.weight', 'distilbert.transformer.layer.5.attention.v_lin.weight', 'distilbert.transformer.layer.5.sa_layer_norm.bias', 'distilbert.transformer.layer.1.attention.k_lin.bias', 'distilbert.transformer.layer.4.attention.k_lin.weight', 'distilbert.transformer.layer.3.attention.q_lin.bias', 'distilbert.transformer.layer.1.ffn.lin2.bias', 'distilbert.transformer.layer.3.ffn.lin1.weight', 'distilbert.transformer.layer.4.attention.q_lin.bias', 'distilbert.transformer.layer.4.attention.v_lin.weight', 'distilbert.transformer.layer.5.output_layer_norm.weight', 'distilbert.transformer.layer.2.attention.q_lin.weight', 'distilbert.transformer.layer.1.sa_layer_norm.bias', 'distilbert.transformer.layer.4.ffn.lin1.bias', 'distilbert.transformer.layer.5.ffn.lin2.weight', 'distilbert.transformer.layer.1.ffn.lin1.bias', 'distilbert.transformer.layer.3.attention.out_lin.weight', 'distilbert.transformer.layer.4.sa_layer_norm.bias', 'distilbert.transformer.layer.4.output_layer_norm.weight', 'distilbert.transformer.layer.5.attention.k_lin.bias', 'distilbert.transformer.layer.0.output_layer_norm.weight', 'distilbert.transformer.layer.3.attention.k_lin.weight', 'vocab_transform.weight', 'distilbert.transformer.layer.4.attention.out_lin.bias', 'distilbert.transformer.layer.4.attention.out_lin.weight', 'distilbert.transformer.layer.2.attention.q_lin.bias', 'distilbert.transformer.layer.2.ffn.lin1.bias', 'distilbert.transformer.layer.5.attention.out_lin.weight', 'vocab_projector.weight', 'distilbert.transformer.layer.2.attention.v_lin.weight', 'distilbert.transformer.layer.3.ffn.lin2.weight', 'distilbert.transformer.layer.3.output_layer_norm.bias', 'distilbert.transformer.layer.5.attention.k_lin.weight', 'distilbert.transformer.layer.1.attention.k_lin.weight', 'distilbert.transformer.layer.0.ffn.lin1.bias', 'distilbert.transformer.layer.0.ffn.lin1.weight', 'distilbert.transformer.layer.2.attention.k_lin.bias', 'distilbert.transformer.layer.1.attention.out_lin.bias', 'distilbert.transformer.layer.0.sa_layer_norm.weight', 'vocab_layer_norm.bias', 'distilbert.transformer.layer.4.sa_layer_norm.weight', 'distilbert.embeddings.word_embeddings.weight', 'distilbert.transformer.layer.2.ffn.lin2.weight', 'distilbert.transformer.layer.5.sa_layer_norm.weight', 'distilbert.transformer.layer.1.attention.v_lin.weight', 'distilbert.transformer.layer.3.sa_layer_norm.weight', 'distilbert.transformer.layer.5.ffn.lin1.weight', 'distilbert.transformer.layer.3.attention.q_lin.weight', 'distilbert.transformer.layer.0.attention.out_lin.weight', 'distilbert.transformer.layer.2.attention.out_lin.weight', 'distilbert.transformer.layer.2.sa_layer_norm.weight', 'distilbert.transformer.layer.3.output_layer_norm.weight', 'distilbert.transformer.layer.5.attention.out_lin.bias', 'distilbert.transformer.layer.1.sa_layer_norm.weight', 'distilbert.transformer.layer.3.attention.k_lin.bias', 'distilbert.transformer.layer.1.output_layer_norm.bias', 'vocab_layer_norm.weight', 'distilbert.transformer.layer.4.attention.q_lin.weight', 'distilbert.transformer.layer.1.ffn.lin1.weight', 'distilbert.transformer.layer.2.attention.v_lin.bias', 'distilbert.transformer.layer.4.attention.v_lin.bias', 'distilbert.transformer.layer.5.output_layer_norm.bias', 'distilbert.embeddings.LayerNorm.bias', 'distilbert.transformer.layer.5.attention.v_lin.bias', 'distilbert.transformer.layer.0.attention.out_lin.bias', 'vocab_transform.bias', 'distilbert.transformer.layer.3.sa_layer_norm.bias', 'distilbert.transformer.layer.2.ffn.lin2.bias']\n",
            "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['encoder.layer.3.attention.output.dense.weight', 'encoder.layer.7.attention.output.LayerNorm.bias', 'encoder.layer.1.attention.output.LayerNorm.weight', 'encoder.layer.6.output.dense.bias', 'encoder.layer.3.attention.output.LayerNorm.weight', 'encoder.layer.1.attention.self.key.weight', 'encoder.layer.0.attention.self.query.bias', 'encoder.layer.10.output.dense.weight', 'encoder.layer.11.attention.self.value.weight', 'encoder.layer.11.attention.output.dense.weight', 'classifier.weight', 'encoder.layer.4.attention.self.query.weight', 'encoder.layer.4.attention.self.value.bias', 'encoder.layer.9.intermediate.dense.weight', 'encoder.layer.10.attention.output.dense.bias', 'encoder.layer.2.attention.self.query.weight', 'encoder.layer.3.attention.self.query.weight', 'encoder.layer.9.attention.output.LayerNorm.weight', 'encoder.layer.5.attention.self.value.bias', 'encoder.layer.1.intermediate.dense.bias', 'encoder.layer.1.attention.output.LayerNorm.bias', 'encoder.layer.7.attention.self.key.weight', 'encoder.layer.0.attention.output.LayerNorm.weight', 'encoder.layer.8.output.dense.weight', 'encoder.layer.11.attention.self.query.bias', 'encoder.layer.3.attention.output.dense.bias', 'encoder.layer.4.output.dense.bias', 'encoder.layer.2.attention.self.query.bias', 'encoder.layer.8.attention.self.value.bias', 'encoder.layer.8.intermediate.dense.weight', 'encoder.layer.7.output.dense.weight', 'encoder.layer.6.attention.output.dense.weight', 'encoder.layer.9.output.LayerNorm.bias', 'encoder.layer.11.attention.self.query.weight', 'encoder.layer.3.intermediate.dense.bias', 'encoder.layer.8.intermediate.dense.bias', 'encoder.layer.9.attention.self.value.bias', 'encoder.layer.4.attention.output.LayerNorm.weight', 'encoder.layer.9.output.LayerNorm.weight', 'encoder.layer.7.attention.self.value.bias', 'encoder.layer.5.output.dense.weight', 'encoder.layer.4.attention.self.value.weight', 'encoder.layer.11.output.dense.weight', 'pooler.dense.weight', 'encoder.layer.0.attention.output.LayerNorm.bias', 'embeddings.LayerNorm.weight', 'encoder.layer.5.intermediate.dense.weight', 'encoder.layer.6.attention.output.LayerNorm.bias', 'encoder.layer.4.attention.output.dense.weight', 'encoder.layer.3.attention.output.LayerNorm.bias', 'encoder.layer.8.output.LayerNorm.bias', 'encoder.layer.6.output.LayerNorm.weight', 'encoder.layer.11.output.LayerNorm.bias', 'encoder.layer.9.attention.self.query.bias', 'encoder.layer.2.output.dense.bias', 'encoder.layer.5.output.LayerNorm.weight', 'encoder.layer.2.intermediate.dense.bias', 'encoder.layer.7.attention.self.key.bias', 'encoder.layer.9.attention.self.value.weight', 'encoder.layer.1.attention.self.key.bias', 'encoder.layer.7.attention.output.dense.weight', 'encoder.layer.5.attention.output.LayerNorm.weight', 'encoder.layer.10.attention.self.key.weight', 'encoder.layer.0.attention.self.key.bias', 'encoder.layer.2.attention.self.key.bias', 'encoder.layer.6.output.LayerNorm.bias', 'encoder.layer.10.output.dense.bias', 'encoder.layer.5.output.dense.bias', 'encoder.layer.1.attention.self.query.weight', 'encoder.layer.0.output.dense.bias', 'encoder.layer.7.attention.self.query.weight', 'encoder.layer.0.output.dense.weight', 'encoder.layer.10.output.LayerNorm.bias', 'encoder.layer.8.output.dense.bias', 'encoder.layer.7.output.LayerNorm.weight', 'encoder.layer.10.intermediate.dense.bias', 'encoder.layer.6.attention.self.value.weight', 'encoder.layer.2.attention.output.LayerNorm.bias', 'encoder.layer.5.attention.output.dense.bias', 'encoder.layer.1.attention.self.query.bias', 'encoder.layer.4.output.LayerNorm.weight', 'encoder.layer.8.attention.self.query.bias', 'encoder.layer.4.attention.self.key.weight', 'encoder.layer.6.attention.self.query.bias', 'encoder.layer.0.attention.self.value.weight', 'encoder.layer.2.attention.output.dense.weight', 'encoder.layer.7.attention.self.query.bias', 'encoder.layer.0.output.LayerNorm.weight', 'encoder.layer.5.attention.output.LayerNorm.bias', 'encoder.layer.6.attention.self.value.bias', 'encoder.layer.6.attention.self.query.weight', 'encoder.layer.7.attention.output.LayerNorm.weight', 'encoder.layer.11.attention.output.dense.bias', 'encoder.layer.7.intermediate.dense.weight', 'encoder.layer.2.intermediate.dense.weight', 'encoder.layer.10.attention.output.LayerNorm.weight', 'encoder.layer.11.attention.output.LayerNorm.weight', 'encoder.layer.7.attention.self.value.weight', 'encoder.layer.8.attention.output.LayerNorm.bias', 'encoder.layer.11.attention.self.value.bias', 'encoder.layer.0.output.LayerNorm.bias', 'encoder.layer.5.attention.self.query.weight', 'encoder.layer.2.output.dense.weight', 'encoder.layer.10.attention.self.key.bias', 'encoder.layer.9.attention.self.key.bias', 'encoder.layer.4.attention.self.query.bias', 'encoder.layer.6.intermediate.dense.weight', 'encoder.layer.6.intermediate.dense.bias', 'encoder.layer.1.attention.output.dense.weight', 'classifier.bias', 'encoder.layer.3.attention.self.value.weight', 'encoder.layer.2.attention.self.key.weight', 'encoder.layer.0.attention.output.dense.bias', 'encoder.layer.0.intermediate.dense.bias', 'encoder.layer.1.output.LayerNorm.weight', 'encoder.layer.6.attention.output.LayerNorm.weight', 'encoder.layer.8.attention.output.dense.weight', 'encoder.layer.5.output.LayerNorm.bias', 'encoder.layer.10.attention.self.query.weight', 'encoder.layer.9.attention.output.dense.bias', 'encoder.layer.3.output.dense.weight', 'encoder.layer.7.output.dense.bias', 'encoder.layer.2.output.LayerNorm.bias', 'encoder.layer.11.attention.output.LayerNorm.bias', 'encoder.layer.0.attention.self.query.weight', 'pooler.dense.bias', 'encoder.layer.3.output.dense.bias', 'encoder.layer.3.attention.self.key.weight', 'encoder.layer.3.output.LayerNorm.bias', 'encoder.layer.2.attention.self.value.bias', 'encoder.layer.10.attention.self.value.weight', 'encoder.layer.10.attention.output.LayerNorm.bias', 'encoder.layer.9.attention.self.key.weight', 'encoder.layer.5.attention.self.key.bias', 'encoder.layer.3.intermediate.dense.weight', 'encoder.layer.7.intermediate.dense.bias', 'encoder.layer.8.attention.output.LayerNorm.weight', 'encoder.layer.9.attention.output.dense.weight', 'encoder.layer.0.attention.self.key.weight', 'encoder.layer.1.intermediate.dense.weight', 'encoder.layer.4.intermediate.dense.weight', 'embeddings.position_embeddings.weight', 'encoder.layer.8.attention.output.dense.bias', 'encoder.layer.9.output.dense.weight', 'encoder.layer.11.attention.self.key.bias', 'encoder.layer.6.attention.self.key.bias', 'encoder.layer.8.attention.self.value.weight', 'encoder.layer.7.attention.output.dense.bias', 'encoder.layer.1.attention.self.value.weight', 'encoder.layer.1.output.dense.bias', 'encoder.layer.4.attention.self.key.bias', 'encoder.layer.10.attention.self.value.bias', 'encoder.layer.4.intermediate.dense.bias', 'encoder.layer.6.attention.output.dense.bias', 'encoder.layer.7.output.LayerNorm.bias', 'encoder.layer.1.attention.self.value.bias', 'encoder.layer.8.attention.self.key.weight', 'encoder.layer.1.output.dense.weight', 'encoder.layer.11.output.LayerNorm.weight', 'encoder.layer.11.output.dense.bias', 'encoder.layer.9.output.dense.bias', 'encoder.layer.9.intermediate.dense.bias', 'encoder.layer.5.intermediate.dense.bias', 'encoder.layer.11.intermediate.dense.bias', 'encoder.layer.1.output.LayerNorm.bias', 'encoder.layer.2.attention.self.value.weight', 'encoder.layer.2.attention.output.dense.bias', 'encoder.layer.0.intermediate.dense.weight', 'encoder.layer.0.attention.output.dense.weight', 'encoder.layer.3.attention.self.query.bias', 'encoder.layer.6.output.dense.weight', 'encoder.layer.3.attention.self.value.bias', 'encoder.layer.5.attention.self.query.bias', 'encoder.layer.10.output.LayerNorm.weight', 'encoder.layer.3.attention.self.key.bias', 'embeddings.word_embeddings.weight', 'encoder.layer.4.attention.output.dense.bias', 'encoder.layer.4.output.LayerNorm.bias', 'encoder.layer.9.attention.self.query.weight', 'encoder.layer.0.attention.self.value.bias', 'encoder.layer.4.output.dense.weight', 'encoder.layer.10.intermediate.dense.weight', 'embeddings.LayerNorm.bias', 'encoder.layer.4.attention.output.LayerNorm.bias', 'encoder.layer.8.attention.self.query.weight', 'encoder.layer.8.output.LayerNorm.weight', 'encoder.layer.1.attention.output.dense.bias', 'encoder.layer.2.attention.output.LayerNorm.weight', 'encoder.layer.9.attention.output.LayerNorm.bias', 'embeddings.token_type_embeddings.weight', 'encoder.layer.6.attention.self.key.weight', 'encoder.layer.10.attention.self.query.bias', 'encoder.layer.3.output.LayerNorm.weight', 'encoder.layer.5.attention.output.dense.weight', 'encoder.layer.11.intermediate.dense.weight', 'encoder.layer.5.attention.self.key.weight', 'encoder.layer.10.attention.output.dense.weight', 'encoder.layer.11.attention.self.key.weight', 'encoder.layer.2.output.LayerNorm.weight', 'encoder.layer.8.attention.self.key.bias', 'encoder.layer.5.attention.self.value.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "***** Running training *****\n",
            "  Num examples = 600\n",
            "  Num Epochs = 3\n",
            "  Instantaneous batch size per device = 16\n",
            "  Total train batch size (w. parallel, distributed & accumulation) = 16\n",
            "  Gradient Accumulation steps = 1\n",
            "  Total optimization steps = 114\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='114' max='114' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [114/114 01:03, Epoch 3/3]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: left;\">\n",
              "      <th>Epoch</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "      <th>Accuracy</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>No log</td>\n",
              "      <td>0.250851</td>\n",
              "      <td>0.495000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2</td>\n",
              "      <td>No log</td>\n",
              "      <td>0.250924</td>\n",
              "      <td>0.495000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3</td>\n",
              "      <td>No log</td>\n",
              "      <td>0.252531</td>\n",
              "      <td>0.495000</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "***** Running Evaluation *****\n",
            "  Num examples = 600\n",
            "  Batch size = 8\n",
            "Saving model checkpoint to /content/drive/MyDrive/run-1/checkpoint-38\n",
            "Configuration saved in /content/drive/MyDrive/run-1/checkpoint-38/config.json\n",
            "Model weights saved in /content/drive/MyDrive/run-1/checkpoint-38/pytorch_model.bin\n",
            "tokenizer config file saved in /content/drive/MyDrive/run-1/checkpoint-38/tokenizer_config.json\n",
            "Special tokens file saved in /content/drive/MyDrive/run-1/checkpoint-38/special_tokens_map.json\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 600\n",
            "  Batch size = 8\n",
            "Saving model checkpoint to /content/drive/MyDrive/run-1/checkpoint-76\n",
            "Configuration saved in /content/drive/MyDrive/run-1/checkpoint-76/config.json\n",
            "Model weights saved in /content/drive/MyDrive/run-1/checkpoint-76/pytorch_model.bin\n",
            "tokenizer config file saved in /content/drive/MyDrive/run-1/checkpoint-76/tokenizer_config.json\n",
            "Special tokens file saved in /content/drive/MyDrive/run-1/checkpoint-76/special_tokens_map.json\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 600\n",
            "  Batch size = 8\n",
            "Saving model checkpoint to /content/drive/MyDrive/run-1/checkpoint-114\n",
            "Configuration saved in /content/drive/MyDrive/run-1/checkpoint-114/config.json\n",
            "Model weights saved in /content/drive/MyDrive/run-1/checkpoint-114/pytorch_model.bin\n",
            "tokenizer config file saved in /content/drive/MyDrive/run-1/checkpoint-114/tokenizer_config.json\n",
            "Special tokens file saved in /content/drive/MyDrive/run-1/checkpoint-114/special_tokens_map.json\n",
            "\n",
            "\n",
            "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
            "\n",
            "\n",
            "Loading best model from /content/drive/MyDrive/run-1/checkpoint-38 (score: 0.25085145235061646).\n",
            "\u001b[32m[I 2021-11-27 22:49:48,366]\u001b[0m Trial 1 finished with value: 0.495 and parameters: {'learning_rate': 4.556893873402387e-05, 'num_train_epochs': 3, 'seed': 39, 'per_device_train_batch_size': 16}. Best is trial 0 with value: 0.495.\u001b[0m\n",
            "Trial:\n",
            "loading configuration file https://huggingface.co/distilbert-base-uncased/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/23454919702d26495337f3da04d1655c7ee010d5ec9d77bdb9e399e00302c0a1.91b885ab15d631bf9cee9dc9d25ece0afd932f2f5130eba28f2055b2220c0333\n",
            "You are using a model of type distilbert to instantiate a model of type bert. This is not supported for all configurations of models and can yield errors.\n",
            "Model config BertConfig {\n",
            "  \"activation\": \"gelu\",\n",
            "  \"architectures\": [\n",
            "    \"DistilBertForMaskedLM\"\n",
            "  ],\n",
            "  \"attention_dropout\": 0.1,\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"classifier_dropout\": null,\n",
            "  \"dim\": 768,\n",
            "  \"dropout\": 0.1,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dim\": 3072,\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 768,\n",
            "  \"id2label\": {\n",
            "    \"0\": \"LABEL_0\"\n",
            "  },\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 3072,\n",
            "  \"label2id\": {\n",
            "    \"LABEL_0\": 0\n",
            "  },\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"bert\",\n",
            "  \"n_heads\": 12,\n",
            "  \"n_layers\": 6,\n",
            "  \"num_attention_heads\": 12,\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"qa_dropout\": 0.1,\n",
            "  \"seq_classif_dropout\": 0.2,\n",
            "  \"sinusoidal_pos_embds\": false,\n",
            "  \"tie_weights_\": true,\n",
            "  \"transformers_version\": \"4.12.5\",\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 30522\n",
            "}\n",
            "\n",
            "loading weights file https://huggingface.co/distilbert-base-uncased/resolve/main/pytorch_model.bin from cache at /root/.cache/huggingface/transformers/9c169103d7e5a73936dd2b627e42851bec0831212b677c637033ee4bce9ab5ee.126183e36667471617ae2f0835fab707baa54b731f991507ebbb55ea85adb12a\n",
            "Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing BertForSequenceClassification: ['distilbert.transformer.layer.3.ffn.lin1.bias', 'distilbert.embeddings.position_embeddings.weight', 'distilbert.transformer.layer.1.attention.v_lin.bias', 'distilbert.transformer.layer.1.attention.q_lin.weight', 'distilbert.transformer.layer.0.attention.q_lin.weight', 'distilbert.embeddings.LayerNorm.weight', 'distilbert.transformer.layer.2.sa_layer_norm.bias', 'distilbert.transformer.layer.3.attention.v_lin.bias', 'distilbert.transformer.layer.5.ffn.lin1.bias', 'distilbert.transformer.layer.1.output_layer_norm.weight', 'distilbert.transformer.layer.5.attention.q_lin.weight', 'distilbert.transformer.layer.4.ffn.lin2.bias', 'distilbert.transformer.layer.0.attention.k_lin.bias', 'distilbert.transformer.layer.3.attention.v_lin.weight', 'distilbert.transformer.layer.0.ffn.lin2.weight', 'distilbert.transformer.layer.4.output_layer_norm.bias', 'distilbert.transformer.layer.2.output_layer_norm.weight', 'distilbert.transformer.layer.2.attention.k_lin.weight', 'distilbert.transformer.layer.4.ffn.lin1.weight', 'distilbert.transformer.layer.1.ffn.lin2.weight', 'distilbert.transformer.layer.5.attention.q_lin.bias', 'distilbert.transformer.layer.0.attention.v_lin.bias', 'distilbert.transformer.layer.2.ffn.lin1.weight', 'distilbert.transformer.layer.1.attention.out_lin.weight', 'distilbert.transformer.layer.5.ffn.lin2.bias', 'distilbert.transformer.layer.3.ffn.lin2.bias', 'distilbert.transformer.layer.1.attention.q_lin.bias', 'vocab_projector.bias', 'distilbert.transformer.layer.2.attention.out_lin.bias', 'distilbert.transformer.layer.0.attention.v_lin.weight', 'distilbert.transformer.layer.0.output_layer_norm.bias', 'distilbert.transformer.layer.0.attention.q_lin.bias', 'distilbert.transformer.layer.0.attention.k_lin.weight', 'distilbert.transformer.layer.0.ffn.lin2.bias', 'distilbert.transformer.layer.0.sa_layer_norm.bias', 'distilbert.transformer.layer.3.attention.out_lin.bias', 'distilbert.transformer.layer.4.attention.k_lin.bias', 'distilbert.transformer.layer.2.output_layer_norm.bias', 'distilbert.transformer.layer.4.ffn.lin2.weight', 'distilbert.transformer.layer.5.attention.v_lin.weight', 'distilbert.transformer.layer.5.sa_layer_norm.bias', 'distilbert.transformer.layer.1.attention.k_lin.bias', 'distilbert.transformer.layer.4.attention.k_lin.weight', 'distilbert.transformer.layer.3.attention.q_lin.bias', 'distilbert.transformer.layer.1.ffn.lin2.bias', 'distilbert.transformer.layer.3.ffn.lin1.weight', 'distilbert.transformer.layer.4.attention.q_lin.bias', 'distilbert.transformer.layer.4.attention.v_lin.weight', 'distilbert.transformer.layer.5.output_layer_norm.weight', 'distilbert.transformer.layer.2.attention.q_lin.weight', 'distilbert.transformer.layer.1.sa_layer_norm.bias', 'distilbert.transformer.layer.4.ffn.lin1.bias', 'distilbert.transformer.layer.5.ffn.lin2.weight', 'distilbert.transformer.layer.1.ffn.lin1.bias', 'distilbert.transformer.layer.3.attention.out_lin.weight', 'distilbert.transformer.layer.4.sa_layer_norm.bias', 'distilbert.transformer.layer.4.output_layer_norm.weight', 'distilbert.transformer.layer.5.attention.k_lin.bias', 'distilbert.transformer.layer.0.output_layer_norm.weight', 'distilbert.transformer.layer.3.attention.k_lin.weight', 'vocab_transform.weight', 'distilbert.transformer.layer.4.attention.out_lin.bias', 'distilbert.transformer.layer.4.attention.out_lin.weight', 'distilbert.transformer.layer.2.attention.q_lin.bias', 'distilbert.transformer.layer.2.ffn.lin1.bias', 'distilbert.transformer.layer.5.attention.out_lin.weight', 'vocab_projector.weight', 'distilbert.transformer.layer.2.attention.v_lin.weight', 'distilbert.transformer.layer.3.ffn.lin2.weight', 'distilbert.transformer.layer.3.output_layer_norm.bias', 'distilbert.transformer.layer.5.attention.k_lin.weight', 'distilbert.transformer.layer.1.attention.k_lin.weight', 'distilbert.transformer.layer.0.ffn.lin1.bias', 'distilbert.transformer.layer.0.ffn.lin1.weight', 'distilbert.transformer.layer.2.attention.k_lin.bias', 'distilbert.transformer.layer.1.attention.out_lin.bias', 'distilbert.transformer.layer.0.sa_layer_norm.weight', 'vocab_layer_norm.bias', 'distilbert.transformer.layer.4.sa_layer_norm.weight', 'distilbert.embeddings.word_embeddings.weight', 'distilbert.transformer.layer.2.ffn.lin2.weight', 'distilbert.transformer.layer.5.sa_layer_norm.weight', 'distilbert.transformer.layer.1.attention.v_lin.weight', 'distilbert.transformer.layer.3.sa_layer_norm.weight', 'distilbert.transformer.layer.5.ffn.lin1.weight', 'distilbert.transformer.layer.3.attention.q_lin.weight', 'distilbert.transformer.layer.0.attention.out_lin.weight', 'distilbert.transformer.layer.2.attention.out_lin.weight', 'distilbert.transformer.layer.2.sa_layer_norm.weight', 'distilbert.transformer.layer.3.output_layer_norm.weight', 'distilbert.transformer.layer.5.attention.out_lin.bias', 'distilbert.transformer.layer.1.sa_layer_norm.weight', 'distilbert.transformer.layer.3.attention.k_lin.bias', 'distilbert.transformer.layer.1.output_layer_norm.bias', 'vocab_layer_norm.weight', 'distilbert.transformer.layer.4.attention.q_lin.weight', 'distilbert.transformer.layer.1.ffn.lin1.weight', 'distilbert.transformer.layer.2.attention.v_lin.bias', 'distilbert.transformer.layer.4.attention.v_lin.bias', 'distilbert.transformer.layer.5.output_layer_norm.bias', 'distilbert.embeddings.LayerNorm.bias', 'distilbert.transformer.layer.5.attention.v_lin.bias', 'distilbert.transformer.layer.0.attention.out_lin.bias', 'vocab_transform.bias', 'distilbert.transformer.layer.3.sa_layer_norm.bias', 'distilbert.transformer.layer.2.ffn.lin2.bias']\n",
            "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['encoder.layer.3.attention.output.dense.weight', 'encoder.layer.7.attention.output.LayerNorm.bias', 'encoder.layer.1.attention.output.LayerNorm.weight', 'encoder.layer.6.output.dense.bias', 'encoder.layer.3.attention.output.LayerNorm.weight', 'encoder.layer.1.attention.self.key.weight', 'encoder.layer.0.attention.self.query.bias', 'encoder.layer.10.output.dense.weight', 'encoder.layer.11.attention.self.value.weight', 'encoder.layer.11.attention.output.dense.weight', 'classifier.weight', 'encoder.layer.4.attention.self.query.weight', 'encoder.layer.4.attention.self.value.bias', 'encoder.layer.9.intermediate.dense.weight', 'encoder.layer.10.attention.output.dense.bias', 'encoder.layer.2.attention.self.query.weight', 'encoder.layer.3.attention.self.query.weight', 'encoder.layer.9.attention.output.LayerNorm.weight', 'encoder.layer.5.attention.self.value.bias', 'encoder.layer.1.intermediate.dense.bias', 'encoder.layer.1.attention.output.LayerNorm.bias', 'encoder.layer.7.attention.self.key.weight', 'encoder.layer.0.attention.output.LayerNorm.weight', 'encoder.layer.8.output.dense.weight', 'encoder.layer.11.attention.self.query.bias', 'encoder.layer.3.attention.output.dense.bias', 'encoder.layer.4.output.dense.bias', 'encoder.layer.2.attention.self.query.bias', 'encoder.layer.8.attention.self.value.bias', 'encoder.layer.8.intermediate.dense.weight', 'encoder.layer.7.output.dense.weight', 'encoder.layer.6.attention.output.dense.weight', 'encoder.layer.9.output.LayerNorm.bias', 'encoder.layer.11.attention.self.query.weight', 'encoder.layer.3.intermediate.dense.bias', 'encoder.layer.8.intermediate.dense.bias', 'encoder.layer.9.attention.self.value.bias', 'encoder.layer.4.attention.output.LayerNorm.weight', 'encoder.layer.9.output.LayerNorm.weight', 'encoder.layer.7.attention.self.value.bias', 'encoder.layer.5.output.dense.weight', 'encoder.layer.4.attention.self.value.weight', 'encoder.layer.11.output.dense.weight', 'pooler.dense.weight', 'encoder.layer.0.attention.output.LayerNorm.bias', 'embeddings.LayerNorm.weight', 'encoder.layer.5.intermediate.dense.weight', 'encoder.layer.6.attention.output.LayerNorm.bias', 'encoder.layer.4.attention.output.dense.weight', 'encoder.layer.3.attention.output.LayerNorm.bias', 'encoder.layer.8.output.LayerNorm.bias', 'encoder.layer.6.output.LayerNorm.weight', 'encoder.layer.11.output.LayerNorm.bias', 'encoder.layer.9.attention.self.query.bias', 'encoder.layer.2.output.dense.bias', 'encoder.layer.5.output.LayerNorm.weight', 'encoder.layer.2.intermediate.dense.bias', 'encoder.layer.7.attention.self.key.bias', 'encoder.layer.9.attention.self.value.weight', 'encoder.layer.1.attention.self.key.bias', 'encoder.layer.7.attention.output.dense.weight', 'encoder.layer.5.attention.output.LayerNorm.weight', 'encoder.layer.10.attention.self.key.weight', 'encoder.layer.0.attention.self.key.bias', 'encoder.layer.2.attention.self.key.bias', 'encoder.layer.6.output.LayerNorm.bias', 'encoder.layer.10.output.dense.bias', 'encoder.layer.5.output.dense.bias', 'encoder.layer.1.attention.self.query.weight', 'encoder.layer.0.output.dense.bias', 'encoder.layer.7.attention.self.query.weight', 'encoder.layer.0.output.dense.weight', 'encoder.layer.10.output.LayerNorm.bias', 'encoder.layer.8.output.dense.bias', 'encoder.layer.7.output.LayerNorm.weight', 'encoder.layer.10.intermediate.dense.bias', 'encoder.layer.6.attention.self.value.weight', 'encoder.layer.2.attention.output.LayerNorm.bias', 'encoder.layer.5.attention.output.dense.bias', 'encoder.layer.1.attention.self.query.bias', 'encoder.layer.4.output.LayerNorm.weight', 'encoder.layer.8.attention.self.query.bias', 'encoder.layer.4.attention.self.key.weight', 'encoder.layer.6.attention.self.query.bias', 'encoder.layer.0.attention.self.value.weight', 'encoder.layer.2.attention.output.dense.weight', 'encoder.layer.7.attention.self.query.bias', 'encoder.layer.0.output.LayerNorm.weight', 'encoder.layer.5.attention.output.LayerNorm.bias', 'encoder.layer.6.attention.self.value.bias', 'encoder.layer.6.attention.self.query.weight', 'encoder.layer.7.attention.output.LayerNorm.weight', 'encoder.layer.11.attention.output.dense.bias', 'encoder.layer.7.intermediate.dense.weight', 'encoder.layer.2.intermediate.dense.weight', 'encoder.layer.10.attention.output.LayerNorm.weight', 'encoder.layer.11.attention.output.LayerNorm.weight', 'encoder.layer.7.attention.self.value.weight', 'encoder.layer.8.attention.output.LayerNorm.bias', 'encoder.layer.11.attention.self.value.bias', 'encoder.layer.0.output.LayerNorm.bias', 'encoder.layer.5.attention.self.query.weight', 'encoder.layer.2.output.dense.weight', 'encoder.layer.10.attention.self.key.bias', 'encoder.layer.9.attention.self.key.bias', 'encoder.layer.4.attention.self.query.bias', 'encoder.layer.6.intermediate.dense.weight', 'encoder.layer.6.intermediate.dense.bias', 'encoder.layer.1.attention.output.dense.weight', 'classifier.bias', 'encoder.layer.3.attention.self.value.weight', 'encoder.layer.2.attention.self.key.weight', 'encoder.layer.0.attention.output.dense.bias', 'encoder.layer.0.intermediate.dense.bias', 'encoder.layer.1.output.LayerNorm.weight', 'encoder.layer.6.attention.output.LayerNorm.weight', 'encoder.layer.8.attention.output.dense.weight', 'encoder.layer.5.output.LayerNorm.bias', 'encoder.layer.10.attention.self.query.weight', 'encoder.layer.9.attention.output.dense.bias', 'encoder.layer.3.output.dense.weight', 'encoder.layer.7.output.dense.bias', 'encoder.layer.2.output.LayerNorm.bias', 'encoder.layer.11.attention.output.LayerNorm.bias', 'encoder.layer.0.attention.self.query.weight', 'pooler.dense.bias', 'encoder.layer.3.output.dense.bias', 'encoder.layer.3.attention.self.key.weight', 'encoder.layer.3.output.LayerNorm.bias', 'encoder.layer.2.attention.self.value.bias', 'encoder.layer.10.attention.self.value.weight', 'encoder.layer.10.attention.output.LayerNorm.bias', 'encoder.layer.9.attention.self.key.weight', 'encoder.layer.5.attention.self.key.bias', 'encoder.layer.3.intermediate.dense.weight', 'encoder.layer.7.intermediate.dense.bias', 'encoder.layer.8.attention.output.LayerNorm.weight', 'encoder.layer.9.attention.output.dense.weight', 'encoder.layer.0.attention.self.key.weight', 'encoder.layer.1.intermediate.dense.weight', 'encoder.layer.4.intermediate.dense.weight', 'embeddings.position_embeddings.weight', 'encoder.layer.8.attention.output.dense.bias', 'encoder.layer.9.output.dense.weight', 'encoder.layer.11.attention.self.key.bias', 'encoder.layer.6.attention.self.key.bias', 'encoder.layer.8.attention.self.value.weight', 'encoder.layer.7.attention.output.dense.bias', 'encoder.layer.1.attention.self.value.weight', 'encoder.layer.1.output.dense.bias', 'encoder.layer.4.attention.self.key.bias', 'encoder.layer.10.attention.self.value.bias', 'encoder.layer.4.intermediate.dense.bias', 'encoder.layer.6.attention.output.dense.bias', 'encoder.layer.7.output.LayerNorm.bias', 'encoder.layer.1.attention.self.value.bias', 'encoder.layer.8.attention.self.key.weight', 'encoder.layer.1.output.dense.weight', 'encoder.layer.11.output.LayerNorm.weight', 'encoder.layer.11.output.dense.bias', 'encoder.layer.9.output.dense.bias', 'encoder.layer.9.intermediate.dense.bias', 'encoder.layer.5.intermediate.dense.bias', 'encoder.layer.11.intermediate.dense.bias', 'encoder.layer.1.output.LayerNorm.bias', 'encoder.layer.2.attention.self.value.weight', 'encoder.layer.2.attention.output.dense.bias', 'encoder.layer.0.intermediate.dense.weight', 'encoder.layer.0.attention.output.dense.weight', 'encoder.layer.3.attention.self.query.bias', 'encoder.layer.6.output.dense.weight', 'encoder.layer.3.attention.self.value.bias', 'encoder.layer.5.attention.self.query.bias', 'encoder.layer.10.output.LayerNorm.weight', 'encoder.layer.3.attention.self.key.bias', 'embeddings.word_embeddings.weight', 'encoder.layer.4.attention.output.dense.bias', 'encoder.layer.4.output.LayerNorm.bias', 'encoder.layer.9.attention.self.query.weight', 'encoder.layer.0.attention.self.value.bias', 'encoder.layer.4.output.dense.weight', 'encoder.layer.10.intermediate.dense.weight', 'embeddings.LayerNorm.bias', 'encoder.layer.4.attention.output.LayerNorm.bias', 'encoder.layer.8.attention.self.query.weight', 'encoder.layer.8.output.LayerNorm.weight', 'encoder.layer.1.attention.output.dense.bias', 'encoder.layer.2.attention.output.LayerNorm.weight', 'encoder.layer.9.attention.output.LayerNorm.bias', 'embeddings.token_type_embeddings.weight', 'encoder.layer.6.attention.self.key.weight', 'encoder.layer.10.attention.self.query.bias', 'encoder.layer.3.output.LayerNorm.weight', 'encoder.layer.5.attention.output.dense.weight', 'encoder.layer.11.intermediate.dense.weight', 'encoder.layer.5.attention.self.key.weight', 'encoder.layer.10.attention.output.dense.weight', 'encoder.layer.11.attention.self.key.weight', 'encoder.layer.2.output.LayerNorm.weight', 'encoder.layer.8.attention.self.key.bias', 'encoder.layer.5.attention.self.value.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "***** Running training *****\n",
            "  Num examples = 600\n",
            "  Num Epochs = 1\n",
            "  Instantaneous batch size per device = 4\n",
            "  Total train batch size (w. parallel, distributed & accumulation) = 4\n",
            "  Gradient Accumulation steps = 1\n",
            "  Total optimization steps = 150\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='151' max='150' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [150/150 00:08, Epoch 1/1]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: left;\">\n",
              "      <th>Epoch</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "      <th>Accuracy</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>No log</td>\n",
              "      <td>0.256935</td>\n",
              "      <td>0.495000</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "***** Running Evaluation *****\n",
            "  Num examples = 600\n",
            "  Batch size = 8\n",
            "Saving model checkpoint to /content/drive/MyDrive/run-2/checkpoint-150\n",
            "Configuration saved in /content/drive/MyDrive/run-2/checkpoint-150/config.json\n",
            "Model weights saved in /content/drive/MyDrive/run-2/checkpoint-150/pytorch_model.bin\n",
            "tokenizer config file saved in /content/drive/MyDrive/run-2/checkpoint-150/tokenizer_config.json\n",
            "Special tokens file saved in /content/drive/MyDrive/run-2/checkpoint-150/special_tokens_map.json\n",
            "\n",
            "\n",
            "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
            "\n",
            "\n",
            "Loading best model from /content/drive/MyDrive/run-2/checkpoint-150 (score: 0.2569348216056824).\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='150' max='150' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [150/150 00:16, Epoch 1/1]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: left;\">\n",
              "      <th>Epoch</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "      <th>Accuracy</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>No log</td>\n",
              "      <td>0.256935</td>\n",
              "      <td>0.495000</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[32m[I 2021-11-27 22:50:07,589]\u001b[0m Trial 2 finished with value: 0.495 and parameters: {'learning_rate': 8.120956194983892e-05, 'num_train_epochs': 1, 'seed': 35, 'per_device_train_batch_size': 4}. Best is trial 0 with value: 0.495.\u001b[0m\n",
            "Trial:\n",
            "loading configuration file https://huggingface.co/distilbert-base-uncased/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/23454919702d26495337f3da04d1655c7ee010d5ec9d77bdb9e399e00302c0a1.91b885ab15d631bf9cee9dc9d25ece0afd932f2f5130eba28f2055b2220c0333\n",
            "You are using a model of type distilbert to instantiate a model of type bert. This is not supported for all configurations of models and can yield errors.\n",
            "Model config BertConfig {\n",
            "  \"activation\": \"gelu\",\n",
            "  \"architectures\": [\n",
            "    \"DistilBertForMaskedLM\"\n",
            "  ],\n",
            "  \"attention_dropout\": 0.1,\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"classifier_dropout\": null,\n",
            "  \"dim\": 768,\n",
            "  \"dropout\": 0.1,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dim\": 3072,\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 768,\n",
            "  \"id2label\": {\n",
            "    \"0\": \"LABEL_0\"\n",
            "  },\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 3072,\n",
            "  \"label2id\": {\n",
            "    \"LABEL_0\": 0\n",
            "  },\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"bert\",\n",
            "  \"n_heads\": 12,\n",
            "  \"n_layers\": 6,\n",
            "  \"num_attention_heads\": 12,\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"qa_dropout\": 0.1,\n",
            "  \"seq_classif_dropout\": 0.2,\n",
            "  \"sinusoidal_pos_embds\": false,\n",
            "  \"tie_weights_\": true,\n",
            "  \"transformers_version\": \"4.12.5\",\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 30522\n",
            "}\n",
            "\n",
            "loading weights file https://huggingface.co/distilbert-base-uncased/resolve/main/pytorch_model.bin from cache at /root/.cache/huggingface/transformers/9c169103d7e5a73936dd2b627e42851bec0831212b677c637033ee4bce9ab5ee.126183e36667471617ae2f0835fab707baa54b731f991507ebbb55ea85adb12a\n",
            "Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing BertForSequenceClassification: ['distilbert.transformer.layer.3.ffn.lin1.bias', 'distilbert.embeddings.position_embeddings.weight', 'distilbert.transformer.layer.1.attention.v_lin.bias', 'distilbert.transformer.layer.1.attention.q_lin.weight', 'distilbert.transformer.layer.0.attention.q_lin.weight', 'distilbert.embeddings.LayerNorm.weight', 'distilbert.transformer.layer.2.sa_layer_norm.bias', 'distilbert.transformer.layer.3.attention.v_lin.bias', 'distilbert.transformer.layer.5.ffn.lin1.bias', 'distilbert.transformer.layer.1.output_layer_norm.weight', 'distilbert.transformer.layer.5.attention.q_lin.weight', 'distilbert.transformer.layer.4.ffn.lin2.bias', 'distilbert.transformer.layer.0.attention.k_lin.bias', 'distilbert.transformer.layer.3.attention.v_lin.weight', 'distilbert.transformer.layer.0.ffn.lin2.weight', 'distilbert.transformer.layer.4.output_layer_norm.bias', 'distilbert.transformer.layer.2.output_layer_norm.weight', 'distilbert.transformer.layer.2.attention.k_lin.weight', 'distilbert.transformer.layer.4.ffn.lin1.weight', 'distilbert.transformer.layer.1.ffn.lin2.weight', 'distilbert.transformer.layer.5.attention.q_lin.bias', 'distilbert.transformer.layer.0.attention.v_lin.bias', 'distilbert.transformer.layer.2.ffn.lin1.weight', 'distilbert.transformer.layer.1.attention.out_lin.weight', 'distilbert.transformer.layer.5.ffn.lin2.bias', 'distilbert.transformer.layer.3.ffn.lin2.bias', 'distilbert.transformer.layer.1.attention.q_lin.bias', 'vocab_projector.bias', 'distilbert.transformer.layer.2.attention.out_lin.bias', 'distilbert.transformer.layer.0.attention.v_lin.weight', 'distilbert.transformer.layer.0.output_layer_norm.bias', 'distilbert.transformer.layer.0.attention.q_lin.bias', 'distilbert.transformer.layer.0.attention.k_lin.weight', 'distilbert.transformer.layer.0.ffn.lin2.bias', 'distilbert.transformer.layer.0.sa_layer_norm.bias', 'distilbert.transformer.layer.3.attention.out_lin.bias', 'distilbert.transformer.layer.4.attention.k_lin.bias', 'distilbert.transformer.layer.2.output_layer_norm.bias', 'distilbert.transformer.layer.4.ffn.lin2.weight', 'distilbert.transformer.layer.5.attention.v_lin.weight', 'distilbert.transformer.layer.5.sa_layer_norm.bias', 'distilbert.transformer.layer.1.attention.k_lin.bias', 'distilbert.transformer.layer.4.attention.k_lin.weight', 'distilbert.transformer.layer.3.attention.q_lin.bias', 'distilbert.transformer.layer.1.ffn.lin2.bias', 'distilbert.transformer.layer.3.ffn.lin1.weight', 'distilbert.transformer.layer.4.attention.q_lin.bias', 'distilbert.transformer.layer.4.attention.v_lin.weight', 'distilbert.transformer.layer.5.output_layer_norm.weight', 'distilbert.transformer.layer.2.attention.q_lin.weight', 'distilbert.transformer.layer.1.sa_layer_norm.bias', 'distilbert.transformer.layer.4.ffn.lin1.bias', 'distilbert.transformer.layer.5.ffn.lin2.weight', 'distilbert.transformer.layer.1.ffn.lin1.bias', 'distilbert.transformer.layer.3.attention.out_lin.weight', 'distilbert.transformer.layer.4.sa_layer_norm.bias', 'distilbert.transformer.layer.4.output_layer_norm.weight', 'distilbert.transformer.layer.5.attention.k_lin.bias', 'distilbert.transformer.layer.0.output_layer_norm.weight', 'distilbert.transformer.layer.3.attention.k_lin.weight', 'vocab_transform.weight', 'distilbert.transformer.layer.4.attention.out_lin.bias', 'distilbert.transformer.layer.4.attention.out_lin.weight', 'distilbert.transformer.layer.2.attention.q_lin.bias', 'distilbert.transformer.layer.2.ffn.lin1.bias', 'distilbert.transformer.layer.5.attention.out_lin.weight', 'vocab_projector.weight', 'distilbert.transformer.layer.2.attention.v_lin.weight', 'distilbert.transformer.layer.3.ffn.lin2.weight', 'distilbert.transformer.layer.3.output_layer_norm.bias', 'distilbert.transformer.layer.5.attention.k_lin.weight', 'distilbert.transformer.layer.1.attention.k_lin.weight', 'distilbert.transformer.layer.0.ffn.lin1.bias', 'distilbert.transformer.layer.0.ffn.lin1.weight', 'distilbert.transformer.layer.2.attention.k_lin.bias', 'distilbert.transformer.layer.1.attention.out_lin.bias', 'distilbert.transformer.layer.0.sa_layer_norm.weight', 'vocab_layer_norm.bias', 'distilbert.transformer.layer.4.sa_layer_norm.weight', 'distilbert.embeddings.word_embeddings.weight', 'distilbert.transformer.layer.2.ffn.lin2.weight', 'distilbert.transformer.layer.5.sa_layer_norm.weight', 'distilbert.transformer.layer.1.attention.v_lin.weight', 'distilbert.transformer.layer.3.sa_layer_norm.weight', 'distilbert.transformer.layer.5.ffn.lin1.weight', 'distilbert.transformer.layer.3.attention.q_lin.weight', 'distilbert.transformer.layer.0.attention.out_lin.weight', 'distilbert.transformer.layer.2.attention.out_lin.weight', 'distilbert.transformer.layer.2.sa_layer_norm.weight', 'distilbert.transformer.layer.3.output_layer_norm.weight', 'distilbert.transformer.layer.5.attention.out_lin.bias', 'distilbert.transformer.layer.1.sa_layer_norm.weight', 'distilbert.transformer.layer.3.attention.k_lin.bias', 'distilbert.transformer.layer.1.output_layer_norm.bias', 'vocab_layer_norm.weight', 'distilbert.transformer.layer.4.attention.q_lin.weight', 'distilbert.transformer.layer.1.ffn.lin1.weight', 'distilbert.transformer.layer.2.attention.v_lin.bias', 'distilbert.transformer.layer.4.attention.v_lin.bias', 'distilbert.transformer.layer.5.output_layer_norm.bias', 'distilbert.embeddings.LayerNorm.bias', 'distilbert.transformer.layer.5.attention.v_lin.bias', 'distilbert.transformer.layer.0.attention.out_lin.bias', 'vocab_transform.bias', 'distilbert.transformer.layer.3.sa_layer_norm.bias', 'distilbert.transformer.layer.2.ffn.lin2.bias']\n",
            "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['encoder.layer.3.attention.output.dense.weight', 'encoder.layer.7.attention.output.LayerNorm.bias', 'encoder.layer.1.attention.output.LayerNorm.weight', 'encoder.layer.6.output.dense.bias', 'encoder.layer.3.attention.output.LayerNorm.weight', 'encoder.layer.1.attention.self.key.weight', 'encoder.layer.0.attention.self.query.bias', 'encoder.layer.10.output.dense.weight', 'encoder.layer.11.attention.self.value.weight', 'encoder.layer.11.attention.output.dense.weight', 'classifier.weight', 'encoder.layer.4.attention.self.query.weight', 'encoder.layer.4.attention.self.value.bias', 'encoder.layer.9.intermediate.dense.weight', 'encoder.layer.10.attention.output.dense.bias', 'encoder.layer.2.attention.self.query.weight', 'encoder.layer.3.attention.self.query.weight', 'encoder.layer.9.attention.output.LayerNorm.weight', 'encoder.layer.5.attention.self.value.bias', 'encoder.layer.1.intermediate.dense.bias', 'encoder.layer.1.attention.output.LayerNorm.bias', 'encoder.layer.7.attention.self.key.weight', 'encoder.layer.0.attention.output.LayerNorm.weight', 'encoder.layer.8.output.dense.weight', 'encoder.layer.11.attention.self.query.bias', 'encoder.layer.3.attention.output.dense.bias', 'encoder.layer.4.output.dense.bias', 'encoder.layer.2.attention.self.query.bias', 'encoder.layer.8.attention.self.value.bias', 'encoder.layer.8.intermediate.dense.weight', 'encoder.layer.7.output.dense.weight', 'encoder.layer.6.attention.output.dense.weight', 'encoder.layer.9.output.LayerNorm.bias', 'encoder.layer.11.attention.self.query.weight', 'encoder.layer.3.intermediate.dense.bias', 'encoder.layer.8.intermediate.dense.bias', 'encoder.layer.9.attention.self.value.bias', 'encoder.layer.4.attention.output.LayerNorm.weight', 'encoder.layer.9.output.LayerNorm.weight', 'encoder.layer.7.attention.self.value.bias', 'encoder.layer.5.output.dense.weight', 'encoder.layer.4.attention.self.value.weight', 'encoder.layer.11.output.dense.weight', 'pooler.dense.weight', 'encoder.layer.0.attention.output.LayerNorm.bias', 'embeddings.LayerNorm.weight', 'encoder.layer.5.intermediate.dense.weight', 'encoder.layer.6.attention.output.LayerNorm.bias', 'encoder.layer.4.attention.output.dense.weight', 'encoder.layer.3.attention.output.LayerNorm.bias', 'encoder.layer.8.output.LayerNorm.bias', 'encoder.layer.6.output.LayerNorm.weight', 'encoder.layer.11.output.LayerNorm.bias', 'encoder.layer.9.attention.self.query.bias', 'encoder.layer.2.output.dense.bias', 'encoder.layer.5.output.LayerNorm.weight', 'encoder.layer.2.intermediate.dense.bias', 'encoder.layer.7.attention.self.key.bias', 'encoder.layer.9.attention.self.value.weight', 'encoder.layer.1.attention.self.key.bias', 'encoder.layer.7.attention.output.dense.weight', 'encoder.layer.5.attention.output.LayerNorm.weight', 'encoder.layer.10.attention.self.key.weight', 'encoder.layer.0.attention.self.key.bias', 'encoder.layer.2.attention.self.key.bias', 'encoder.layer.6.output.LayerNorm.bias', 'encoder.layer.10.output.dense.bias', 'encoder.layer.5.output.dense.bias', 'encoder.layer.1.attention.self.query.weight', 'encoder.layer.0.output.dense.bias', 'encoder.layer.7.attention.self.query.weight', 'encoder.layer.0.output.dense.weight', 'encoder.layer.10.output.LayerNorm.bias', 'encoder.layer.8.output.dense.bias', 'encoder.layer.7.output.LayerNorm.weight', 'encoder.layer.10.intermediate.dense.bias', 'encoder.layer.6.attention.self.value.weight', 'encoder.layer.2.attention.output.LayerNorm.bias', 'encoder.layer.5.attention.output.dense.bias', 'encoder.layer.1.attention.self.query.bias', 'encoder.layer.4.output.LayerNorm.weight', 'encoder.layer.8.attention.self.query.bias', 'encoder.layer.4.attention.self.key.weight', 'encoder.layer.6.attention.self.query.bias', 'encoder.layer.0.attention.self.value.weight', 'encoder.layer.2.attention.output.dense.weight', 'encoder.layer.7.attention.self.query.bias', 'encoder.layer.0.output.LayerNorm.weight', 'encoder.layer.5.attention.output.LayerNorm.bias', 'encoder.layer.6.attention.self.value.bias', 'encoder.layer.6.attention.self.query.weight', 'encoder.layer.7.attention.output.LayerNorm.weight', 'encoder.layer.11.attention.output.dense.bias', 'encoder.layer.7.intermediate.dense.weight', 'encoder.layer.2.intermediate.dense.weight', 'encoder.layer.10.attention.output.LayerNorm.weight', 'encoder.layer.11.attention.output.LayerNorm.weight', 'encoder.layer.7.attention.self.value.weight', 'encoder.layer.8.attention.output.LayerNorm.bias', 'encoder.layer.11.attention.self.value.bias', 'encoder.layer.0.output.LayerNorm.bias', 'encoder.layer.5.attention.self.query.weight', 'encoder.layer.2.output.dense.weight', 'encoder.layer.10.attention.self.key.bias', 'encoder.layer.9.attention.self.key.bias', 'encoder.layer.4.attention.self.query.bias', 'encoder.layer.6.intermediate.dense.weight', 'encoder.layer.6.intermediate.dense.bias', 'encoder.layer.1.attention.output.dense.weight', 'classifier.bias', 'encoder.layer.3.attention.self.value.weight', 'encoder.layer.2.attention.self.key.weight', 'encoder.layer.0.attention.output.dense.bias', 'encoder.layer.0.intermediate.dense.bias', 'encoder.layer.1.output.LayerNorm.weight', 'encoder.layer.6.attention.output.LayerNorm.weight', 'encoder.layer.8.attention.output.dense.weight', 'encoder.layer.5.output.LayerNorm.bias', 'encoder.layer.10.attention.self.query.weight', 'encoder.layer.9.attention.output.dense.bias', 'encoder.layer.3.output.dense.weight', 'encoder.layer.7.output.dense.bias', 'encoder.layer.2.output.LayerNorm.bias', 'encoder.layer.11.attention.output.LayerNorm.bias', 'encoder.layer.0.attention.self.query.weight', 'pooler.dense.bias', 'encoder.layer.3.output.dense.bias', 'encoder.layer.3.attention.self.key.weight', 'encoder.layer.3.output.LayerNorm.bias', 'encoder.layer.2.attention.self.value.bias', 'encoder.layer.10.attention.self.value.weight', 'encoder.layer.10.attention.output.LayerNorm.bias', 'encoder.layer.9.attention.self.key.weight', 'encoder.layer.5.attention.self.key.bias', 'encoder.layer.3.intermediate.dense.weight', 'encoder.layer.7.intermediate.dense.bias', 'encoder.layer.8.attention.output.LayerNorm.weight', 'encoder.layer.9.attention.output.dense.weight', 'encoder.layer.0.attention.self.key.weight', 'encoder.layer.1.intermediate.dense.weight', 'encoder.layer.4.intermediate.dense.weight', 'embeddings.position_embeddings.weight', 'encoder.layer.8.attention.output.dense.bias', 'encoder.layer.9.output.dense.weight', 'encoder.layer.11.attention.self.key.bias', 'encoder.layer.6.attention.self.key.bias', 'encoder.layer.8.attention.self.value.weight', 'encoder.layer.7.attention.output.dense.bias', 'encoder.layer.1.attention.self.value.weight', 'encoder.layer.1.output.dense.bias', 'encoder.layer.4.attention.self.key.bias', 'encoder.layer.10.attention.self.value.bias', 'encoder.layer.4.intermediate.dense.bias', 'encoder.layer.6.attention.output.dense.bias', 'encoder.layer.7.output.LayerNorm.bias', 'encoder.layer.1.attention.self.value.bias', 'encoder.layer.8.attention.self.key.weight', 'encoder.layer.1.output.dense.weight', 'encoder.layer.11.output.LayerNorm.weight', 'encoder.layer.11.output.dense.bias', 'encoder.layer.9.output.dense.bias', 'encoder.layer.9.intermediate.dense.bias', 'encoder.layer.5.intermediate.dense.bias', 'encoder.layer.11.intermediate.dense.bias', 'encoder.layer.1.output.LayerNorm.bias', 'encoder.layer.2.attention.self.value.weight', 'encoder.layer.2.attention.output.dense.bias', 'encoder.layer.0.intermediate.dense.weight', 'encoder.layer.0.attention.output.dense.weight', 'encoder.layer.3.attention.self.query.bias', 'encoder.layer.6.output.dense.weight', 'encoder.layer.3.attention.self.value.bias', 'encoder.layer.5.attention.self.query.bias', 'encoder.layer.10.output.LayerNorm.weight', 'encoder.layer.3.attention.self.key.bias', 'embeddings.word_embeddings.weight', 'encoder.layer.4.attention.output.dense.bias', 'encoder.layer.4.output.LayerNorm.bias', 'encoder.layer.9.attention.self.query.weight', 'encoder.layer.0.attention.self.value.bias', 'encoder.layer.4.output.dense.weight', 'encoder.layer.10.intermediate.dense.weight', 'embeddings.LayerNorm.bias', 'encoder.layer.4.attention.output.LayerNorm.bias', 'encoder.layer.8.attention.self.query.weight', 'encoder.layer.8.output.LayerNorm.weight', 'encoder.layer.1.attention.output.dense.bias', 'encoder.layer.2.attention.output.LayerNorm.weight', 'encoder.layer.9.attention.output.LayerNorm.bias', 'embeddings.token_type_embeddings.weight', 'encoder.layer.6.attention.self.key.weight', 'encoder.layer.10.attention.self.query.bias', 'encoder.layer.3.output.LayerNorm.weight', 'encoder.layer.5.attention.output.dense.weight', 'encoder.layer.11.intermediate.dense.weight', 'encoder.layer.5.attention.self.key.weight', 'encoder.layer.10.attention.output.dense.weight', 'encoder.layer.11.attention.self.key.weight', 'encoder.layer.2.output.LayerNorm.weight', 'encoder.layer.8.attention.self.key.bias', 'encoder.layer.5.attention.self.value.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "***** Running training *****\n",
            "  Num examples = 600\n",
            "  Num Epochs = 2\n",
            "  Instantaneous batch size per device = 4\n",
            "  Total train batch size (w. parallel, distributed & accumulation) = 4\n",
            "  Gradient Accumulation steps = 1\n",
            "  Total optimization steps = 300\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='300' max='300' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [300/300 00:34, Epoch 2/2]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: left;\">\n",
              "      <th>Epoch</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "      <th>Accuracy</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>No log</td>\n",
              "      <td>0.274658</td>\n",
              "      <td>0.495000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2</td>\n",
              "      <td>No log</td>\n",
              "      <td>0.250933</td>\n",
              "      <td>0.495000</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "***** Running Evaluation *****\n",
            "  Num examples = 600\n",
            "  Batch size = 8\n",
            "Saving model checkpoint to /content/drive/MyDrive/run-3/checkpoint-150\n",
            "Configuration saved in /content/drive/MyDrive/run-3/checkpoint-150/config.json\n",
            "Model weights saved in /content/drive/MyDrive/run-3/checkpoint-150/pytorch_model.bin\n",
            "tokenizer config file saved in /content/drive/MyDrive/run-3/checkpoint-150/tokenizer_config.json\n",
            "Special tokens file saved in /content/drive/MyDrive/run-3/checkpoint-150/special_tokens_map.json\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 600\n",
            "  Batch size = 8\n",
            "Saving model checkpoint to /content/drive/MyDrive/run-3/checkpoint-300\n",
            "Configuration saved in /content/drive/MyDrive/run-3/checkpoint-300/config.json\n",
            "Model weights saved in /content/drive/MyDrive/run-3/checkpoint-300/pytorch_model.bin\n",
            "tokenizer config file saved in /content/drive/MyDrive/run-3/checkpoint-300/tokenizer_config.json\n",
            "Special tokens file saved in /content/drive/MyDrive/run-3/checkpoint-300/special_tokens_map.json\n",
            "\n",
            "\n",
            "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
            "\n",
            "\n",
            "Loading best model from /content/drive/MyDrive/run-3/checkpoint-300 (score: 0.25093328952789307).\n",
            "\u001b[32m[I 2021-11-27 22:50:44,218]\u001b[0m Trial 3 finished with value: 0.495 and parameters: {'learning_rate': 3.36175012770204e-05, 'num_train_epochs': 2, 'seed': 7, 'per_device_train_batch_size': 4}. Best is trial 0 with value: 0.495.\u001b[0m\n",
            "Trial:\n",
            "loading configuration file https://huggingface.co/distilbert-base-uncased/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/23454919702d26495337f3da04d1655c7ee010d5ec9d77bdb9e399e00302c0a1.91b885ab15d631bf9cee9dc9d25ece0afd932f2f5130eba28f2055b2220c0333\n",
            "You are using a model of type distilbert to instantiate a model of type bert. This is not supported for all configurations of models and can yield errors.\n",
            "Model config BertConfig {\n",
            "  \"activation\": \"gelu\",\n",
            "  \"architectures\": [\n",
            "    \"DistilBertForMaskedLM\"\n",
            "  ],\n",
            "  \"attention_dropout\": 0.1,\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"classifier_dropout\": null,\n",
            "  \"dim\": 768,\n",
            "  \"dropout\": 0.1,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dim\": 3072,\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 768,\n",
            "  \"id2label\": {\n",
            "    \"0\": \"LABEL_0\"\n",
            "  },\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 3072,\n",
            "  \"label2id\": {\n",
            "    \"LABEL_0\": 0\n",
            "  },\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"bert\",\n",
            "  \"n_heads\": 12,\n",
            "  \"n_layers\": 6,\n",
            "  \"num_attention_heads\": 12,\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"qa_dropout\": 0.1,\n",
            "  \"seq_classif_dropout\": 0.2,\n",
            "  \"sinusoidal_pos_embds\": false,\n",
            "  \"tie_weights_\": true,\n",
            "  \"transformers_version\": \"4.12.5\",\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 30522\n",
            "}\n",
            "\n",
            "loading weights file https://huggingface.co/distilbert-base-uncased/resolve/main/pytorch_model.bin from cache at /root/.cache/huggingface/transformers/9c169103d7e5a73936dd2b627e42851bec0831212b677c637033ee4bce9ab5ee.126183e36667471617ae2f0835fab707baa54b731f991507ebbb55ea85adb12a\n",
            "Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing BertForSequenceClassification: ['distilbert.transformer.layer.3.ffn.lin1.bias', 'distilbert.embeddings.position_embeddings.weight', 'distilbert.transformer.layer.1.attention.v_lin.bias', 'distilbert.transformer.layer.1.attention.q_lin.weight', 'distilbert.transformer.layer.0.attention.q_lin.weight', 'distilbert.embeddings.LayerNorm.weight', 'distilbert.transformer.layer.2.sa_layer_norm.bias', 'distilbert.transformer.layer.3.attention.v_lin.bias', 'distilbert.transformer.layer.5.ffn.lin1.bias', 'distilbert.transformer.layer.1.output_layer_norm.weight', 'distilbert.transformer.layer.5.attention.q_lin.weight', 'distilbert.transformer.layer.4.ffn.lin2.bias', 'distilbert.transformer.layer.0.attention.k_lin.bias', 'distilbert.transformer.layer.3.attention.v_lin.weight', 'distilbert.transformer.layer.0.ffn.lin2.weight', 'distilbert.transformer.layer.4.output_layer_norm.bias', 'distilbert.transformer.layer.2.output_layer_norm.weight', 'distilbert.transformer.layer.2.attention.k_lin.weight', 'distilbert.transformer.layer.4.ffn.lin1.weight', 'distilbert.transformer.layer.1.ffn.lin2.weight', 'distilbert.transformer.layer.5.attention.q_lin.bias', 'distilbert.transformer.layer.0.attention.v_lin.bias', 'distilbert.transformer.layer.2.ffn.lin1.weight', 'distilbert.transformer.layer.1.attention.out_lin.weight', 'distilbert.transformer.layer.5.ffn.lin2.bias', 'distilbert.transformer.layer.3.ffn.lin2.bias', 'distilbert.transformer.layer.1.attention.q_lin.bias', 'vocab_projector.bias', 'distilbert.transformer.layer.2.attention.out_lin.bias', 'distilbert.transformer.layer.0.attention.v_lin.weight', 'distilbert.transformer.layer.0.output_layer_norm.bias', 'distilbert.transformer.layer.0.attention.q_lin.bias', 'distilbert.transformer.layer.0.attention.k_lin.weight', 'distilbert.transformer.layer.0.ffn.lin2.bias', 'distilbert.transformer.layer.0.sa_layer_norm.bias', 'distilbert.transformer.layer.3.attention.out_lin.bias', 'distilbert.transformer.layer.4.attention.k_lin.bias', 'distilbert.transformer.layer.2.output_layer_norm.bias', 'distilbert.transformer.layer.4.ffn.lin2.weight', 'distilbert.transformer.layer.5.attention.v_lin.weight', 'distilbert.transformer.layer.5.sa_layer_norm.bias', 'distilbert.transformer.layer.1.attention.k_lin.bias', 'distilbert.transformer.layer.4.attention.k_lin.weight', 'distilbert.transformer.layer.3.attention.q_lin.bias', 'distilbert.transformer.layer.1.ffn.lin2.bias', 'distilbert.transformer.layer.3.ffn.lin1.weight', 'distilbert.transformer.layer.4.attention.q_lin.bias', 'distilbert.transformer.layer.4.attention.v_lin.weight', 'distilbert.transformer.layer.5.output_layer_norm.weight', 'distilbert.transformer.layer.2.attention.q_lin.weight', 'distilbert.transformer.layer.1.sa_layer_norm.bias', 'distilbert.transformer.layer.4.ffn.lin1.bias', 'distilbert.transformer.layer.5.ffn.lin2.weight', 'distilbert.transformer.layer.1.ffn.lin1.bias', 'distilbert.transformer.layer.3.attention.out_lin.weight', 'distilbert.transformer.layer.4.sa_layer_norm.bias', 'distilbert.transformer.layer.4.output_layer_norm.weight', 'distilbert.transformer.layer.5.attention.k_lin.bias', 'distilbert.transformer.layer.0.output_layer_norm.weight', 'distilbert.transformer.layer.3.attention.k_lin.weight', 'vocab_transform.weight', 'distilbert.transformer.layer.4.attention.out_lin.bias', 'distilbert.transformer.layer.4.attention.out_lin.weight', 'distilbert.transformer.layer.2.attention.q_lin.bias', 'distilbert.transformer.layer.2.ffn.lin1.bias', 'distilbert.transformer.layer.5.attention.out_lin.weight', 'vocab_projector.weight', 'distilbert.transformer.layer.2.attention.v_lin.weight', 'distilbert.transformer.layer.3.ffn.lin2.weight', 'distilbert.transformer.layer.3.output_layer_norm.bias', 'distilbert.transformer.layer.5.attention.k_lin.weight', 'distilbert.transformer.layer.1.attention.k_lin.weight', 'distilbert.transformer.layer.0.ffn.lin1.bias', 'distilbert.transformer.layer.0.ffn.lin1.weight', 'distilbert.transformer.layer.2.attention.k_lin.bias', 'distilbert.transformer.layer.1.attention.out_lin.bias', 'distilbert.transformer.layer.0.sa_layer_norm.weight', 'vocab_layer_norm.bias', 'distilbert.transformer.layer.4.sa_layer_norm.weight', 'distilbert.embeddings.word_embeddings.weight', 'distilbert.transformer.layer.2.ffn.lin2.weight', 'distilbert.transformer.layer.5.sa_layer_norm.weight', 'distilbert.transformer.layer.1.attention.v_lin.weight', 'distilbert.transformer.layer.3.sa_layer_norm.weight', 'distilbert.transformer.layer.5.ffn.lin1.weight', 'distilbert.transformer.layer.3.attention.q_lin.weight', 'distilbert.transformer.layer.0.attention.out_lin.weight', 'distilbert.transformer.layer.2.attention.out_lin.weight', 'distilbert.transformer.layer.2.sa_layer_norm.weight', 'distilbert.transformer.layer.3.output_layer_norm.weight', 'distilbert.transformer.layer.5.attention.out_lin.bias', 'distilbert.transformer.layer.1.sa_layer_norm.weight', 'distilbert.transformer.layer.3.attention.k_lin.bias', 'distilbert.transformer.layer.1.output_layer_norm.bias', 'vocab_layer_norm.weight', 'distilbert.transformer.layer.4.attention.q_lin.weight', 'distilbert.transformer.layer.1.ffn.lin1.weight', 'distilbert.transformer.layer.2.attention.v_lin.bias', 'distilbert.transformer.layer.4.attention.v_lin.bias', 'distilbert.transformer.layer.5.output_layer_norm.bias', 'distilbert.embeddings.LayerNorm.bias', 'distilbert.transformer.layer.5.attention.v_lin.bias', 'distilbert.transformer.layer.0.attention.out_lin.bias', 'vocab_transform.bias', 'distilbert.transformer.layer.3.sa_layer_norm.bias', 'distilbert.transformer.layer.2.ffn.lin2.bias']\n",
            "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['encoder.layer.3.attention.output.dense.weight', 'encoder.layer.7.attention.output.LayerNorm.bias', 'encoder.layer.1.attention.output.LayerNorm.weight', 'encoder.layer.6.output.dense.bias', 'encoder.layer.3.attention.output.LayerNorm.weight', 'encoder.layer.1.attention.self.key.weight', 'encoder.layer.0.attention.self.query.bias', 'encoder.layer.10.output.dense.weight', 'encoder.layer.11.attention.self.value.weight', 'encoder.layer.11.attention.output.dense.weight', 'classifier.weight', 'encoder.layer.4.attention.self.query.weight', 'encoder.layer.4.attention.self.value.bias', 'encoder.layer.9.intermediate.dense.weight', 'encoder.layer.10.attention.output.dense.bias', 'encoder.layer.2.attention.self.query.weight', 'encoder.layer.3.attention.self.query.weight', 'encoder.layer.9.attention.output.LayerNorm.weight', 'encoder.layer.5.attention.self.value.bias', 'encoder.layer.1.intermediate.dense.bias', 'encoder.layer.1.attention.output.LayerNorm.bias', 'encoder.layer.7.attention.self.key.weight', 'encoder.layer.0.attention.output.LayerNorm.weight', 'encoder.layer.8.output.dense.weight', 'encoder.layer.11.attention.self.query.bias', 'encoder.layer.3.attention.output.dense.bias', 'encoder.layer.4.output.dense.bias', 'encoder.layer.2.attention.self.query.bias', 'encoder.layer.8.attention.self.value.bias', 'encoder.layer.8.intermediate.dense.weight', 'encoder.layer.7.output.dense.weight', 'encoder.layer.6.attention.output.dense.weight', 'encoder.layer.9.output.LayerNorm.bias', 'encoder.layer.11.attention.self.query.weight', 'encoder.layer.3.intermediate.dense.bias', 'encoder.layer.8.intermediate.dense.bias', 'encoder.layer.9.attention.self.value.bias', 'encoder.layer.4.attention.output.LayerNorm.weight', 'encoder.layer.9.output.LayerNorm.weight', 'encoder.layer.7.attention.self.value.bias', 'encoder.layer.5.output.dense.weight', 'encoder.layer.4.attention.self.value.weight', 'encoder.layer.11.output.dense.weight', 'pooler.dense.weight', 'encoder.layer.0.attention.output.LayerNorm.bias', 'embeddings.LayerNorm.weight', 'encoder.layer.5.intermediate.dense.weight', 'encoder.layer.6.attention.output.LayerNorm.bias', 'encoder.layer.4.attention.output.dense.weight', 'encoder.layer.3.attention.output.LayerNorm.bias', 'encoder.layer.8.output.LayerNorm.bias', 'encoder.layer.6.output.LayerNorm.weight', 'encoder.layer.11.output.LayerNorm.bias', 'encoder.layer.9.attention.self.query.bias', 'encoder.layer.2.output.dense.bias', 'encoder.layer.5.output.LayerNorm.weight', 'encoder.layer.2.intermediate.dense.bias', 'encoder.layer.7.attention.self.key.bias', 'encoder.layer.9.attention.self.value.weight', 'encoder.layer.1.attention.self.key.bias', 'encoder.layer.7.attention.output.dense.weight', 'encoder.layer.5.attention.output.LayerNorm.weight', 'encoder.layer.10.attention.self.key.weight', 'encoder.layer.0.attention.self.key.bias', 'encoder.layer.2.attention.self.key.bias', 'encoder.layer.6.output.LayerNorm.bias', 'encoder.layer.10.output.dense.bias', 'encoder.layer.5.output.dense.bias', 'encoder.layer.1.attention.self.query.weight', 'encoder.layer.0.output.dense.bias', 'encoder.layer.7.attention.self.query.weight', 'encoder.layer.0.output.dense.weight', 'encoder.layer.10.output.LayerNorm.bias', 'encoder.layer.8.output.dense.bias', 'encoder.layer.7.output.LayerNorm.weight', 'encoder.layer.10.intermediate.dense.bias', 'encoder.layer.6.attention.self.value.weight', 'encoder.layer.2.attention.output.LayerNorm.bias', 'encoder.layer.5.attention.output.dense.bias', 'encoder.layer.1.attention.self.query.bias', 'encoder.layer.4.output.LayerNorm.weight', 'encoder.layer.8.attention.self.query.bias', 'encoder.layer.4.attention.self.key.weight', 'encoder.layer.6.attention.self.query.bias', 'encoder.layer.0.attention.self.value.weight', 'encoder.layer.2.attention.output.dense.weight', 'encoder.layer.7.attention.self.query.bias', 'encoder.layer.0.output.LayerNorm.weight', 'encoder.layer.5.attention.output.LayerNorm.bias', 'encoder.layer.6.attention.self.value.bias', 'encoder.layer.6.attention.self.query.weight', 'encoder.layer.7.attention.output.LayerNorm.weight', 'encoder.layer.11.attention.output.dense.bias', 'encoder.layer.7.intermediate.dense.weight', 'encoder.layer.2.intermediate.dense.weight', 'encoder.layer.10.attention.output.LayerNorm.weight', 'encoder.layer.11.attention.output.LayerNorm.weight', 'encoder.layer.7.attention.self.value.weight', 'encoder.layer.8.attention.output.LayerNorm.bias', 'encoder.layer.11.attention.self.value.bias', 'encoder.layer.0.output.LayerNorm.bias', 'encoder.layer.5.attention.self.query.weight', 'encoder.layer.2.output.dense.weight', 'encoder.layer.10.attention.self.key.bias', 'encoder.layer.9.attention.self.key.bias', 'encoder.layer.4.attention.self.query.bias', 'encoder.layer.6.intermediate.dense.weight', 'encoder.layer.6.intermediate.dense.bias', 'encoder.layer.1.attention.output.dense.weight', 'classifier.bias', 'encoder.layer.3.attention.self.value.weight', 'encoder.layer.2.attention.self.key.weight', 'encoder.layer.0.attention.output.dense.bias', 'encoder.layer.0.intermediate.dense.bias', 'encoder.layer.1.output.LayerNorm.weight', 'encoder.layer.6.attention.output.LayerNorm.weight', 'encoder.layer.8.attention.output.dense.weight', 'encoder.layer.5.output.LayerNorm.bias', 'encoder.layer.10.attention.self.query.weight', 'encoder.layer.9.attention.output.dense.bias', 'encoder.layer.3.output.dense.weight', 'encoder.layer.7.output.dense.bias', 'encoder.layer.2.output.LayerNorm.bias', 'encoder.layer.11.attention.output.LayerNorm.bias', 'encoder.layer.0.attention.self.query.weight', 'pooler.dense.bias', 'encoder.layer.3.output.dense.bias', 'encoder.layer.3.attention.self.key.weight', 'encoder.layer.3.output.LayerNorm.bias', 'encoder.layer.2.attention.self.value.bias', 'encoder.layer.10.attention.self.value.weight', 'encoder.layer.10.attention.output.LayerNorm.bias', 'encoder.layer.9.attention.self.key.weight', 'encoder.layer.5.attention.self.key.bias', 'encoder.layer.3.intermediate.dense.weight', 'encoder.layer.7.intermediate.dense.bias', 'encoder.layer.8.attention.output.LayerNorm.weight', 'encoder.layer.9.attention.output.dense.weight', 'encoder.layer.0.attention.self.key.weight', 'encoder.layer.1.intermediate.dense.weight', 'encoder.layer.4.intermediate.dense.weight', 'embeddings.position_embeddings.weight', 'encoder.layer.8.attention.output.dense.bias', 'encoder.layer.9.output.dense.weight', 'encoder.layer.11.attention.self.key.bias', 'encoder.layer.6.attention.self.key.bias', 'encoder.layer.8.attention.self.value.weight', 'encoder.layer.7.attention.output.dense.bias', 'encoder.layer.1.attention.self.value.weight', 'encoder.layer.1.output.dense.bias', 'encoder.layer.4.attention.self.key.bias', 'encoder.layer.10.attention.self.value.bias', 'encoder.layer.4.intermediate.dense.bias', 'encoder.layer.6.attention.output.dense.bias', 'encoder.layer.7.output.LayerNorm.bias', 'encoder.layer.1.attention.self.value.bias', 'encoder.layer.8.attention.self.key.weight', 'encoder.layer.1.output.dense.weight', 'encoder.layer.11.output.LayerNorm.weight', 'encoder.layer.11.output.dense.bias', 'encoder.layer.9.output.dense.bias', 'encoder.layer.9.intermediate.dense.bias', 'encoder.layer.5.intermediate.dense.bias', 'encoder.layer.11.intermediate.dense.bias', 'encoder.layer.1.output.LayerNorm.bias', 'encoder.layer.2.attention.self.value.weight', 'encoder.layer.2.attention.output.dense.bias', 'encoder.layer.0.intermediate.dense.weight', 'encoder.layer.0.attention.output.dense.weight', 'encoder.layer.3.attention.self.query.bias', 'encoder.layer.6.output.dense.weight', 'encoder.layer.3.attention.self.value.bias', 'encoder.layer.5.attention.self.query.bias', 'encoder.layer.10.output.LayerNorm.weight', 'encoder.layer.3.attention.self.key.bias', 'embeddings.word_embeddings.weight', 'encoder.layer.4.attention.output.dense.bias', 'encoder.layer.4.output.LayerNorm.bias', 'encoder.layer.9.attention.self.query.weight', 'encoder.layer.0.attention.self.value.bias', 'encoder.layer.4.output.dense.weight', 'encoder.layer.10.intermediate.dense.weight', 'embeddings.LayerNorm.bias', 'encoder.layer.4.attention.output.LayerNorm.bias', 'encoder.layer.8.attention.self.query.weight', 'encoder.layer.8.output.LayerNorm.weight', 'encoder.layer.1.attention.output.dense.bias', 'encoder.layer.2.attention.output.LayerNorm.weight', 'encoder.layer.9.attention.output.LayerNorm.bias', 'embeddings.token_type_embeddings.weight', 'encoder.layer.6.attention.self.key.weight', 'encoder.layer.10.attention.self.query.bias', 'encoder.layer.3.output.LayerNorm.weight', 'encoder.layer.5.attention.output.dense.weight', 'encoder.layer.11.intermediate.dense.weight', 'encoder.layer.5.attention.self.key.weight', 'encoder.layer.10.attention.output.dense.weight', 'encoder.layer.11.attention.self.key.weight', 'encoder.layer.2.output.LayerNorm.weight', 'encoder.layer.8.attention.self.key.bias', 'encoder.layer.5.attention.self.value.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "***** Running training *****\n",
            "  Num examples = 600\n",
            "  Num Epochs = 5\n",
            "  Instantaneous batch size per device = 32\n",
            "  Total train batch size (w. parallel, distributed & accumulation) = 32\n",
            "  Gradient Accumulation steps = 1\n",
            "  Total optimization steps = 95\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='95' max='95' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [95/95 01:06, Epoch 5/5]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: left;\">\n",
              "      <th>Epoch</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "      <th>Accuracy</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>No log</td>\n",
              "      <td>0.252458</td>\n",
              "      <td>0.495000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2</td>\n",
              "      <td>No log</td>\n",
              "      <td>0.270648</td>\n",
              "      <td>0.495000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3</td>\n",
              "      <td>No log</td>\n",
              "      <td>0.250123</td>\n",
              "      <td>0.495000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4</td>\n",
              "      <td>No log</td>\n",
              "      <td>0.251451</td>\n",
              "      <td>0.495000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>5</td>\n",
              "      <td>No log</td>\n",
              "      <td>0.249977</td>\n",
              "      <td>0.495000</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "***** Running Evaluation *****\n",
            "  Num examples = 600\n",
            "  Batch size = 8\n",
            "Saving model checkpoint to /content/drive/MyDrive/run-4/checkpoint-19\n",
            "Configuration saved in /content/drive/MyDrive/run-4/checkpoint-19/config.json\n",
            "Model weights saved in /content/drive/MyDrive/run-4/checkpoint-19/pytorch_model.bin\n",
            "tokenizer config file saved in /content/drive/MyDrive/run-4/checkpoint-19/tokenizer_config.json\n",
            "Special tokens file saved in /content/drive/MyDrive/run-4/checkpoint-19/special_tokens_map.json\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 600\n",
            "  Batch size = 8\n",
            "Saving model checkpoint to /content/drive/MyDrive/run-4/checkpoint-38\n",
            "Configuration saved in /content/drive/MyDrive/run-4/checkpoint-38/config.json\n",
            "Model weights saved in /content/drive/MyDrive/run-4/checkpoint-38/pytorch_model.bin\n",
            "tokenizer config file saved in /content/drive/MyDrive/run-4/checkpoint-38/tokenizer_config.json\n",
            "Special tokens file saved in /content/drive/MyDrive/run-4/checkpoint-38/special_tokens_map.json\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 600\n",
            "  Batch size = 8\n",
            "Saving model checkpoint to /content/drive/MyDrive/run-4/checkpoint-57\n",
            "Configuration saved in /content/drive/MyDrive/run-4/checkpoint-57/config.json\n",
            "Model weights saved in /content/drive/MyDrive/run-4/checkpoint-57/pytorch_model.bin\n",
            "tokenizer config file saved in /content/drive/MyDrive/run-4/checkpoint-57/tokenizer_config.json\n",
            "Special tokens file saved in /content/drive/MyDrive/run-4/checkpoint-57/special_tokens_map.json\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 600\n",
            "  Batch size = 8\n",
            "Saving model checkpoint to /content/drive/MyDrive/run-4/checkpoint-76\n",
            "Configuration saved in /content/drive/MyDrive/run-4/checkpoint-76/config.json\n",
            "Model weights saved in /content/drive/MyDrive/run-4/checkpoint-76/pytorch_model.bin\n",
            "tokenizer config file saved in /content/drive/MyDrive/run-4/checkpoint-76/tokenizer_config.json\n",
            "Special tokens file saved in /content/drive/MyDrive/run-4/checkpoint-76/special_tokens_map.json\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 600\n",
            "  Batch size = 8\n",
            "Saving model checkpoint to /content/drive/MyDrive/run-4/checkpoint-95\n",
            "Configuration saved in /content/drive/MyDrive/run-4/checkpoint-95/config.json\n",
            "Model weights saved in /content/drive/MyDrive/run-4/checkpoint-95/pytorch_model.bin\n",
            "tokenizer config file saved in /content/drive/MyDrive/run-4/checkpoint-95/tokenizer_config.json\n",
            "Special tokens file saved in /content/drive/MyDrive/run-4/checkpoint-95/special_tokens_map.json\n",
            "\n",
            "\n",
            "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
            "\n",
            "\n",
            "Loading best model from /content/drive/MyDrive/run-4/checkpoint-95 (score: 0.2499767541885376).\n",
            "\u001b[32m[I 2021-11-27 22:51:52,924]\u001b[0m Trial 4 finished with value: 0.495 and parameters: {'learning_rate': 8.834924552290687e-05, 'num_train_epochs': 5, 'seed': 34, 'per_device_train_batch_size': 32}. Best is trial 0 with value: 0.495.\u001b[0m\n",
            "Trial:\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-7-932bc8a1c29d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0mcompute_metrics\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcompute_metrics\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m )\n\u001b[0;32m---> 11\u001b[0;31m \u001b[0mbest_run\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhyperparameter_search\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_trials\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdirection\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"maximize\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/transformers/trainer.py\u001b[0m in \u001b[0;36mhyperparameter_search\u001b[0;34m(self, hp_space, compute_objective, n_trials, direction, backend, hp_name, **kwargs)\u001b[0m\n\u001b[1;32m   1766\u001b[0m             \u001b[0mHPSearchBackend\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSIGOPT\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mrun_hp_search_sigopt\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1767\u001b[0m         }\n\u001b[0;32m-> 1768\u001b[0;31m         \u001b[0mbest_run\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbackend_dict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mbackend\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_trials\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdirection\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1769\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1770\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhp_search_backend\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/transformers/integrations.py\u001b[0m in \u001b[0;36mrun_hp_search_optuna\u001b[0;34m(trainer, n_trials, direction, **kwargs)\u001b[0m\n\u001b[1;32m    158\u001b[0m     \u001b[0mn_jobs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"n_jobs\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    159\u001b[0m     \u001b[0mstudy\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moptuna\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreate_study\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdirection\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdirection\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 160\u001b[0;31m     \u001b[0mstudy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptimize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_objective\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_trials\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mn_trials\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_jobs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mn_jobs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    161\u001b[0m     \u001b[0mbest_trial\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstudy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbest_trial\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    162\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mBestRun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbest_trial\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumber\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbest_trial\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbest_trial\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/optuna/study/study.py\u001b[0m in \u001b[0;36moptimize\u001b[0;34m(self, func, n_trials, timeout, n_jobs, catch, callbacks, gc_after_trial, show_progress_bar)\u001b[0m\n\u001b[1;32m    407\u001b[0m             \u001b[0mcallbacks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcallbacks\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    408\u001b[0m             \u001b[0mgc_after_trial\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mgc_after_trial\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 409\u001b[0;31m             \u001b[0mshow_progress_bar\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mshow_progress_bar\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    410\u001b[0m         )\n\u001b[1;32m    411\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/optuna/study/_optimize.py\u001b[0m in \u001b[0;36m_optimize\u001b[0;34m(study, func, n_trials, timeout, n_jobs, catch, callbacks, gc_after_trial, show_progress_bar)\u001b[0m\n\u001b[1;32m     74\u001b[0m                 \u001b[0mreseed_sampler_rng\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     75\u001b[0m                 \u001b[0mtime_start\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 76\u001b[0;31m                 \u001b[0mprogress_bar\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mprogress_bar\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     77\u001b[0m             )\n\u001b[1;32m     78\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/optuna/study/_optimize.py\u001b[0m in \u001b[0;36m_optimize_sequential\u001b[0;34m(study, func, n_trials, timeout, catch, callbacks, gc_after_trial, reseed_sampler_rng, time_start, progress_bar)\u001b[0m\n\u001b[1;32m    161\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    162\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 163\u001b[0;31m             \u001b[0mtrial\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_run_trial\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstudy\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    164\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    165\u001b[0m             \u001b[0;32mraise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/optuna/study/_optimize.py\u001b[0m in \u001b[0;36m_run_trial\u001b[0;34m(study, func, catch)\u001b[0m\n\u001b[1;32m    211\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    212\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 213\u001b[0;31m         \u001b[0mvalue_or_values\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrial\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    214\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mexceptions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTrialPruned\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    215\u001b[0m         \u001b[0;31m# TODO(mamu): Handle multi-objective cases.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/transformers/integrations.py\u001b[0m in \u001b[0;36m_objective\u001b[0;34m(trial, checkpoint_dir)\u001b[0m\n\u001b[1;32m    148\u001b[0m                     \u001b[0mcheckpoint\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcheckpoint_dir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msubdir\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    149\u001b[0m         \u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mobjective\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 150\u001b[0;31m         \u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresume_from_checkpoint\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcheckpoint\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrial\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrial\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    151\u001b[0m         \u001b[0;31m# If there hasn't been any evaluation during the training loop.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    152\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrainer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"objective\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/transformers/trainer.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[1;32m   1064\u001b[0m             \u001b[0;31m# Seed must be set before instantiating the model when using model_init.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1065\u001b[0m             \u001b[0mset_seed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mseed\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1066\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcall_model_init\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrial\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1067\u001b[0m             \u001b[0mmodel_reloaded\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1068\u001b[0m             \u001b[0;31m# Reinitializes optimizer and scheduler\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/transformers/trainer.py\u001b[0m in \u001b[0;36mcall_model_init\u001b[0;34m(self, trial)\u001b[0m\n\u001b[1;32m    931\u001b[0m         \u001b[0mmodel_init_argcount\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnumber_of_arguments\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel_init\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    932\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mmodel_init_argcount\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 933\u001b[0;31m             \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel_init\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    934\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mmodel_init_argcount\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    935\u001b[0m             \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel_init\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrial\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-7-932bc8a1c29d>\u001b[0m in \u001b[0;36mmodel_init\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mmodel_init\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mBertForSequenceClassification\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_pretrained\u001b[0m\u001b[0;34m(\u001b[0m \u001b[0;34m\"distilbert-base-uncased\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_labels\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m trainer = Trainer(\n\u001b[1;32m      4\u001b[0m     \u001b[0mmodel_init\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmodel_init\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0margs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtraining_args\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/transformers/modeling_utils.py\u001b[0m in \u001b[0;36mfrom_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, *model_args, **kwargs)\u001b[0m\n\u001b[1;32m   1269\u001b[0m                 \u001b[0m_from_auto\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfrom_auto_class\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1270\u001b[0m                 \u001b[0m_from_pipeline\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfrom_pipeline\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1271\u001b[0;31m                 \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1272\u001b[0m             )\n\u001b[1;32m   1273\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/transformers/configuration_utils.py\u001b[0m in \u001b[0;36mfrom_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, **kwargs)\u001b[0m\n\u001b[1;32m    491\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    492\u001b[0m         \"\"\"\n\u001b[0;32m--> 493\u001b[0;31m         \u001b[0mconfig_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcls\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_config_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpretrained_model_name_or_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    494\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;34m\"model_type\"\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mconfig_dict\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcls\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"model_type\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mconfig_dict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"model_type\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mcls\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel_type\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    495\u001b[0m             logger.warn(\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/transformers/configuration_utils.py\u001b[0m in \u001b[0;36mget_config_dict\u001b[0;34m(cls, pretrained_model_name_or_path, **kwargs)\u001b[0m\n\u001b[1;32m    556\u001b[0m                 \u001b[0mlocal_files_only\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlocal_files_only\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    557\u001b[0m                 \u001b[0muse_auth_token\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0muse_auth_token\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 558\u001b[0;31m                 \u001b[0muser_agent\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0muser_agent\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    559\u001b[0m             )\n\u001b[1;32m    560\u001b[0m             \u001b[0;31m# Load config dict\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/transformers/file_utils.py\u001b[0m in \u001b[0;36mcached_path\u001b[0;34m(url_or_filename, cache_dir, force_download, proxies, resume_download, user_agent, extract_compressed_file, force_extract, use_auth_token, local_files_only)\u001b[0m\n\u001b[1;32m   1497\u001b[0m             \u001b[0muser_agent\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0muser_agent\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1498\u001b[0m             \u001b[0muse_auth_token\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0muse_auth_token\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1499\u001b[0;31m             \u001b[0mlocal_files_only\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlocal_files_only\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1500\u001b[0m         )\n\u001b[1;32m   1501\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexists\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0murl_or_filename\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/transformers/file_utils.py\u001b[0m in \u001b[0;36mget_from_cache\u001b[0;34m(url, cache_dir, force_download, proxies, etag_timeout, resume_download, user_agent, use_auth_token, local_files_only)\u001b[0m\n\u001b[1;32m   1660\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mlocal_files_only\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1661\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1662\u001b[0;31m             \u001b[0mr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrequests\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhead\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mheaders\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mheaders\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mallow_redirects\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mproxies\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mproxies\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0metag_timeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1663\u001b[0m             \u001b[0mr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mraise_for_status\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1664\u001b[0m             \u001b[0metag\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mheaders\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"X-Linked-Etag\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mheaders\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"ETag\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/requests/api.py\u001b[0m in \u001b[0;36mhead\u001b[0;34m(url, **kwargs)\u001b[0m\n\u001b[1;32m    102\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    103\u001b[0m     \u001b[0mkwargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msetdefault\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'allow_redirects'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 104\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mrequest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'head'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    105\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    106\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/requests/api.py\u001b[0m in \u001b[0;36mrequest\u001b[0;34m(method, url, **kwargs)\u001b[0m\n\u001b[1;32m     59\u001b[0m     \u001b[0;31m# cases, and look like a memory leak in others.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     60\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0msessions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSession\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0msession\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 61\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0msession\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmethod\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmethod\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0murl\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     62\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/requests/sessions.py\u001b[0m in \u001b[0;36mrequest\u001b[0;34m(self, method, url, params, data, headers, cookies, files, auth, timeout, allow_redirects, proxies, hooks, stream, verify, cert, json)\u001b[0m\n\u001b[1;32m    528\u001b[0m         }\n\u001b[1;32m    529\u001b[0m         \u001b[0msend_kwargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msettings\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 530\u001b[0;31m         \u001b[0mresp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprep\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0msend_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    531\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    532\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mresp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/requests/sessions.py\u001b[0m in \u001b[0;36msend\u001b[0;34m(self, request, **kwargs)\u001b[0m\n\u001b[1;32m    641\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    642\u001b[0m         \u001b[0;31m# Send the request\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 643\u001b[0;31m         \u001b[0mr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0madapter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    644\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    645\u001b[0m         \u001b[0;31m# Total elapsed time of the request (approximately)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/requests/adapters.py\u001b[0m in \u001b[0;36msend\u001b[0;34m(self, request, stream, timeout, verify, cert, proxies)\u001b[0m\n\u001b[1;32m    447\u001b[0m                     \u001b[0mdecode_content\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    448\u001b[0m                     \u001b[0mretries\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax_retries\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 449\u001b[0;31m                     \u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    450\u001b[0m                 )\n\u001b[1;32m    451\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/urllib3/connectionpool.py\u001b[0m in \u001b[0;36murlopen\u001b[0;34m(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, **response_kw)\u001b[0m\n\u001b[1;32m    598\u001b[0m                                                   \u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtimeout_obj\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    599\u001b[0m                                                   \u001b[0mbody\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbody\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mheaders\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mheaders\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 600\u001b[0;31m                                                   chunked=chunked)\n\u001b[0m\u001b[1;32m    601\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    602\u001b[0m             \u001b[0;31m# If we're going to release the connection in ``finally:``, then\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/urllib3/connectionpool.py\u001b[0m in \u001b[0;36m_make_request\u001b[0;34m(self, conn, method, url, timeout, chunked, **httplib_request_kw)\u001b[0m\n\u001b[1;32m    341\u001b[0m         \u001b[0;31m# Trigger any extra validation we need to do.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    342\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 343\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_validate_conn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    344\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mSocketTimeout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mBaseSSLError\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    345\u001b[0m             \u001b[0;31m# Py2 raises this as a BaseSSLError, Py3 raises it as socket timeout.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/urllib3/connectionpool.py\u001b[0m in \u001b[0;36m_validate_conn\u001b[0;34m(self, conn)\u001b[0m\n\u001b[1;32m    837\u001b[0m         \u001b[0;31m# Force connect early to allow us to validate the connection.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    838\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'sock'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# AppEngine might not have  `.sock`\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 839\u001b[0;31m             \u001b[0mconn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconnect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    840\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    841\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mconn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_verified\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/urllib3/connection.py\u001b[0m in \u001b[0;36mconnect\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    299\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mconnect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    300\u001b[0m         \u001b[0;31m# Add certificate verification\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 301\u001b[0;31m         \u001b[0mconn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_new_conn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    302\u001b[0m         \u001b[0mhostname\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhost\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    303\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/urllib3/connection.py\u001b[0m in \u001b[0;36m_new_conn\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    157\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    158\u001b[0m             conn = connection.create_connection(\n\u001b[0;32m--> 159\u001b[0;31m                 (self._dns_host, self.port), self.timeout, **extra_kw)\n\u001b[0m\u001b[1;32m    160\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    161\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mSocketTimeout\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/urllib3/util/connection.py\u001b[0m in \u001b[0;36mcreate_connection\u001b[0;34m(address, timeout, source_address, socket_options)\u001b[0m\n\u001b[1;32m     68\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0msource_address\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m                 \u001b[0msock\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbind\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msource_address\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 70\u001b[0;31m             \u001b[0msock\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconnect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msa\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     71\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0msock\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     72\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d7da7ef9"
      },
      "source": [
        "\n",
        "import numpy as np\n",
        "from datasets import load_metric\n",
        "\n",
        "metric = load_metric(\"accuracy\")\n",
        "\n",
        "def compute_metrics(eval_pred):\n",
        "    logits, labels = eval_pred\n",
        "    predictions = np.argmax(logits, axis=-1)\n",
        "    return metric.compute(predictions=predictions, references=labels)\n",
        "\n",
        "\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=\"/content/drive/MyDrive/\",\n",
        "    save_strategy = \"epoch\",\n",
        "    evaluation_strategy=\"epoch\",\n",
        "    per_device_train_batch_size=13,\n",
        "    per_device_eval_batch_size=13,\n",
        "    num_train_epochs=15,\n",
        "    weight_decay = .00005\n",
        ")\n",
        "\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=encoded_dataset[\"train\"],\n",
        "    eval_dataset=encoded_dataset[\"test\"],\n",
        "    compute_metrics=compute_metrics,\n",
        "   \n",
        "\n",
        ")\n",
        "trainer.evaluate()\n"
      ],
      "id": "d7da7ef9",
      "execution_count": null,
      "outputs": []
    }
  ]
}